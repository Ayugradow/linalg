\chapter{Matrices}
\section{Definitions and Basic Properties}

Matrices are simply the mathematical name given to a table of values. For instance, we can have numerical matrices
\[\begin{pmatrix}
1 & 2 & 9\\
-8 & 5 & \pi
\end{pmatrix}, \begin{pmatrix}
0 & 0\\
0 & 0
\end{pmatrix},
\begin{pmatrix}
1 & 0 & 0\\
0 & 5 & 0\\
0 & 0 & -16
\end{pmatrix},\cdots\] or of any other kind, really
\[\begin{pmatrix}
\text{Geometria} & \text{Analítica}\\
\text{Álgebra} & \text{Linear}
\end{pmatrix},\begin{pmatrix}
\sqcup & \triangle & \triangledown\\
\circledcirc & \nabla & \sum
\end{pmatrix},\cdots.\]

At first, it might seem rather pointless to study these objects - and even more pointless to define operations and do maths with them. A motivation for that will be left for a posterior chapter. At this point, however, we will simply give a ``brief'' motivation:

\begin{ex}
	Five friends, Ann, Beth, Carl, Dave and Eliza have decided to celebrate Carl's birthday by playing a couple of bowling matches. In total, they ended up playing three matches and got the following results:
	\[\bordermatrix{
	&\text{Ann} & \text{Beth} & \text{Carl} & \text{Dave} &\text{Eliza}\cr
	\text{First game} & 101 & 96 & 99 & 87 & 123\cr
	\text{Second game} & 95 & 100 & 110 & 80 & 102\cr
	\text{Third game} & 90 & 103 & 80 & 86 & 110}.\]Unhappy with the results, Dave commited to practicing a lot. A few months later it was his birthday, and he invited everyone else for a rematch. They once again played for a total of three games and got the following results:
	\[\bordermatrix{
		&\text{Ann} & \text{Beth} & \text{Carl} & \text{Dave} &\text{Eliza}\cr
		\text{First game} & 97 & 87 & 90 & 150 & 103\cr
		\text{Second game} & 80 & 105 & 100 & 170 & 98\cr
		\text{Third game} & 120 & 110 & 80 & 127 & 115}.\] Astonished with the results, they decided to check who had made the highest amount of points throughout all six games.
	\[\bordermatrix{
		&\text{Ann} & \text{Beth} & \text{Carl} & \text{Dave} &\text{Eliza}\cr
		\text{First game} & 101+97 & 96+87 & 99+90 & 87+150 & 123+103\cr
		\text{Second game} & 95+80 & 100+105 & 110+100 & 80+170 & 102+98\cr
		\text{Third game} & 90+120 & 103+110 & 80+80 & 86+127 & 110+115}\]\[=\bordermatrix{
		&\text{Ann} & \text{Beth} & \text{Carl} & \text{Dave} &\text{Eliza}\cr
		 & 198 & 183 & 189 & 237 & 226\cr
		 & 175 & 205 & 210 & 250 & 200\cr
		 & 210 & 213 & 160 & 213 & 225}\]
\end{ex}

\subsection{Adding and Multiplying by numbers}

\begin{df}
	Let us denote by $M_{n\times m}(\R)$ the \textbf{the set of all matrices with $n$ rows and $m$ columns, with entries in $\R$}. In case $n=m$ we'll say that the matrices are \textbf{square} and denote it simply by $M_n(\R)$.
	
	Analogously, we'll denote the element in row $i$, column $j$ of a matrix $M$ by $M_{i,j}$.
\end{df}

\begin{ex}
	The matrix \[M=\begin{pmatrix}
	1 & 2 & 9\\
	-8 & 5 & \pi
	\end{pmatrix}\]clearly belongs to $M_{2\times 3}(\R)$. Furthermore, we have: $M_{1,1}=1,M_{1,2}=2,M_{1,3}=9,M_{2,1}=-8,M_{2,2}=5$ and $M_{2,3}=\pi$.
	
	\bigskip
	Conversely, if we say that a matrix $N\in M_{3\times 2}(\R)$ has entries $N_{1,1}=0=N_{3,2},N_{1,2}=1,N_{2,1}=-1,N_{2,2}=\pi$ and $N_{3,1}=-\pi$, then we can recover $N$:
	\[N=\begin{pmatrix}
	0 & 1\\
	-1 & \pi\\
	-\pi &0
	\end{pmatrix}\]
\end{ex}

From this example we get a very powerful tool: \textbf{a matrix is uniquely determined by its entries} - that is, given a matrix $M$, there exists a unique collection of entries $M_{i,j}$; and given a collection $a_{i,j}$ there exists a unique matrix $A$ whose entries are precisely $A_{i,j}=a_{i,j}$.

That might sound obvious, but it's also what allows us, for instance, to make the following definition:

\begin{df}
	Given two matrices $M,N\in M_{n\times m}(\R)$, we define the \textbf{sum of $M$ and $N$} to be the matrix $M+N$ given by
	\[(M+N)_{i,j}:= M_{i,j}+N_{i,j}.\]
\end{df}
\begin{rmk}
	Here, the symbol ``$x:=y$'' means ``we are defining $x$ to be equal to $y$'', or ``$x=y$ by definition''.
\end{rmk}
\begin{rmk}
	Note that the above definition makes sense: for each pair of indices $i,j$, both $M_{i,j}$ and $N_{i,j}$ are real numbers (things we know how to add together!) and therefore $M_{i,j}+N_{i,j}$ is also a real number. We're then collecting all those additions, varying $i$ and $j$, and calling the resulting matrix, whose entries are the addition of entries of both $M$ and $N$, of $M+N$.
\end{rmk}
\begin{exerc}
	As we have defined it, our matrix sum only makes sense for matrices with the same number of rows and columns. Try making up a new definition such that you can add together any two matrices, no matter their sizes. Why don't we use such a definition?
\end{exerc}

Similarly to what we did with addition, defining it entry-by-entry, we can also define multiplying by numbers:

\begin{df}
	Let $a\in \R$ be a real number and $M\in M_{n\times m}(\R)$ be any $n\times m$ real matrix. We define the \textbf{product of $a$ copies of $M$} to be the matrix $aM$ given by
	\[(aM)_{i,j}:=a\cdot M_{i,j}.\]
\end{df}

Once again, the same reasoning as above shows us that this definition makes sense, since $a$ and $M_{i,j}$ are real numbers (things we know how to multiply together!), so $aM_{i,j}$ is also a real number.

\begin{ex}
	Let $M=\begin{pmatrix}
	1 & -8\\
	2 & 5\\
	9 & \pi
	\end{pmatrix}$ and $N=\begin{pmatrix}
	0 & 1\\
	-1 & \pi\\
	-\pi &0
	\end{pmatrix}$ be real matrices, and $a=\pi$. Then, we can compute:
	\begin{alignat*}{3}
	&(M+N)_{1,1}=M_{1,1}+N_{1,1}=1+0=1\quad&\quad&(M+N)_{1,2}=M_{1,2}+N_{1,2}=-8+1=-7\\
	&(M+N)_{2,1}=M_{2,1}+N_{2,1}=2+(-1)=1\quad&\quad&(M+N)_{2,2}=M_{2,2}+N_{2,2}=5+\pi\\
	&(M+N)_{3,1}=M_{3,1}+N_{3,1}=9+(-\pi)=9-\pi\quad&\quad&(M+N)_{3,2}=M_{3,2}+N_{3,2}=\pi+0=\pi
	\end{alignat*}and write
	\[M+N=\begin{pmatrix}
	1 & -7\\
	1 & 5+\pi\\
	9-\pi & \pi
	\end{pmatrix}.\]
	\tcblower
	Similarly, we can compute
	\begin{alignat*}{3}
	&(\pi M)_{1,1}=\pi\cdot 1=\pi\quad&\quad &(\pi M)_{1,2}=\pi\cdot (-8)=-8\pi\\
	&(\pi M)_{2,1}=\pi\cdot 2 = 2\pi\quad&\quad &(\pi M)_{2,2}=\pi\cdot 5 = 5\pi\\
	&(\pi M)_{3,1}=\pi\cdot 9 = 9\pi\quad&\quad 	&(\pi M)_{3,2}=\pi\cdot \pi=\pi^2
	\end{alignat*}and write
	\[\pi M=\begin{pmatrix}
	\pi & -8\pi\\
	2\pi & 5\pi\\
	9\pi &\pi^2
	\end{pmatrix}.\]
\end{ex}

\begin{exerc}
	Compute, using the data from the preceding example, $aN$ and $a(M+N)$. After that, compute $(M+M)+N$ and $2M+N$. What can you say about those last two results?
\end{exerc}

\subsection{Products(?)}

Now that we know how to add matrices and how to multiply them by numbers, it would be natural to ask if we can multiply matrices by other matrices. We could even, inspired by what we did before, define that given two matrices $M,N\in M_{n\times m}(\R)$, their product will be a matrix $M\times N$ given by
\[(M\times N)_{i,j}:=M_{i,j}\cdot N_{i,j}\]which would make sense, since for each pair of indices $i,j$, both $M_{i,j}$ and $N_{i,j}$ are real numbers (things we know how to multiply together!). Not only that, but this multiplication would have a lot of interesting properties: It would be commutative, associative, would have an inverse, a neutral element...

\underline{Then why don't we define the matrix multiplication like that?}

The simple answer is that even though this product seems counter-intuitive at first, as the course goes on we'll see that it's actually a very useful and natural notion of multiplication, even if not the most intuitive one.

\begin{df}[Classical Multiplication]\label{df:produto-classico}
	Given two matrices $M\in M_{n\times m}(\R)$ and $N\in M_{m\times l}(\R)$, we define the \textbf{product of $M$ and $N$} to be the matrix $MN$ given by
	\[(MN)_{i,j}:=M_{i,1}N_{1,j}+M_{i,2}N_{2,j}+\cdots+M_{i,m}N_{m,j}.\]
\end{df}

\begin{rmk}
	Once again, we can note that this definition makes total sense, since for each trio of indices $i$, $j$ and $k$, both $M_{i,k}$ and $N_{k,j}$ are realm numbers (things we know how to multiply together!) and so $M_{i,k}N_{k,j}$ is also a real number; and since $(MN)_{i,j}$ is just a sum of real numbers, $(MN)_{i,j}$ is, itself, a real number. Furthermore, this definition makes it clear why we need the amount of columns of $M$ to be the same as the amount of rows of $N$: that's exactly the amount of terms we're adding together for each entry.
\end{rmk}

That's the classical matrix multiplication we usually learn in high school. However, as we've already discussed, it does not ``make sense'' - it seems to simply come out of nowhere, and it's needlessly complicated.

We'll try and make this definition more intuitive now. Don't be scared, whatever we do next, the end result will be the same as doing whatever we did above. The idea is just to explain each step so this definition makes as least \textit{some} sense.

So, with that in mind, don't waste your time computing products as we're about to show, but, at the same time, try and understand the underlying reasoning behind everything that comes next.

\bigskip
\begin{df}
	We define $e_{i,j}^n\in M_n(\R)$ the be the matrix given by:
	\[(e_{i,j}^n)_{r,s}:=\begin{cases}
	1\text{ if } r=i \text{ and } s=j\\
	0\text{ otherwise }
	\end{cases}.\]
\end{df}

\begin{ex}
	The matrix $e^2_{1,2}\in M_{2}(\R)$ is just the $2\times 2$ matrix which has a $1$ in row 1, column 2, and a $0$ everywhere else, that is,\[e^2_{1,2}=\begin{pmatrix}
	0 & 1\\
	0 & 0
	\end{pmatrix}.\] On the other hand, the matrix $e^3_{1,2}\in M_3(\R)$ is the $3\times 3$ matrix which has a $1$ in row 1, column 2, and a $0$ everywhere else, that is, \[e^3_{1,2}=\begin{pmatrix}
	0 & 1&0\\
	0 & 0&0\\
	0&0&0
	\end{pmatrix}.\]
\end{ex}

Further ahead, we'll extend this definition to non-square matrices, but, for now, we're going to focus our efforts on understanding square matrices, since they are simpler and easier to understand.

\begin{df}
	We define by $E^n_{i,j}:M_{n\times m}(\R)\to M_{n\times m}(\R)$ the \textbf{function} given by:
	\[\bordermatrix{&&&&\cr
		&M_{1,1} & M_{1,2} & \cdots & M_{1,m}\cr
		& M_{2,1}& M_{2,2} & \cdots & M_{2,m}\cr
		& \vdots & \vdots & \ddots & \vdots\cr		
		j\text{-th row}& M_{j,1} & M_{j,2} & \cdots & M_{j,m}\cr
		& \vdots & \vdots & \ddots & \vdots\cr
		&M_{n,1} & M_{n,2} & \cdots & M_{n,m}	
	}\mapsto
	\bordermatrix{
	&&&&\cr
	&0&0&\cdots&0\cr
	&0&0&\cdots&0\cr
	& \vdots & \vdots & \ddots & \vdots\cr
	i\text{-th row}& M_{j,1} & M_{j,2} & \cdots & M_{j,m}\cr
	& \vdots & \vdots & \ddots & \vdots\cr
	&0&0&\cdots&0
	}.
	\]
\end{df}

That is, $E^n_{i,j}$ is the function that takes a matrix with $n$ rows into another matrix with $n$ rows, simply by copying its $j$-th row into the $i$-th row of the resulting matrix, and filling all the other entries with zeroes.

\begin{ex}
	The function $E^{3}_{1,2}$ applied to $M=\begin{pmatrix}
	1 & -8\\
	2 & 5\\
	9 & \pi
	\end{pmatrix}$ gives us the matrix \(E^3_{1,2}(M)=\bordermatrix{
	&&\cr
	& 2 & 5\cr
	& 0 &0\cr
	&0 &0
	}\) simply by copying the second row of $M$, pasting it in the first row of the new matrix and filling everything else with $0$.

	Analogously, if $N=\begin{pmatrix}
	0 & 1\\
	-1 & \pi\\
	-\pi & 0
	\end{pmatrix}$, then $E^3_{1,2}(N)=\begin{pmatrix}
	-1 & \pi\\
	0 & 0\\
	0 & 0
	\end{pmatrix}$ simply by copying the second row of $N$, pasting it in the first row of the new matrix and filling everything else with $0$.
\end{ex}

\begin{rmk}
	Here, the ``exponent'' of the function is simply there to show us the amount of rows our input matrices can have. In the examples above, the exponent $3$, for instance, simply indicates that $E^3_{1,2}$ can only be applied in matrices with precisely $3$ rows..
\end{rmk}
\begin{exerc}
	Using the data from the previous example, compute $E^3_{1,1}, E^3_{1,3}, E^3_{2,1}, E^3_{2,2}, E^3_{2,3},E^3_{3,1}, E^3_{3,2}$ and $E^3_{3,3}$ applied both to $M$ and $N$.
	
	What can you say about $E^3_{i,j}$ when $i=j$? In other words, compute $E^3_{1,1},E^3_{2,2}$ and $E^3_{3,3}$ applied to $M$ and $N$. What do the resulting matrices have in special?
\end{exerc}
\begin{exerc}
	Using the previous example, is there some pair of indices $i,j$ such that $E^3_{i,j}(E^3_{i,j}(M))=E^3_{i,j}(M)$? What about for $N$?
\end{exerc}

Finally we can get our first definition of matrix multiplication:

\begin{df}
	Given a matrix $M\in M_{n\times m}(\R)$ and $e^n_{i,j}\in M_n(\R)$, we define the \textbf{product of $e^n_{i,j}$ and $M$} to be the matrix $e^n_{i,j}M$ given by
	\[e^n_{i,j}M:=E^n_{i,j}(M).\]
\end{df}

\begin{ex}
	Still using the data from the previous example, we can compute
	\[\begin{pmatrix}
	0 & 1 & 0\\
	0&0&0\\
	0&0&0
	\end{pmatrix}\cdot\begin{pmatrix}
	1 & -8\\
	2 & 5\\
	9 &\pi
	\end{pmatrix}=e^3_{1,2}M:=E^3_{1,2}(M)=\begin{pmatrix}
	2 & 5\\0&0\\0&0
	\end{pmatrix}\]and
		\[\begin{pmatrix}
	0 & 1 & 0\\
	0&0&0\\
	0&0&0
	\end{pmatrix}\cdot\begin{pmatrix}
	0 & 1\\
	-1 & \pi\\
	-\pi &0
	\end{pmatrix}=e^3_{1,2}N:=E^3_{1,2}(N)=\begin{pmatrix}
	-1 &\pi\\0&0\\0&0
	\end{pmatrix}.\]
\end{ex}

\begin{rmk}
	Let us show, with a few examples, that our current working definition coincides with the classical definition \ref{df:produto-classico}:
	\[\begin{pmatrix}
	0 & 1 & 0\\
	0&0&0\\
	0&0&0
	\end{pmatrix}\cdot\begin{pmatrix}
	1 & -8\\
	2 & 5\\
	9 &\pi
	\end{pmatrix}=\begin{pmatrix}
	0\cdot 1+1\cdot 2+0\cdot 9 & 0\cdot(-8)+1\cdot 5+0\cdot\pi\\
	0\cdot1+0\cdot2+0\cdot 9 & 0\cdot(-8)+0\cdot 5+0\cdot \pi\\
	0\cdot1+0\cdot2+0\cdot 9 & 0\cdot(-8)+0\cdot 5+0\cdot \pi
	\end{pmatrix}=\begin{pmatrix}
	2 & 5\\0&0\\0&0
	\end{pmatrix}\]which is precisely the same result we get when applying $E^3_{1,2}$ to $M$.
\end{rmk}

\begin{exerc}
	Apply $E^4_{3,2}$ to a generic matrix $M=\begin{pmatrix}
	a & b\\
	c & d\\
	e & f\\
	g & h
	\end{pmatrix}\in M_{4\times 2}(\R)$. After that, compute $e^4_{3,2}M$ the usual way and compare the results.
\end{exerc}


\bigskip
The next step would be to consider matrices which are \textit{almost} $e^n_{i,j}$. For instance, how can we multiply $\begin{pmatrix}
0 & 8 & 0\\
0 & 0 & 0\\
0&0&0
\end{pmatrix}$ and $M=\begin{pmatrix}
1 & -8\\
2 & 5\\
9 & \pi
\end{pmatrix}$? Well, we can start by noticing that $\begin{pmatrix}
0 & 8 & 0\\
0 & 0 & 0\\
0&0&0
\end{pmatrix}=8\cdot\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 0\\
0&0&0
\end{pmatrix}=8\cdot e^3_{1,2}$ and that we know not only how to multiply numbers and matrices, but also $e^3_{1,2}$ and $M$.

Put another way, we want to compute $(8\cdot e^3_{1,2})\cdot M$ given that we already know how to compute $e^3_{1,2}\cdot M$. Since multiplication of real numbers is associative ($(ab)c=a(bc)$) it makes sense for us to wish that this new operation we're defining is also associative. So we can simply define
\[(8\cdot e^3_{1,2})\cdot M:=8\cdot(e^3_{1,2}\cdot M)\]which makes sense, since $8\in \R$, $e^3_{1,2}\cdot M\in M_{n\times m}(\R)$ and \textbf{we already know how to multiply numbers and matrices together}.

Let us define, then:

\begin{df}
	Given any real number $a\in\ R$, a real matrix $M\in M_{n\times m}(\R)$ and a square matrix $e^n_{i,j}\in M_n(\R)$, we define the \textbf{product between $ae^n_{ij}$ and $M$} to be the matrix $(ae^n_{i,j})M$ given by
	\[(ae^n_{i,j})M:=a(e^n_{i,j}M).\]
\end{df}

\begin{exerc}
	Using $M$ and $N$ from the previous example, compute $\begin{pmatrix}
	0 & 5 & 0\\
	0&0&0\\
	0&0&0
	\end{pmatrix}\cdot M$ and $\begin{pmatrix}
	0 & 0 & 0\\
	0&0&-\pi\\
	0&0&0
	\end{pmatrix}\cdot N$ using this new definition, and then compare the result with what you would've obtained using the usual definition.
\end{exerc}

\bigskip
Another kind of \textit{almost}-$e^n_{i,j}$ matrix are matrices which can be written as \textit{sums} of $e^n_{i,j}$ matrices. For instance, consider $A=\begin{pmatrix}
0 & 1 & 1\\
1 & 0 & 0\\
0 & 1 & 0
\end{pmatrix}$ which can be rewritten as
\[A=\begin{pmatrix}
0 & 1 & 1\\
1 & 0 & 0\\
0 & 1 & 0
\end{pmatrix}=\begin{pmatrix}
0 & 1 & 0\\
0&0&0\\
0&0&0
\end{pmatrix}+ \begin{pmatrix}
0 & 0 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{pmatrix}  + \begin{pmatrix}
0 & 0 & 0 \\
1 & 0 & 0 \\
0 & 0 & 0 
\end{pmatrix}  + \begin{pmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 1 & 0 
\end{pmatrix}  =e^3_{1,2}+e^3_{1,3}+e^3_{2,1}+e^3_{3,2}.\] Now, we want to learn how to multiply any matrix, such as $M=\begin{pmatrix}
1 & -8\\
2 & 5\\
9 & \pi
\end{pmatrix}$ by $A$, to obtain $A\cdot M$. Again, we're gonna use the fact that we know how to \textbf{add matrices}, \textbf{multiply matrices $e^3_{i,j}$ by $M$} and that we kno that \textbf{real number multiplication distributes over addition}. And so we can define,
\[AM=(e^3_{1,2}+e^3_{1,3}+e^3_{2,1}+e^3_{3,2})M:=e^3_{1,2}M+e^3_{1,3}M+e^3_{2,1}M+e^3_{3,2}M,\]and compute
\[e^3_{1,2}M=\begin{pmatrix}
2 & 5\\
0 & 0\\
0 & 0
\end{pmatrix},e^3_{1,3}M=\begin{pmatrix}
9 & \pi\\
0 & 0\\
0 & 0
\end{pmatrix},e^3_{2,1}M=\begin{pmatrix}
0 & 0\\
1 & -8\\
0 & 0
\end{pmatrix},e^3_{3,2}M=\begin{pmatrix}
0 & 0\\
9 & \pi\\
0 & 0
\end{pmatrix}\]to see that
\[AM=\begin{pmatrix}
2 & 5\\
0 & 0\\
0 & 0
\end{pmatrix}+\begin{pmatrix}
9 & \pi\\
0 & 0\\
0 & 0
\end{pmatrix}+\begin{pmatrix}
0 & 0\\
1 & -8\\
0 & 0
\end{pmatrix}+\begin{pmatrix}
0 & 0\\
9 & \pi\\
0 & 0
\end{pmatrix}=\begin{pmatrix}
11 & 5+\pi\\
10 & \pi-8\\
0 & 0
\end{pmatrix}\]

\begin{exerc}
	Compare, once again, this result with what you'd get by doing the usual matrix multiplication.
\end{exerc}

\begin{df}
	Given a collection $\{e^n_{i_1,j_1},e^n_{i_2,j_2},\cdots,e^n_{i_l,j_l}\}$ of $e_{i,j}^n$ matrices and any matrix $M\in M_{n\times m}(\R)$, we define the \textbf{product between the sum of all the $e_{i,j}^n$'s and $M$} to be the matrix $(e^n_{i_1,j_1}+e^n_{i_2,j_2}+\cdots+e^n_{i_l,j_l})M$ given by
	\[(e^n_{i_1,j_1}+e^n_{i_2,j_2}+\cdots+e^n_{i_l,j_l})M:=e^n_{i_1,j_1}M+e^n_{i_2,j_2}M+\cdots+e^n_{i_l,j_l}M.\]
\end{df}

\bigskip
Finally, let us combine the two definitions: Consider the matrix $A=\begin{pmatrix}
0 & 5 & 2\\
-7 & 0 & 0\\
0 & 10 & 0
\end{pmatrix}$ as well as the matrix $M=\begin{pmatrix}
1 & -8\\
2 & 5\\
9 & \pi
\end{pmatrix}$. How can we compute $AM$?

Well, we can first break $A$ as a sum of matrices \textit{almost}-$e^n_{i,j}$:
\[A=\begin{pmatrix}
0 & 5 & 2\\
-7 & 0 & 0\\
0 & 10 & 0
\end{pmatrix}=\begin{pmatrix}
0 & 5 & 0\\
0&0&0\\
0&0&0
\end{pmatrix}+ \begin{pmatrix}
0 & 0 & 2 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{pmatrix}  + \begin{pmatrix}
0 & 0 & 0 \\
-7 & 0 & 0 \\
0 & 0 & 0 
\end{pmatrix}  + \begin{pmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 10 & 0 
\end{pmatrix}\]and write each of these as a product of a number by a matrix $e^n_{i,j}$
\begin{alignat*}{7}
A=\begin{pmatrix}
0 & 5 & 2\\
-7 & 0 & 0\\
0 & 10 & 0
\end{pmatrix}&=&\begin{pmatrix}
0 & 5 & 0\\
0&0&0\\
0&0&0
\end{pmatrix}&+&\begin{pmatrix}
0 & 0 & 2 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{pmatrix}&+&\begin{pmatrix}
0 & 0 & 0 \\
-7 & 0 & 0 \\
0 & 0 & 0 
\end{pmatrix}&+&\begin{pmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 10 & 0 
\end{pmatrix}\\&=&5e^3_{1,2} \quad\ &+&2e^3_{1,3}\quad&+&(-7)e^3_{2,1}\quad&+&10e^3_{3,2}\quad
\end{alignat*}and finally, since we already know how to multiply each of these by $M$, we can define
\[AM=(5e^3_{1,2}+2e^3_{1,3}+(-7)e^3_{2,1}+10e^3_{3,2})M:=5e^3_{1,2}M+2e^3_{1,3}M+(-7)e^3_{2,1}M+10e^3_{3,2}M.\] Which gives us:
\begin{align*}
AM&=5\begin{pmatrix}
2 & 5\\
0 & 0\\
0 & 0
\end{pmatrix}+2\begin{pmatrix}
9 & \pi\\
0 & 0\\
0 & 0
\end{pmatrix}+(-7)\begin{pmatrix}
0 & 0\\
1 & -8\\
0 & 0
\end{pmatrix}+10\begin{pmatrix}
0 & 0\\
9 & \pi\\
0 & 0
\end{pmatrix}\\
&=\begin{pmatrix}
10 & 25\\
0 & 0\\
0 & 0
\end{pmatrix}+\begin{pmatrix}
18 & 2\pi\\
0 & 0\\
0 & 0
\end{pmatrix}+\begin{pmatrix}
0 & 0\\
-7 & 56\\
0 & 0
\end{pmatrix}+\begin{pmatrix}
0 & 0\\
90 & 10\pi\\
0 & 0
\end{pmatrix}=\begin{pmatrix}
28 & 25+2\pi\\
83 & 56+10\pi\\
0 & 0
\end{pmatrix}.
\end{align*}

\begin{rmk}
	Note that any matrix can be rewritten just as we did above - that is, by breaking it up as a sum of multiples of \textit{almost}-$e^n_{i,j}$ matrices.
\end{rmk}

And with this we can, at long last, define the product of any matrix by any square matrix:

\begin{df}
	Let $A\in M_n(\R)$ be a square matrix and $M\in M_{n\times m}(\R)$ any matrix. We define the \textbf{product of $A$ and $M$} as being the matrix $AM$ given by
	\begin{align*}
	AM&=(A_{1,1}e^n_{1,1}+A_{1,2}e^n_{1,2}+\cdots+A_{1,n}e^n_{1,n}+A_{2,1}e^n_{2,1}+\cdots A_{n,n}e^n_{n,n})M\\&:=A_{1,1}e^n_{1,1}M+A_{1,2}e^n_{1,2}M+\cdots+A_{1,n}e^n_{1,n}M+A_{2,1}e^n_{2,1}M+\cdots A_{n,n}e^n_{n,n}M
	\end{align*}
\end{df}

\begin{ex}
	Given a square matrix $A=\begin{pmatrix}
	a & b\\
	c & d
	\end{pmatrix}$ and any matrix $M=\begin{pmatrix}
a' &b' & c'\\
d' & e' & f'
\end{pmatrix}$, we get, by that definition, that $AM$ is just
\[\begin{pmatrix}
a & 0\\
0 & 0
\end{pmatrix}\begin{pmatrix}
a' &b' & c'\\
d' & e' & f'
\end{pmatrix}+\begin{pmatrix}
0 & b\\
0 & 0
\end{pmatrix}\begin{pmatrix}
a' &b' & c'\\
d' & e' & f'
\end{pmatrix}+\begin{pmatrix}
0 & 0\\
c & 0
\end{pmatrix}\begin{pmatrix}
a' &b' & c'\\
d' & e' & f'
\end{pmatrix}+\begin{pmatrix}
0 & 0\\
0 & d
\end{pmatrix}\begin{pmatrix}
a' &b' & c'\\
d' & e' & f'
\end{pmatrix}\]
\[=\begin{pmatrix}
aa' &ab' & ac'\\
0 & 0 & 0
\end{pmatrix}+\begin{pmatrix}
bd' & be' & bf'\\
0&0&0
\end{pmatrix}+\begin{pmatrix}
0&0&0\\
ca' &cb' & cc'
\end{pmatrix}+\begin{pmatrix}
0&0&0\\
dd' & de' & df'
\end{pmatrix}\]
\[=\begin{pmatrix}
aa'+bd' &ba'+be' & ac'+bf'\\
ca'+dd' & cb'+de' & cc'+df'
\end{pmatrix}.\]
\end{ex}

\begin{rmk}
	Em matemática, quando vamos escrever uma soma muito grande, ou com muitos termos, costumamos usar um símbolo especial para isso - $\sum$ - o \textbf{somatório}. Ele funciona da seguinte maneira: Ao invés de escrever $x_1+x_2+\cdots+x_n$ vamos escrever
	\[\sum_{i=1}^n x_i.\] A ideia é a seguinte: $i$ denota um índice variável, e os números $1$ e $n$ que aparecem abaixo e acima, respectivamente, do somatório indicam qual o valor mínimo e máximo que $i$ pode assumir, sempre variando de 1 em 1.
\end{rmk}

\begin{ex}
	Podemos escrever a soma de todos os números naturais entre 1 e $n$ usando \(\sum_{i=1}^ni,\) por exemplo,
	\[1+2+3+4=\sum_{i=1}^4i.\]
	
	Podemos escrever a soma dos quadrados dos números naturais entre 1 e $n$ como \(\sum_{i=1}^ni^2,\) por exemplo,
	\[1+4+9+16+25=1^2+2^2+3^2+4^2+5^2=\sum_{i=1}^5i^2.\]
	
	Podemos escrever a soma dos $n$ primeiros números ímpares e pares, respectivamente, como $\sum_{i=1}^n 2i-1\text{ e }\sum_{i=1}^n2i,$ por exemplo,
	\[1+3+5+7+9+11 = (2\cdot1-1)+(2\cdot2-1)+(2\cdot3-1)+(2\cdot3-1)+(2\cdot4-1)+(2\cdot5-1)+(2\cdot6-1) = \sum_{i=1}^62i-1\]\[2+4+6+8+10+12+14 = (2\cdot 1)+(2\cdot2)+(2\cdot3)+(2\cdot4)+(2\cdot5)+(2\cdot6)+(2\cdot7 )= \sum_{i=1}^72i\]
	
	Podemos escrever o produto clássico (\ref{df:produto-classico}) das matrizes $M\in M_{n,m}(\R)$ e $N\in M_{m,l}(\R)$ como $$(MN)_{i,j}=\sum_{k=1}^{m}M_{i,k}N_{k,j}.$$
\end{ex}

Com isso, por exemplo, podemos tornar a definição de produto mais compacta:
\begin{df}
	Seja $A\in M_n(\R)$ uma matriz quadrada e $M\in M_{n\times m}(\R)$ qualquer. Definimos o \textbf{produto de $A$ com $M$} como sendo a matriz $AM$ dada por
	\begin{align*}
	AM&=\left(\sum_{j=1}^n\sum_{i=1}^nA_{i,j}e^n_{i,j}\right)M\\&:=\sum_{j=1}^n\sum_{i=1}^n\left(A_{i,j}e^n_{i,j}M\right)
	\end{align*}
\end{df}

\bigskip

Finalmente, para encerrar esta seção, vamos aprender a multiplicar matrizes de tamanho qualquer, usando o raciocínio acima.

\begin{df}
	Vamos denotar por $e_{i,j}^{n,m}$ a matriz $n\times m$ dada por
	\[(e^{n,m}_{i,j})_{r,s}:=\begin{cases}
	1,\text{ se } r=i \text{ e } s=j\\
	0, \text{ caso contrário}.
	\end{cases}\]
\end{df}

Ou seja, é apenas uma generalização das nossas já familiares matrizes $e^n_{i,j}$ para matrizes não-quadradas com a mesma propriedade - elas têm 1 na entrada $i,j$ e 0 em todas as outras entradas.

Novamente, podemos definir agora funções que vão realizar nossas multiplicações:
\begin{df}
	Definimos por $E^{l,n}_{i,j}:M_{n\times m}(\R)\to M_{l\times m}(\R)$ a \textbf{função} dada por:
	\[\bordermatrix{&&&&\cr
		&M_{1,1} & M_{1,2} & \cdots & M_{1,m}\cr
		& M_{2,1}& M_{2,2} & \cdots & M_{2,m}\cr
		& \vdots & \vdots & \ddots & \vdots\cr		
		j\text{-ésima linha}& M_{j,1} & M_{j,2} & \cdots & M_{j,m}\cr
		& \vdots & \vdots & \ddots & \vdots\cr
		&M_{n,1} & M_{n,2} & \cdots & M_{n,m}	
	}\mapsto
	\bordermatrix{
		&&&&\cr
		&0&0&\cdots&0\cr
		&0&0&\cdots&0\cr
		& \vdots & \vdots & \ddots & \vdots\cr
		i\text{-ésima linha}& M_{j,1} & M_{j,2} & \cdots & M_{j,m}\cr
		& \vdots & \vdots & \ddots & \vdots\cr
		&0&0&\cdots&0
	}.
	\]
\end{df}

\begin{rmk}
	Assim como antes, o expoente em $E^{l,n}_{i,j}$ significa apenas que estamos transformando matrizes de $n$ linhas em matrizes de $l$ linhas.
\end{rmk}

\begin{ex}
	Dada uma matriz $M=\begin{pmatrix}
	5 & -8 & \sqrt 2\\
	4 & 4 & 0
	\end{pmatrix}\in M_{2\times 3}(\R)$, temos que
	\[E^{3,2}_{1,1}(M)=\begin{pmatrix}
	5 & -8 & \sqrt2\\
	0&0&0\\
	0&0&0
	\end{pmatrix}\quad\quad E^{3,2}_{1,2}(M)=\begin{pmatrix}
	4 & 4&0\\
	0&0&0\\
	0&0&0
	\end{pmatrix}\]
	\[E^{3,2}_{2,1}(M)=\begin{pmatrix}
	0&0&0\\
	5 & -8 & \sqrt2\\	
	0&0&0
	\end{pmatrix}\quad\quad E^{3,2}_{2,2}(M)=\begin{pmatrix}
	0&0&0\\
	4 & 4 & 0\\	
	0&0&0
	\end{pmatrix}\]
	\[E^{3,2}_{3,1}(M)=\begin{pmatrix}
	0&0&0\\
	0&0 & 0\\	
	5 & -8 & \sqrt2
	\end{pmatrix}\quad\quad E^{3,2}_{3,2}(M)=\begin{pmatrix}
	0&0&0\\
	0&0&0\\	
	4 & 4 & 0
	\end{pmatrix}\]
\end{ex}

Com isso, vamos simplesmente imitar o procedimento anterior:
\begin{df}
	Dada uma matriz $M\in M_{n\times m}(\R)$, $l\in \N$ e qualquer par de índices $i,j$ com $i\leq l$ e $j\leq n$, definimos o produto de $e^{l,n}_{i,j}$ com $M$ como sendo a matriz $e^{l,n}_{i,j}M$ dada por
	\[e^{l,n}_{i,j}M:=E^{l,n}_{i,j}(M).\]
\end{df}

\begin{ex}
	Continuando o exemplo acima, em que $M=\begin{pmatrix}
	5 & -8 & \sqrt 2\\
	4 & 4 & 0
	\end{pmatrix}\in M_{2\times 3}(\R)$, temos que
	\[\begin{pmatrix}
	1 & 0\\
	0&0\\
	0&0
	\end{pmatrix}\cdot \begin{pmatrix}
	5 & -8 & \sqrt 2\\
	4 & 4 & 0
	\end{pmatrix}=e^{3,2}_{1,1}M:=E^{3,2}_{1,1}(M)=\begin{pmatrix}
	5 & -8 & \sqrt2\\
	0&0&0\\
	0&0&0
	\end{pmatrix}\]
	\[\begin{pmatrix}
	0&1\\
	0&0\\
	0&0
	\end{pmatrix}\cdot \begin{pmatrix}
	5 & -8 & \sqrt 2\\
	4 & 4 & 0
	\end{pmatrix}=e^{3,2}_{1,2}M:=E^{3,2}_{1,2}(M)=\begin{pmatrix}
	4 & 4&0\\
	0&0&0\\
	0&0&0
	\end{pmatrix}\]
	\[\begin{pmatrix}
	0&0\\
	1&0\\
	0&0
	\end{pmatrix}\cdot \begin{pmatrix}
	5 & -8 & \sqrt 2\\
	4 & 4 & 0
	\end{pmatrix}=e^{3,2}_{2,1}M:=E^{3,2}_{2,1}(M)=\begin{pmatrix}
	0&0&0\\
	5 & -8 & \sqrt2\\	
	0&0&0
	\end{pmatrix}\]
	\[\begin{pmatrix}
	0 & 0\\
	0&1\\
	0&0
	\end{pmatrix}\cdot \begin{pmatrix}
	5 & -8 & \sqrt 2\\
	4 & 4 & 0
	\end{pmatrix}=e^{3,2}_{1,1}M:=E^{3,2}_{2,2}(M)=\begin{pmatrix}
	0&0&0\\
	4 & 4 & 0\\	
	0&0&0
	\end{pmatrix}\]
	\[\begin{pmatrix}
	0&0\\
	0&0\\
	1&0
	\end{pmatrix}\cdot \begin{pmatrix}
	5 & -8 & \sqrt 2\\
	4 & 4 & 0
	\end{pmatrix}=e^{3,2}_{3,1}M:=E^{3,2}_{3,1}(M)=\begin{pmatrix}
	0&0&0\\
	0&0&0\\	
	5 & -8 & \sqrt2
	\end{pmatrix}\]
	\[\begin{pmatrix}
	0&0\\
	0&0\\
	0&1
	\end{pmatrix}\cdot \begin{pmatrix}
	5 & -8 & \sqrt 2\\
	4 & 4 & 0
	\end{pmatrix}=e^{3,2}_{3,2}M:=E^{3,2}_{3,2}(M)=\begin{pmatrix}
	0&0&0\\
	0&0&0\\	
	4 & 4 & 0
	\end{pmatrix}\]
\end{ex}

\begin{exerc}
	Essa multiplicação coincide com a multiplicação clássica de matrizes? Se sim, você consegue mostrar que isso sempre é verdade? Se não, você consegue mostrar um caso onde falha?
\end{exerc}

Feito isso, vamos agora simplesmente repetir o que fizemos antes acima para ensinar a multiplicar matrizes de tamanho ``qualquer''.

\begin{ex}
	Considere as matrizes $M=\begin{pmatrix}
	a&b&c\\
	d&e&f
	\end{pmatrix}\in M_{2\times 3}(\R)$ e $N=\begin{pmatrix}
	\alpha &\beta\\
	\gamma & \delta\\
	\epsilon & \eta	
	\end{pmatrix}\in M_{3\times 2}(\R)$. Para calcular $MN$ podemos fazer como fizemos com as matrizes quadradas:
	
	Primeiro, vamos decompor $M$ em matrizes $e^{l,n}_{i,j}$:
	
	\begin{align*}
	M&=\begin{pmatrix}
	a&b&c\\
	d&e&f
	\end{pmatrix}\\
	&=\begin{pmatrix}
	a&0&0\\
	0&0&0
	\end{pmatrix}+
	\begin{pmatrix}
	0&b&0\\
	0&0&0
	\end{pmatrix}+
	\begin{pmatrix}
	0&0&c\\
	0&0&0
	\end{pmatrix}+
	\begin{pmatrix}
	0&0&0\\
	d&0&0
	\end{pmatrix}+
	\begin{pmatrix}
	0&0&0\\
	0&e&0
	\end{pmatrix}+
	\begin{pmatrix}
	0&0&0\\
	0&0&f
	\end{pmatrix}\\
	&=a\begin{pmatrix}
	1&0&0\\
	0&0&0
	\end{pmatrix}+b
	\begin{pmatrix}
	0&1&0\\
	0&0&0
	\end{pmatrix}+c
	\begin{pmatrix}
	0&0&1\\
	0&0&0
	\end{pmatrix}+d
	\begin{pmatrix}
	0&0&0\\
	1&0&0
	\end{pmatrix}+e
	\begin{pmatrix}
	0&0&0\\
	0&1&0
	\end{pmatrix}+f
	\begin{pmatrix}
	0&0&0\\
	0&0&1
	\end{pmatrix}
	\end{align*}e cada uma dessas matrizes $e^{l,n}_{i,j}$ sabemos multiplicar por $N$:
	\begin{align*}
		\begin{pmatrix}
		1&0&0\\
		0&0&0
		\end{pmatrix}\begin{pmatrix}
		\alpha &\beta\\
		\gamma & \delta\\
		\epsilon & \eta	
		\end{pmatrix}:=\begin{pmatrix}
		\alpha &\beta\\
		0 & 0	
		\end{pmatrix}\quad\quad&\begin{pmatrix}
		0&0&0\\
		1&0&0
		\end{pmatrix}\begin{pmatrix}
		\alpha &\beta\\
		\gamma & \delta\\
		\epsilon & \eta	
		\end{pmatrix}:=\begin{pmatrix}
		0 & 0\\
		\alpha &\beta
		\end{pmatrix}\\		
		\begin{pmatrix}
		0&1&0\\
		0&0&0
		\end{pmatrix}\begin{pmatrix}
		\alpha &\beta\\
		\gamma & \delta\\
		\epsilon & \eta	
		\end{pmatrix}:=\begin{pmatrix}
		\gamma &\delta\\
		0 & 0	
		\end{pmatrix}\quad\quad&\begin{pmatrix}
		0&0&0\\
		0&1&0
		\end{pmatrix}\begin{pmatrix}
		\alpha &\beta\\
		\gamma & \delta\\
		\epsilon & \eta	
		\end{pmatrix}:=\begin{pmatrix}
		0 & 0\\
		\gamma &\delta
		\end{pmatrix}\\		
		\begin{pmatrix}
		0&0&1\\
		0&0&0
		\end{pmatrix}\begin{pmatrix}
		\alpha &\beta\\
		\gamma & \delta\\
		\epsilon & \eta	
		\end{pmatrix}:=\begin{pmatrix}
		\epsilon &\eta\\
		0 & 0	
		\end{pmatrix}\quad\quad&\begin{pmatrix}
		0&0&0\\
		0&0&1
		\end{pmatrix}\begin{pmatrix}
		\alpha &\beta\\
		\gamma & \delta\\
		\epsilon & \eta	
		\end{pmatrix}:=\begin{pmatrix}
		0 & 0\\
		\epsilon &\eta
		\end{pmatrix}
	\end{align*}e cada uma dessas matrizes sabemos multiplicar por números:
	\begin{align*}
	a\begin{pmatrix}
	\alpha &\beta\\
	0 & 0	
	\end{pmatrix}=\begin{pmatrix}
	a\alpha &a\beta\\
	0 & 0	
	\end{pmatrix} \quad\quad& d\begin{pmatrix}
	0 & 0\\
	\alpha &\beta
	\end{pmatrix}=\begin{pmatrix}
	0 & 0\\
	d\alpha&d\beta
	\end{pmatrix}\\
	b\begin{pmatrix}
	\gamma &\delta\\
	0 & 0	
	\end{pmatrix}=\begin{pmatrix}
	b\gamma &b\delta\\
	0 & 0	
	\end{pmatrix} \quad\quad& e\begin{pmatrix}
	0 & 0\\
	\gamma &\delta
	\end{pmatrix}=\begin{pmatrix}
	0 & 0\\
	e\gamma &e\delta
	\end{pmatrix}\\
	c\begin{pmatrix}
	\epsilon &\eta\\
	0 & 0	
	\end{pmatrix}=\begin{pmatrix}
	c\epsilon &c\eta\\
	0 & 0	
	\end{pmatrix} \quad\quad& f\begin{pmatrix}
	0 & 0\\
	\epsilon &\eta
	\end{pmatrix}=\begin{pmatrix}
	0 & 0\\
	f\epsilon &f\eta
	\end{pmatrix}
	\end{align*}e como todas têm o mesmo tamanho, nós sabemos somar todas elas:
	\begin{gather*}	
	\begin{pmatrix}
	a\alpha &a\beta\\
	0 & 0	
	\end{pmatrix}+\begin{pmatrix}
	b\gamma &b\delta\\
	0 & 0	
	\end{pmatrix}+\begin{pmatrix}
	c\epsilon &c\eta\\
	0 & 0	
	\end{pmatrix}+\begin{pmatrix}
	0 & 0\\
	d\alpha&d\beta
	\end{pmatrix}+\begin{pmatrix}
	0 & 0\\
	e\gamma &e\delta
	\end{pmatrix}+\begin{pmatrix}
	0 & 0\\
	f\epsilon &f\eta
	\end{pmatrix}\\
	=\begin{pmatrix}
	a\alpha+b\gamma+c\epsilon & a\beta+b\delta+c\eta\\
	d\alpha+e\gamma+f\epsilon & d\beta+e\delta+f\eta
	\end{pmatrix}
	\end{gather*}e finalmente, vemos chamar esse resultado de $MN$.
\end{ex}

Finalmente podemos definir o produto de matrizes compatíveis:
\begin{df}
	Sejam $M\in M_{n\times m}(\R)$ e $N\in M_{m\times l}(\R)$. Definimos o produto de $M$ com $N$ como sendo a matriz $MN$ dada por
	\[MN:=\sum_{j=1}^m\sum_{i=1}^n\left(M_{i,j}e^{n,m}_{i,j}N\right)\]
\end{df}

\begin{exerc}
	Calcule, usando os dados do exemplo acima, $NM$, usando o método que preferir (isto é, o método clássico, ou o que estamos definindo) e compare o resultado que você obtiver com o $MN$ calculado acima.
\end{exerc}
\begin{exerc}
	Sejam $A$ e $B$ matrizes quaisquer, tais que ambos os produtos $AB$ e $BA$ existem. Isso significa que $AB=BA$? Se sim, tente provar por que isso é verdade. Se não, dê um contra-exemplo.
\end{exerc}

\begin{exerc}
	Tente provar as seguintes propriedades:
	\begin{itemize}
		\item[] \textbf{(Comutatividade da soma)} Para quaisquer duas matrizes $M,N\in M_{n\times m}(\R)$ temos que $M+N=N+M$;
		\item[] \textbf{(Elemento neutro da soma)} Para qualquer matriz $M\in M_{n\times m}(\R)$ existe uma única matriz $Z\in M_{n\times m}(\R)$ tal que $M+Z=M$. Vamos chamar essa matriz de \textbf{zero} e notar por 0;
		\item[] \textbf{(Inverso da soma)} Para qualquer matriz $M\in M_{n\times m}(\R)$ existe uma única matriz $N\in M_{n\times m}(\R)$ tal que $M+N=0$. Vamos chamar essa matriz de \textbf{inversa  aditiva de $M$} e notar por $-M$;
		\item[] \textbf{(Associatividade da soma) }Para quaisquer três matrizes $L,M,N\in M_{n\times m}(\R)$ temos que $L+(M+N)=(L+M)+N$;
		\item[] \textbf{(Comutatividade do produto por número) }Para qualquer número real $a\in \R$ e qualquer matriz $M\in M_{n\times m}(\R)$ temos que $aM=Ma$;
		\item[] \textbf{(Elemento neutro do produto por número)} Para qualquer matriz $M\in M_{n\times m}(\R)$ existe um único número $u\in \R$ tal que $uM=M$; 
		\item[] \textbf{(Associatividade do produto por número)} Para quaisquer dois números $a,b\in \R$ e qualquer matriz $M\in M_{n\times m}(\R)$, temos que $a(bM)=(ab)M$;
		\item[] \textbf{(Distributividade do produto por número sobre a soma)} Para quaisquer duas matrizes $M,N\in M_{n\times m}(\R)$ e qualquer número real $a\in \R$ temos que $a(M+N)=aM+aN$;
		\item[] \textbf{(Distributividade do produto por número sobre a soma de números)} Para quaisquer dois números reais $a,b\in \R$ e qualquer matriz $M\in M_{n\times m}(\R)$ temos que $(a+b)M=aM+bM$;
		\item[] \textbf{(Elemento neutro do produto)} Para qualquer matriz $M\in M_{n\times m}(\R)$ existe uma única matriz $I\in M_m(\R)$ tal que $MI=M=IM$. Vamos chamar essa matriz de \textbf{identidade};
		\item[] \textbf{(Associatividade do produto)} Para quaisquer três matrizes $L\in M_{n\times m}(\R)$, $M\in M_{m\times l}(\R)$ e $N\in M_{l\times k}(\R)$ temos que $(LM)N=L(MN)$;
		\item[] \textbf{(Associatividade do produto com produto por números)} Para quaisquer duas matrizes $M\in M_{n\times m}(\R)$ e $N\in M_{m\times l}(\R)$ e qualquer número real $a\in \R$ temos que $a(MN)=(aM)N$;
		\item[] \textbf{(Distributividade do produto sobre a soma)} Para quaisquer duas matrizes $L,M\in M_{n\times m}(\R)$ qualquer matriz $N\in M_{m\times l}(\R)$ e qualquer matriz $K\in M_{o\times n}(\R)$ temos que $K(L+M)=KL+KM$ e $(L+M)N=LN+MN$.
	\end{itemize}
\end{exerc}
\pagebreak

\section{Sistemas Lineares}

Vamos agora tentar dar uma primeira justificativa para o estudo de matrizes reais: A resolução de sistemas lineares.

\begin{df}
	Um \textbf{sistema linear de $n$ equações em $m$ variáveis} consiste em uma coleção de $n$ equações, em que a $i$-ésima equação é da forma $a_{i,1}x_1+a_{i,2}x_2+\cdots+a_{i,m}x_m=b_i$, em que $\{a_{i,j}\}_{i,j\in \N}$ e $\{b_i\}_{i\in \N}$ são números reais. Vamos denotar sistemas dessa forma por
	\[
	\begin{cases}
	a_{1,1}x_1+a_{1,2}x_2+&\cdots\quad+a_{1,m}x_m=b_1\\
	a_{2,1}x_1+a_{2,2}x_2+&\cdots\quad+a_{2,m}x_m=b_2\\
	&\ \vdots\\
	a_{n,1}x_1+a_{n,2}x_2+&\cdots\quad+a_{n,m}x_m=b_n
	\end{cases}
	\]
\end{df}

\begin{rmk}
	A motivação por trás de chamarmos tais sistemas de \textbf{lineares} ficará para um capítulo posterior.
\end{rmk}

\begin{ex}
	Considere o sistema de equações
	\[\begin{cases}
	5x+4y-7z=0\\
	3x+y-z=9.
	\end{cases}\] Ele é composto de duas equações ($5x+4y-7z=0$ e $3x+y-z=9$), ambas são lineares e ambas têm três variáveis ($x$, $y$ e $z$). Assim, o sistema acima é um sistema linear de duas equações e três incógnitas.
	
	\tcblower
	
	Contudo, o sistema
	\[
	\begin{cases}
	8x+y+z=2\\
	-4x+y+z^2=4
	\end{cases}
	\]não é linear. Ele é composto de duas equações e três variáveis - contudo, a segunda equação \textbf{não é linear}.
\end{ex}

Em outras palavras, sistemas lineares são sistemas de equações onde só aparecem somas de números multiplicados a variáveis - nada de funções trigonométricas, nada de potências, nada de funções exponenciais ou qualquer outro tipo de funções. Apenas multiplicação por números.

Se voltarmos para a definição de sistemas lineares de $n$ equações e $m$ incógnitas, vamos ver que cada equação tem exatamente $m$ números (chamados coeficientes) multiplicando as variáveis. Assim, como temos $n$ equações, temos $n\times m$ coeficientes no total. Isso sugere que podemos pegar um sistema
\[
\begin{cases}
a_{1,1}x_1+a_{1,2}x_2+&\cdots\quad+a_{1,m}x_m=b_1\\
a_{2,1}x_1+a_{2,2}x_2+&\cdots\quad+a_{2,m}x_m=b_2\\
&\ \vdots\\
a_{n,1}x_1+a_{n,2}x_2+&\cdots\quad+a_{n,m}x_m=b_n
\end{cases}
\]e representá-lo por uma matriz, da seguinte forma:

Primeiro, temos uma matriz
\[A=\begin{pmatrix}
a_{1,1} & a_{1,2} & \cdots & a_{1,m}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,m}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,m}
\end{pmatrix}\]chamada de \textbf{matriz de coeficientes} que faz exatamente o que o nome diz - coleta todos os coeficientes do sistema.

Em seguida, temos uma matriz
\[X=\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_m
\end{pmatrix}\]chamada de \textbf{matriz de variáveis} que, novamente, é auto-descritiva: ela contém todas as variáveis do sistema.

Por fim, temos uma matriz
\[B=\begin{pmatrix}
b_1\\
b_2\\
\vdots\\
b_n
\end{pmatrix}\] chamada de \textbf{matriz resultante} que, mais uma vez, é simplesmente a matriz que contém todos os resultados do sistema.

Por que isso é legal? Bom, primeiro vamos notar que $A\in M_{n\times m}(\R)$, $X$ é uma matriz $m\times 1$ com entradas que são variáveis, e que $B\in M_{n,1}(\R)$ . Agora, lembrando da multiplicação de matrizes, não é difícil ver que podemos multiplicar $AX$. Mas o que seria, por exemplo, o elemento na posição $i,j$ de $AX$? Bom, por definição,

\[(AX)_{i,j}=\sum_{l=1}^m A_{i,l}X_{l,j}=\sum_{l=1}^m a_{i,l}X_{l,j}\]e como $AX\in M_{n,1}(\R)$, $j=1$, de forma que podemos continuar:
\[(AX)_{i,j}=(AX)_{i,1}=\sum_{l=1}^m a_{i,l}X_{l,j}=\sum_{l=1}^m a_{i,l}X_{l,1}=\sum_{l=1}^m a_{i,l}x_l,\]e expandindo temos
\[(AX)_{i,1}=a_{i,1}x_1+a_{i,2}x_2+\cdots+a_{i,m}x_m\]que é exatamente $b_i$. Mas $b_i=B_{i,1}$. Disso temos que $(AX)_{i,1}=B_{i,1}$ - ou seja, as matrizes $AX$ e $B$ são iguais em \textbf{todas} as entradas e, portanto, são \textbf{iguais}.

Em outras palavras, ao escrever um sistema usando as matrizes $A$, $X$ e $B$, vemos que o sistema pode ser descrito como a igualdade de matrizes $AX=B$. Ou, dito de outra maneira - se $A'\in M_{n\times m}(\R)$, $X'$ é uma matriz $m\times 1$ com entradas que são variáveis, e $B'\in M_{n,1}(\R)$ são matrizes quaisquer, então uma equação matricial $A'X'=B'$ determina um único sistema linear.

\subsection{Escalonamentos}

Contudo, resolver sistemas lineares não é tarefa fácil. Vamos aqui apresentar uma técnica - chamada \textbf{escalonamento} - para resolver (se possível!) um sistema linear. Antes disso, porém, precisamos entender o que significa \textit{resolver um sistema linear}.

\begin{df}
	Dado um sistema linear $AX=B$, dizemos que \textbf{$C$ é uma solução para o sistema} se $AC=B$.
\end{df}

\begin{ex}
	Dado o sistema linear
	\[\begin{pmatrix}
	1 & 1 & 1\\
	0 & 1 & 1\\
	0 & 0 & 1
	\end{pmatrix}\begin{pmatrix}
	x\\
	y\\
	z
	\end{pmatrix}=\begin{pmatrix}
	6\\
	3\\
	1
	\end{pmatrix},\] temos que uma solução do sistema é a matriz
	\[\begin{pmatrix}
	3\\
	2\\
	1
	\end{pmatrix}.\]
	
	De fato,
	\[\begin{pmatrix}
	1 & 1 & 1\\
	0 & 1 & 1\\
	0 & 0 & 1
	\end{pmatrix}\begin{pmatrix}
	3\\
	2\\
	1
	\end{pmatrix}=\begin{pmatrix}
	3+2+1\\
	2+1\\
	1
	\end{pmatrix}=\begin{pmatrix}
	6\\
	3\\
	1
	\end{pmatrix}.\]
	
	Afirmamos ainda que essa solução é única.
	
	De fato, suponha que temos alguma matrix $\begin{pmatrix}
	a\\b\\c
	\end{pmatrix}$ tal que 
	\[\begin{pmatrix}
	1 & 1 & 1\\
	0 & 1 & 1\\
	0 & 0 & 1
	\end{pmatrix}\begin{pmatrix}
	a\\b\\c
	\end{pmatrix}=\begin{pmatrix}
	6\\
	3\\
	1
	\end{pmatrix}.\] Então teríamos três equações: $a+b+c=6$, $b+c=3$ e $c=1$ Como $c=1$ e $b+c=3$, segue que $b=2$. Similarmente, como $b+c=3$ e $a+b+c=6$, segue que $a=3$ -  ou seja, a matriz dada é, de fato, a única solução do sistema.
\end{ex}

\begin{exerc}
	Encontre uma solução do sistema
	\[\begin{pmatrix}
	1 & 0 & 0\\ 1&1&0\\1&1&1
	\end{pmatrix}\begin{pmatrix}
	x\\y\\z
	\end{pmatrix}=\begin{pmatrix}
	6\\13\\21
	\end{pmatrix}.\] Essa solução é única? Se não, encontre outra. Se sim, prove.
\end{exerc}

Vamos agora lembrar de como resolvíamos sistemas de equações lineares antes:

\begin{ex}
	Considere o sistema
	\[\begin{cases}
	4x+2y=0\\
	-4x+9y=22
	\end{cases}.\] Nós podemos perceber que o coeficiente de $x$ em ambas as equações é o mesmo, com o sinal invertido. Isso nos diz que se somarmos as equações, obteremos uma nova equação com o coeficiente de $x$ sendo 0
	\[
	\begin{array}[b]{lr}
	&4x+2y=0\\
	+&-4x+9y=22\\
	\hline
	&0x+11y=22
	\end{array}
	\]com isso, conseguimos resolver a equação $11y=22$, que tem como única solução $y=2$. Agora, com o valor de $y$ em mãos, podemos escolher qualquer equação original e resolvê-la - por exemplo a primeira
	\[\begin{array}{rl}
		4x+2y=0&\\
		4x+2\cdot2=0 & \text{(usando }y=2\text{)}\\
		4x+4=0&\\
		4(x+1)=0 & \text{(colocando 4 em evidência)}\\
		x+1=0 &\\
		x=-1,
	\end{array}\] e agora afirmamos que a solução do sistema original é $(-1,2)$.
	
	O que fizemos, em suma, foi trocar o sistema
	\[\begin{cases}
	4x+2y=0\\
	-4x+9y=22
	\end{cases}\]pelo sistema
	\[
	\begin{cases}
	4x+2y=0\\
	11y=22,
	\end{cases}\]resolver esse segundo sistema e afirmar que essa solução obtida também é solução do sistema original.
	
	\tcblower
	
	Pensando do ponto de vista de matrizes, vamos chamar
	\[A=\begin{pmatrix}
	4 &2\\-4 &9
	\end{pmatrix}\]a matriz de coeficientes do sistema original e 
	\[A'=\begin{pmatrix}
	4 & 2\\ 0 & 11
	\end{pmatrix}\]a matriz de coeficientes do sistema que obtemos somando as duas equações do sistema original.
	
	Mas como obtivemos a matriz $A'$? Vamos analisar ela por partes:
	
	\begin{itemize}
		\item A linha 1 de $A'$ é simplesmente a linha 1 de $A$;
		\item A linha 2 de $A'$ é simplesmente a soma das linhas 1 e 2 de $A$.
	\end{itemize}

Ou seja, se escrevermos
\[A'=\begin{pmatrix}
4 & 2\\0&0
\end{pmatrix}+\begin{pmatrix}
0&0\\0&11
\end{pmatrix}\]podemos ver que $\left(\begin{smallmatrix}
4&2\\0&0
\end{smallmatrix}\right)=e^2_{1,1}A$. Além disso, como já dissemos, 
\[\begin{pmatrix}
0&0\\0&11
\end{pmatrix}=\begin{pmatrix}
0&0\\4&2
\end{pmatrix}+\begin{pmatrix}
0&0\\-4&9
\end{pmatrix}=e^2_{2,1}A+e^2_{2,2}A,\]ou seja,
\[A'=\begin{pmatrix}
4 & 2\\0&0
\end{pmatrix}+\begin{pmatrix}
0&0\\0&11
\end{pmatrix}=e^2_{1,1}A+e^2_{2,1}A+e^2_{2,2}A=(e^2_{1,1}+e^2_{2,1}+e^2_{2,2})A\]que é simplesmente
\[A'=\begin{pmatrix}
1 & 0\\1 & 1
\end{pmatrix}A.\]Em outras palavras, nós multiplicamos $A$ por uma soma de matrizes $e^n_{i,j}$ e obtivemos uma matriz $A'$ com as mesmas soluções de $A$.
\end{ex}
\begin{ex}
	Similarmente ao exemplo anterior, considere o sistema linear
	\[\begin{cases}
	-7x+24y=8\\
	x-3y=1
	\end{cases}.\]Para resolvê-lo, geralmente somaríamos a linha 1 com sete vezes a linha 2:
	\[
	\begin{array}[b]{lr}
	&-7x+24y=8\\
	+&7(x-3y=1)\\
	\hline
	&0x+3y=15
	\end{array}
	\]e repetindo acima, podemos encontrar $y=5$. Agora, com $y$ em mãos, podemos escolher alguma das equações originais (por exemplo, a segunda) e resolvê-la, obtendo $x=16$, e, portanto, a solução é o par $(16,5)$.
	
	\tcblower
	
	Novamente, temos duas matrizes: A matriz do sistema original
	\[A=\begin{pmatrix}
	-7 & 24\\1&-3
	\end{pmatrix}\]e a matriz que obtivemos somando a primeira linha com sete vezes a segunda linha
	\[A'=\begin{pmatrix}
	1&-3\\0&3
	\end{pmatrix}.\]Será que conseguimos expressar $A'$ como um produto de alguma matriz por $A$ - como fizemos antes?
	
	Vamos começar desmembrando $A'$:
	\[A'=\begin{pmatrix}
	1 & -3 \\0&0
	\end{pmatrix}+\begin{pmatrix}
	0&0\\0&3
	\end{pmatrix}\]em que o primeiro pedaço não é nada mais que uma cópia da segunda linha de $A$ - ou seja, $e^2_{1,2}A$ - e o segundo pedaço é a soma da primeira linha de $A$ com sete vezes a segunda linha de $A$ - ou seja, 
	\[\begin{pmatrix}
	0&0\\0&3
	\end{pmatrix}=\begin{pmatrix}
	0&0\\-7&24
	\end{pmatrix}+7\begin{pmatrix}
	0&0\\1&-3
	\end{pmatrix}=e^2_{2,1}A+7e^2_{2,2}A\]e, portanto, 
	\[A'=e^2_{1,2}A+e^2_{2,1}A+7e^2_{2,2}A=(e^2_{1,2}+e^2_{2,1}+7e^2_{2,2})A,\]ou seja,
	\[A'=\begin{pmatrix}
	0&1\\
	1 & 7
	\end{pmatrix}A,\]como queríamos.
	
	Neste caso, multiplicamos $A$ por uma matriz que era soma de múltiplos de matrizes $e^n_{i,j}$ e ainda obtivemos uma matriz que tem as mesmas soluções.
\end{ex}

\begin{rmk}
	Cuidado! Nem toda multiplicação de $A$ por matrizes $e^n_{i,j}$ preserva soluções. Por exemplo, fazendo $\begin{pmatrix}
	0&0\\1&1
	\end{pmatrix}$ vezes $A=\begin{pmatrix}
	-7 & 24\\1&-3
	\end{pmatrix}$ obtemos a matriz $\begin{pmatrix}
	1 & -3\\1&-3
	\end{pmatrix}$ que claramente não tem as mesmas soluções que $A$.
	
	Em breve veremos condições para que isso não aconteça.
\end{rmk}

Mais à frente veremos exatamente \textit{porquê} isso é verdade. Por agora, nos basta focar nessas operações.

\begin{df}
	Um \textbf{escalonamento} de uma matriz $A\in M_{n\times m}(\R)$ qualquer, é uma matriz $A'$ que pode ser obtida de $A$ por composições das seguintes operações:
	\begin{itemize}
		\item Somar uma linha de $A$ a um múltiplo de outra linha de $A$;
		\item Trocar duas linhas de $A$;
		\item Multiplicar uma linha inteira de $A$ por um mesmo número.
	\end{itemize}
\end{df}

Pelos exemplos acima, podemos ver que sempre podemos realizar escalonamentos de $A$ por multiplicações $XA$, em que $X$ é uma soma de múltiplos de matrizes $e^n_{i,j}$.

\begin{exerc}
	Construa uma matriz $X\in M_2(\R)$ diferente de 0 que seja soma de matrizes $e^2_{i,j}$ tal que $XA$ \textbf{não} seja um escalonamento de $A$ (ou seja, não tenha as mesmas soluções de $A$) para qualquer matriz $A\in M_{2\times m}(\R)$.
\end{exerc}

Finalmente, podemos enunciar o teorema mais forte desta seção:

\begin{theorem}\label{thm:sol-escsol}
	Uma matriz $A$ possui solução se, e somente se, todo escalonamento de $A$ possui solução.
\end{theorem}

Não vamos demonstrar esse teorema agora, mas vamos chamar a atenção para o seguinte corolário:

\begin{cor}\label{cor:sol-id}
	Se, a matriz identidade é um escalonamento de $A$, então $A$ possui solução única.
\end{cor}

\begin{rmk}
	Em matemática, um \textbf{corolário} é um resultado que segue imediatamente de um resultado anterior. Então estamos afirmando que o
	\Cref{cor:sol-id} segue imediatamente do \Cref{thm:sol-escsol}.
\end{rmk}

\begin{exerc}
	Prove o \Cref{cor:sol-id} assumindo que o \Cref{thm:sol-escsol} seja verdade.
\end{exerc}

\bigskip
\subsection{Resolvendo sistemas lineares}

Vamos finalmente aprender a obter soluções usando escalonamentos. Para isso, considere o exemplo abaixo:

\begin{ex}
	Sejam $$A=\begin{pmatrix}
	1&1\\1&1\\1&-1\\1&-1
	\end{pmatrix},\quad B=\begin{pmatrix}
	1\\-1\\1\\-1
	\end{pmatrix}$$ e $AX=B$ um sistema linear. Para escalonar $A$ vamos fazer:
	\begin{alignat*}{4}
	\begin{pmatrix}
	1&1\\1&1\\1&-1\\1&-1
	\end{pmatrix}\quad&\rightsquigarrow\quad\bordermatrix{&&\cr&1&1\cr l_2-l_1&0&0\cr l_3-l_1&0&-2\cr l_4-l_1&0&-2}\quad&\rightsquigarrow\quad\bordermatrix{&&\cr&1&1\cr&0&0\cr\frac{l_3}{-2}&0&1\cr&0&-2}\quad&\rightsquigarrow\quad\bordermatrix{
	&&\cr l_1-l_3&1&0\cr &0&0\cr &0&1\cr l_4+2l_3 & 0 & 0
	}
	\end{alignat*}no primeiro passo multiplicamos $A$ por $P_1=\begin{pmatrix}
	1&0&0&0\\-1&1&0&0\\-1&0&1&0\\-1&0&0&1
	\end{pmatrix}$, no segundo por $P_2=\begin{pmatrix}
	1&0&0&0\\0&1&0&0\\0&0&\frac{1}{-2}&0\\0&0&0&1
	\end{pmatrix}$ e no terceiro por $P_3=\begin{pmatrix}
	1&0&-1&0\\0&1&0&0\\0&0&1&0\\0&0&2&1
	\end{pmatrix}$, obtendo a matrix escalonada $P_3P_2P_1A$.
	
	Se fizermos, agora, $(P_3P_2P_1A)X$ e lembrarmos que o produto de matrizes é associativo, teremos
	\[(P_3P_2P_1A)X=P_3P_2P_1(AX)=P_3P_2P_1B\]onde do lado direito aparece $B$ \underline{escalonada pelas mesmas matrizes que $A$ foi escalonada}. Além disso, a equação $(P_3P_2P_1A)X=P_3P_2P_1B$ nos diz que a solução ($X$) do sistema original continua sendo solução do sistema escalonado. Assim, podemos calcular $P_3P_2P_1B$:
	\[P_1B=\bordermatrix{&\cr l_1&1\cr l_2-l_1&-2\cr l_3-l_1&0\cr l_4-l_1&-2}
	\]
	\[P_2(P_1B)=\bordermatrix{&\cr l_1&1\cr l_2&-2\cr \frac{l_3}{-2}&0\cr l_4&-2}
	\]
	\[P_3(P_2P_1B)=\bordermatrix{&\cr l_1-l_3&1\cr l_2&-2\cr l_3&0\cr l_4+2l_3&-2},\]ou seja,
	\[\begin{pmatrix}
	1&0\\0&0\\0&1\\0&0
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	1\\-2\\0\\-2
	\end{pmatrix}\quad\text{ e }\quad\begin{pmatrix}
	1&1\\1&1\\1&-1\\1&-1
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	1\\-1\\1\\-1
	\end{pmatrix}\]têm as mesmas soluções.
	
	Contudo, o sistema escalonado nos diz que (pela última linha) $0x+0y=-2$, e nós sabemos que $0a=0$ para qualquer $a\in \R$. Em particular, não existe $(x,y)\in \R^2$ tal que $0x+0y=-2$ - ou seja, o sistema escalonado \textbf{não admite solução} e, portanto, o sistema original também não.
\end{ex}

\begin{rmk}
	De maneira geral, sempre que, ao escalonar uma matriz, obtivermos uma linha de zeros sendo igual a algo diferente de zero, podemos parar o escalonamento e concluir, imediatamente, que o sistema não possui soluções.
\end{rmk}

\begin{df}
	Seja $AX=B$ um sistema linear. Denotamos por $\begin{augmatrix}{c:c}
	A&B
	\end{augmatrix}$ a \textbf{matrix aumentada do sistema} obtida de $A$ simplesmente colando uma cópia de $B$ à direita.
\end{df}

\begin{ex}
	No exemplo anterior, em que $A=\begin{pmatrix}
	1&1\\1&1\\1&-1\\1&-1
	\end{pmatrix}$ e $B=\begin{pmatrix}
	1\\-1\\1\\-1
	\end{pmatrix}$, a \textbf{matrix aumentada} é
	\[A\mid B=\left(\begin{array}{c c:c}
	1&1&1\\1&1&-1\\1&-1&1\\1&-1&-1
	\end{array}\right).\]A vantagem de se trabalhar com a matriz aumentada é a seguinte: Ao invés de escalonar $A$ e depois repetir o procedimento em $B$, nós escalonamos ambas as matrizes ao mesmo tempo:
	\begin{alignat*}{4}
	\begin{augmatrix}{cc:c}
	1&1&1\\1&1&-1\\1&-1&1\\1&-1&-1
	\end{augmatrix}\quad&\rightsquigarrow\quad\begin{augmatrix}{cc:c}
	1&1&1\\0&0&-2\\0&-2&0\\0&-2&-2
	\end{augmatrix}\quad&\rightsquigarrow\quad\begin{augmatrix}{cc:c}
	1&1&1\\0&0&-2\\0&1&0\\0&-2&-2
	\end{augmatrix}\quad&\rightsquigarrow\quad\begin{augmatrix}{cc:c}
	1&0&0\\0&0&-2\\0&1&0\\0&0&-2
	\end{augmatrix}.
	\end{alignat*}
	
	Por exemplo, para o sistema linear
	\[\begin{pmatrix}
	1&1\\1&-1
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	1\\1
	\end{pmatrix}\]temos a matriz aumentada $\begin{augmatrix}{cc:c}
	1&1&1\\1&-1&1
	\end{augmatrix}$ que podemos escalonar e obter
	\[\begin{augmatrix}{cc:c}
	1&1&1\\1&-1&1
	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
	1&1&1\\0&-2&0
	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
	1&1&1\\0&1&0
	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
	1&0&1\\0&1&0
	\end{augmatrix}\]e podemos dizer imediatamente que as equações são $1x=1$ e $1y=0$, donde vemos que a única solução do sistema é $(1,0)\in \R^2$.
\end{ex}

\begin{ex}
	Vamos resolver um último sistema antes de avançar:
	\[\begin{pmatrix}
	1 & 2 & 1 & 1\\1&3&-1&2
	\end{pmatrix}\begin{pmatrix}
	x\\y\\z\\w
	\end{pmatrix}=\begin{pmatrix}
	1\\3
	\end{pmatrix},\]cuja matriz aumentada é $\begin{augmatrix}{cccc:c}
	1&2&1&1&1\\
	1&3&-1&2&3
	\end{augmatrix}$. Escalonando obtemos
	\[\begin{augmatrix}{cccc:c}
	1&2&1&1&1\\
	1&3&-1&2&3
	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cccc:c}
	1&2&1&1&1\\0&1&-2&1&2
	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cccc:c}
	1&0&5&-1&-3\\0&1&-2&1&2
	\end{augmatrix}\]que nos dá as equações $x+5z-w=-3$ e $y-2z+w=2$. Não temos mais restrições no sistema, então todos os pontos que satisfazem a essas equações são soluções.
	
	Para denotar, então, essa solução, note que podemos isolar $x$ e $y$ acima, obtendo $x=-5z+w-3$ e $y=2z-w+2$. Assim, vemos que \textit{para cada valor distinto de $z$ e $w$ temos uma solução diferente do sistema}. Por exemplo, se $w=z=0$, temos que $x=-3$ e $y=2$. Substituindo, então, o ponto $(-3,2,0,0)$ no sistema original vemos que
	\[(-3)+2(2)+(0)+(0)=4-3=1\]
	\[(-3)+3(2)-1(0)+2(0)=6-3=3\]e, de fato, o ponto $(-3,2,0,0)$ é solução.
	
	De maneira geral, como para cada valor possível de $z$ e $w$ temos uma solução, vamos notar o conjunto de todas as soluções por 
	\[S=\{(-5z+w-3,2z-w+2,z,w)\in\R^4\mid z\in \R,w\in \R\}.\]
\end{ex}

\begin{rmk}
	No caso do exemplo acima dizemos que $z$ e $w$ são \textbf{variáveis livres}. A ideia é que elas não têm um valor fixo, mas são ``livres'' para assumir o valor que quiserem e o sistema continua tendo solução.
\end{rmk}

\begin{exerc}
	Use os exemplos acima para responder: Seja $AX=B$ um sistema $n\times m$, com $n\neq m$. O que podemos dizer sobre a existência de soluções do sistema?
	
	E no caso das matrizes quadradas, i.e., $n=m$?
\end{exerc}

\subsection{Sistemas homogêneos}

\begin{df}
	Um sistema linear $AX=B$ é dito \textbf{homogêneo} se $B=0$.
\end{df}

Poderíamos nos perguntar a importância de destacar sistemas homogêneos dentre todos os sistemas lineares. Contudo, se nos lembrarmos do que já fizemos, vamos ver que um sistema não possui solução se, e somente se, sua forma escalonada possui uma linha de zeros, com a entrada correspondente na matriz resultante diferente de zero - por exemplo, algo da forma
\[\begin{pmatrix}
1&0\\
0&0\\
\end{pmatrix}\begin{pmatrix}
x\\y
\end{pmatrix}=\begin{pmatrix}
1\\1
\end{pmatrix}.\] Mas ao lidar com sistemas homogêneos, todas as entradas da matriz resultante são nulas - ou seja, mesmo que durante o processo de escalonamento alguma linha se anule, ainda assim o sistema continua tendo solução.

\begin{prop}
	Todo sistema linear homogêneo possui solução.
\end{prop}

Um jeito óbvio de ver isso é:

\begin{ex}
	Considere o sistema linear homogêneo
	\[\begin{pmatrix}
	1&7\\8&9\\17&-10\\25&3
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	0\\0
	\end{pmatrix}.\] Ao invés de tentar escalonar o sistema, note que estamos multiplicando a matrix $\begin{pmatrix}
	1&7\\8&9\\17&-10\\25&3
	\end{pmatrix}$ por uma outra matriz, e queremos que o resultado seja $0$. Mas já sabemos que $A0=0$ para qualquer matriz $A$ - ou seja, tomando $X_0=\begin{pmatrix}
	0\\0
	\end{pmatrix}$ vemos que $X_0$ é solução do sistema.
\end{ex}

De fato, todo sistema homogêneo possui a solução $(0,0)$ - por isso ela é chamada de \textbf{solução trivial}.

\begin{ex}
	Vamos voltar a um exemplo que fizemos acima:
	\[\begin{pmatrix}
	1 & 2 & 1 & 1\\1&3&-1&2
	\end{pmatrix}\begin{pmatrix}
	x\\y\\z\\w
	\end{pmatrix}=\begin{pmatrix}
	1\\3
	\end{pmatrix}.\] Já vimos que o conjunto de soluções desse sistema, $S$, é da forma
	\[S=\{(x,y,z,w)\in \R^4\mid x=-5z+w-3,y=2z-w+2\}.\] Vamos, agora, resolver o sistema homogêneo associado ao sistema acima, em que nós simplesmente trocamos a matriz resultante pela matriz de zeros:
	\[\begin{pmatrix}
	1 & 2 & 1 & 1\\1&3&-1&2
	\end{pmatrix}\begin{pmatrix}
	x\\y\\z\\w
	\end{pmatrix}=\begin{pmatrix}
	0\\0
	\end{pmatrix}.\]
	
	Escalonando obtemos
	\[\begin{augmatrix}{cccc:c}
	1&2&1&1&0\\
	1&3&-1&2&0
	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cccc:c}
	1&2&1&1&0\\0&1&-2&1&0
	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cccc:c}
	1&0&5&-1&0\\0&1&-2&1&0
	\end{augmatrix}\]que nos dá as equações $x+5z-w=0$ e $y-2z+w=0$. Não temos mais restrições no sistema, então todos os pontos que satisfazem a essas equações são soluções - ou seja, o conjunto de soluções do sistema homogêneo, $S_0$, é da forma
	\[S_0=\{(x,y,z,w)\in\R^4\mid x=-5z+w,y=2z-w\}.\]
	
	Comparando os dois conjuntos, não é difícil ver que se $X_0$ é uma solução do sistema homogêneo, então $X_0+\begin{pmatrix}
	-3\\2\\0\\0
	\end{pmatrix}$ é solução do sistema original.
	
	Por exemplo, escolhendo $X_0=\begin{pmatrix}
	1\\-1\\0\\1
	\end{pmatrix}$ vamos primeiro conferir que $X_0$ é, de fato, solução do sistema homogêneo:
	\[\begin{pmatrix}
		1 & 2 & 1 & 1\\1&3&-1&2
	\end{pmatrix}\begin{pmatrix}
	1\\-1\\0\\1
\end{pmatrix}=\begin{pmatrix}
1\cdot 1+2\cdot(-1)+1\cdot 0+1\cdot 1\\
1\cdot 1+3\cdot(-1)+(-1)\cdot 0+2\cdot 1
\end{pmatrix}=\begin{pmatrix}
1-2+0+1\\1-3+0+2
\end{pmatrix}=\begin{pmatrix}
0\\0
\end{pmatrix}.\] Agora vamos somar $\begin{pmatrix}
-3\\2\\0\\0
\end{pmatrix}$ a $X_0$, obtendo uma nova matriz
\[X=\begin{pmatrix}
1\\-1\\0\\1
\end{pmatrix}+\begin{pmatrix}
-3\\2\\0\\0
\end{pmatrix}=\begin{pmatrix}
-2\\1\\0\\1
\end{pmatrix}.\] Afirmamos que $X$ é solução do sistema original. Vamos conferir:
\[\begin{pmatrix}
1&2&1&1\\1&3&-1&2
\end{pmatrix}\begin{pmatrix}
-2\\1\\0\\1
\end{pmatrix}=\begin{pmatrix}
1\cdot(-2)+2\cdot 1+1\cdot 0+1\cdot1\\
1\cdot(-2)+3\cdot1+(-1)\cdot 0+2\cdot1
\end{pmatrix}=\begin{pmatrix}
-2+2+0+1\\-2+3+0+2
\end{pmatrix}=\begin{pmatrix}
1\\3
\end{pmatrix},\]ou seja, $X$ é de fato uma solução do sistema original.

Será que isso acontece por acaso?
\end{ex}

\begin{prop}
	Dado um sistema linear $AX=B$, uma solução $X_0$ do sistema homogêneo $AX=0$, e $X_1$ uma solução qualquer do sistema $AX=B$, então $X_0+X_1$ é solução de $AX=B$.
\end{prop}
\begin{proof}
	Se $X_0$ é solução do sistema homogêneo, temos que $AX_0=0$. Similarmente, se $X_1$ é solução do sistema $AX=B$, temos que $AX_1=B$. Agora, como o produto de matrizes distribui sobre a soma, temos que
	\[A(X_0+X_1)=AX_0+AX_1=0+B=B,\] ou seja, $X_0+X_1$ também é solução do sistema $AX=B$.
\end{proof}

Mas será que qualquer solução do sistema pode ser calculada assim - sabendo uma solução do sistema homogêneo e uma solução do sistema original? A resposta é sim, e vamos mostrar em seguida, mas antes, vamos fazer um exemplo:

\begin{ex}
	Ainda no exemplo anterior, considere o sistema
	\[\begin{pmatrix}
	1 & 2 & 1 & 1\\1&3&-1&2
	\end{pmatrix}\begin{pmatrix}
	x\\y\\z\\w
	\end{pmatrix}=\begin{pmatrix}
	1\\3
	\end{pmatrix}.\] Já vimos que $X_0=\begin{pmatrix}
	1\\-1\\0\\1
	\end{pmatrix}$ é solução do sistema homogêneo. Vamos escolher qualquer outra solução do sistema original, por exemplo, $X_1=\begin{pmatrix}
	-13\\6\\2\\0
	\end{pmatrix}$ (verifique que isso é uma solução!). Será que existe alguma solução $X_2$ tal que $X_1=X_0+X_2$? Ora, resolver isso é o mesmo que resolver $X_2=X_1-X_0$ - o que nós já sabemos fazer:
	\[X_2=\begin{pmatrix}
	-13\\6\\2\\0
	\end{pmatrix}-\begin{pmatrix}
	1\\-1\\0\\1
	\end{pmatrix}=\begin{pmatrix}
	-14\\7\\2\\-1
	\end{pmatrix}.\] Nos resta mostrar que $X_2$ é solução:
	\[\begin{pmatrix}
	1 & 2 & 1 & 1\\1&3&-1&2
	\end{pmatrix}\begin{pmatrix}
	-14\\7\\2\\-1
	\end{pmatrix}=\begin{pmatrix}
	1\cdot(-14)+2\cdot 7+1\cdot2+1\cdot(-1)\\
	1\cdot(-14)+3\cdot7+(-1)\cdot2+2\cdot(-1)
	\end{pmatrix}=\begin{pmatrix}
	-14+14+2-1\\-14+21-2-2
	\end{pmatrix}=\begin{pmatrix}
	1\\3
	\end{pmatrix},\]como queríamos.
\end{ex}

\begin{prop}
	Dado um sistema linear $AX=B$, uma solução do sistema homogêneo $X_0$ e uma solução do sistema original $X_1$, então existe uma solução do sistema original $X_2$ tal que $X_1=X_0+X_2$.
\end{prop}

Finalmente, vamos caminhar para uma caracterização geral desse tipo de problema:

\begin{ex}
	Mais uma vez, vamos voltar ao sistema 
	\[\begin{pmatrix}
	1 & 2 & 1 & 1\\1&3&-1&2
	\end{pmatrix}\begin{pmatrix}
	x\\y\\z\\w
	\end{pmatrix}=\begin{pmatrix}
	1\\3
	\end{pmatrix}.\] Já vimos que $S_0=\{(x,y,z,w)\in\R^4\mid x=-5z+w,y=2z-w\}$ - ou seja, uma matrix $X_0$ está em $S_0$ se, e somente se, $X_0$ é da forma
	\[X_0=\begin{pmatrix}
	-5z+w\\2z-w\\z\\w
	\end{pmatrix}.\] Mas podemos reescrever isso como
	\[X_0=\begin{pmatrix}
	-5z\\2z\\z\\0
	\end{pmatrix}+\begin{pmatrix}
	w\\-w\\0\\w
	\end{pmatrix},\] que finalmente se torna
	\[X_0=z\begin{pmatrix}
	-5\\2\\1\\0
	\end{pmatrix}+w\begin{pmatrix}
	1\\-1\\0\\1
	\end{pmatrix}.\]Ou seja, para quaisquer valores de $z,w\in \R$ temos uma solução do sistema homogêneo.
	
	Também já vimos que $S=\{(x,y,z,w)\in \R^4\mid x=-5z+w-3,y=2z-w+2\}$, e podemos fazer a mesma análise: $X_1$ é solução se, e somente se, é da forma
	\[X_1=\begin{pmatrix}
	-5z+w-3\\2z-w+2\\z\\w
	\end{pmatrix}=z\begin{pmatrix}
	-5\\2\\1\\0
	\end{pmatrix}+w\begin{pmatrix}
	1\\-1\\0\\1
	\end{pmatrix}+\begin{pmatrix}
	-3\\2\\0\\0
	\end{pmatrix}=X_0+\begin{pmatrix}
	-3\\2\\0\\0
	\end{pmatrix}.\]
	
	Isso nos mostra que qualquer solução $X_1$ do sistema original é da forma $X_0+\begin{pmatrix}
	-3\\2\\0\\0
	\end{pmatrix}$.
	
	Mas o que é a matriz $\begin{pmatrix}
	-3\\2\\0\\0
	\end{pmatrix}$? Se voltarmos a quando resolvemos o sistema pela primeira vez, vamos ver que $-3$ e $2$ é exatamente como fica a matriz resultante após o escalonamento. Os zeros nas linhas de baixo, então, simbolizam o fato de que o sistema não tem informação suficiente para determinar todas as quatro variáveis.
	
	Por fim, note que se $X_0$ e $X'_0$ são soluções do sistema homogêneo, então, pelo que fizemos acima, 
	\[X_0=z\begin{pmatrix}
		-5\\2\\1\\0
	\end{pmatrix}+w\begin{pmatrix}
		1\\-1\\0\\1
	\end{pmatrix}\]e
	\[X'_0=z'\begin{pmatrix}
	-5\\2\\1\\0
	\end{pmatrix}+w'\begin{pmatrix}
	1\\-1\\0\\1
	\end{pmatrix},\] e como $z,z',w.w'$ são números reais, podemos escrever $w'=\frac{ww'}{w}=w\frac{w'}{w}$ e $z'=\frac{zz'}{z}=z\frac{z'}{z}$, ou seja,
	\[X'_0=z'\begin{pmatrix}
	-5\\2\\1\\0
	\end{pmatrix}+w'\begin{pmatrix}
	1\\-1\\0\\1
	\end{pmatrix}=\frac{z'}{z}\left(z\begin{pmatrix}
	-5\\2\\1\\0
	\end{pmatrix}\right)+\frac{w'}{w}\left(w\begin{pmatrix}
	1\\-1\\0\\1
	\end{pmatrix}\right),\]e vemos que duas soluções do sistema homogêneo diferem apenas por multiplicações de números.
\end{ex}

Para obter um grande resultado conclusivo para esta seção, precisaremos avançar mais no curso.

\section{Matrizes Inversas}

Já vimos que as matrizes possuem o que costumamos chama de \textit{identidade multiplicativa} - ou seja, uma matriz tal que toda matriz vezes ela é a própria matriz.

\begin{df}
	A matriz $I_n\in M_n(\R)$ dada por
	\[I_n:=\begin{pmatrix}
	1&0&\cdots&0\\0&1&\cdots&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&1
	\end{pmatrix}\] é chamada de \textbf{matriz identidade $n\times n$}.
\end{df}

\begin{ex}
	Dada a matriz $A=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}\in M_2(\R)$, é fácil ver que
	\[AI_2=I_2A=A.\]
	
	De fato:
	\[AI_2=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}\begin{pmatrix}
	1&0\\0&1
	\end{pmatrix}=\begin{pmatrix}
	a\cdot1+b\cdot 0&a\cdot0+b\cdot 1\\c\cdot1+d\cdot0&c\cdot0+d\cdot0
	\end{pmatrix}=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}=A\]e
	\[I_2A=\begin{pmatrix}
	1&0\\0&1
	\end{pmatrix}\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}=\begin{pmatrix}
	1\cdot a+0\cdot c&1\cdot b+0\cdot d\\0\cdot a+ 1\cdot c&0\cdot b+1\cdot d
	\end{pmatrix}=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}=A.\]
\end{ex}

Os números reais também têm essa propriedade: Existe um número real (1) tal que para qualquer número real $a$ temos $a\cdot 1=1\cdot a=a$.

Além disso, os números reais têm outra propriedade: Para qualquer número real $a$ existe um (único) número real $a^{-1}$ tal que $a\cdot a^{-1}=a^{-1}\cdot a=1$. Nós chamamos esse número de \textit{inverso de $a$}.

Surge então a pergunta natural: Será que matrizes reais têm inversas?

\begin{rmk}
	Note que se toda matriz quadrada $A\in M_n(\R)$ possui inversa, então qualquer sistema da forma $AX=B$ pode ser resolvido multiplicando ambos os lados por $A^{-1}$:
	\[AX=B\Leftrightarrow A^{-1}(AX)=A^{-1}B\Leftrightarrow(A^{-1}A)X=A^{-1}B\Leftrightarrow X=A^{-1}B.\] Dito de outra forma, se toda matriz $A$ for inversível, todo sistema $AX=B$ teria solução dada por $X=A^{-1}B$.
\end{rmk}

Contudo, como já vimos acima, \textit{nem todo sistema possui solução}. Isso nos diz imediatamente que \textit{nem toda matriz possui inversa}.

Para ver quais são as matrizes que possuem inversa, vamos precisar de dar uma definição formal para elas.

\begin{df}
	Dada uma matriz $A\in M_n(\R)$, dizemos que \textbf{$A$ é inversível} se existe uma matriz $B\in M_n(\R)$ tal que $AB=BA=I_n$.
\end{df}

\begin{exerc}
	Prove que se $B$ e $C$ são duas inversas para $A$, então $B=C$ (dica: comece assim: ``como $I_n$ vezes qualquer matriz é a própria matriz, $B=BI_n$ e $C=I_nC$. Além disso, como $B$ e $C$ são inversos de $A$, temos que $I_n=AC=BA$'').
\end{exerc}

Vamos agora abordar um método para calcular inversos, quando estes existirem:

\begin{ex}
	Considere a matriz\[A=\begin{pmatrix}
	2&5\\1&3
	\end{pmatrix}.\] Queremos achar uma matriz $B$ tal que $AB=I_2$. Mas isso é um sistema linear! Nós temos uma matriz de dados iniciais ($A$) que multiplicada por uma matriz que queremos determinar ($B$) dá uma matriz de resultados ($I_2$). E nós já sabemos resolver sistemas lineares - via escalonamento! Então, vamos lá:
	
	\[\begin{array}{rl}
	\begin{augmatrix}{cc:cc}
	2&5&1&0\\
	1&3&0&1
	\end{augmatrix}&\rightsquigarrow\begin{augmatrix}{cc:cc}
	1&\frac{5}{2}&\frac{1}{2}&0\\
	1&3&0&1
	\end{augmatrix}\\\\&\rightsquigarrow\begin{augmatrix}{cc:cc}
	1&\frac{5}{2}&\frac{1}{2}&0\\
	0&\frac{1}{2}&-\frac{1}{2}&1
	\end{augmatrix}\\\\&\rightsquigarrow\begin{augmatrix}{cc:cc}
	1&\frac{5}{2}&\frac{1}{2}&0\\
	0&1&-1&2
	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:cc}
	1&0&3&-5\\
	0&1&-1&2
	\end{augmatrix}
	\end{array}\]Então estamos dizendo que a matriz inversa de $A$ é a matriz
	\[A^{-1}=\begin{pmatrix}
	3&-5\\-1&2
	\end{pmatrix}.\]
	
	Vamos testar:
	\[AA^{-1}=\begin{pmatrix}
	2&5\\1&3
	\end{pmatrix}\begin{pmatrix}
	3&-5\\-1&2
	\end{pmatrix}=\begin{pmatrix}
	2\cdot3+5\cdot(-1)&2\cdot(-5)+5\cdot2\\
	1\cdot3+3\cdot(-1)&1\cdot(-5)+3\cdot2
	\end{pmatrix}=\begin{pmatrix}
	6-5 & -10+10\\3-3&-5+6
	\end{pmatrix}=\begin{pmatrix}
	1&0\\0&1
	\end{pmatrix}\]e
	\[A^{-1}A=\begin{pmatrix}
	3&-5\\-1&2
	\end{pmatrix}\begin{pmatrix}
	2&5\\1&3
	\end{pmatrix}=\begin{pmatrix}
	3\cdot2+(-5)\cdot1&3\cdot5+(-5)\cdot3\\
	(-1)\cdot2+2\cdot1&(-1)\cdot5+2\cdot3
	\end{pmatrix}=\begin{pmatrix}
	6-5&-5+5\\
	-2+2&-5+6
	\end{pmatrix}=\begin{pmatrix}
	1&0\\0&1
	\end{pmatrix}\]
\end{ex}

\begin{prop}
	Seja $\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}\in M_2(\R)$ uma matriz. Então a inversa, se existir, é da forma
	\[\frac{1}{ad-bc}\begin{pmatrix}
	d&-b\\
	-c&a
	\end{pmatrix}.\]
\end{prop}

Essa é a primeira vez que vemos o determinante de uma matriz aparecendo. Mais pra frente vamos definir o determinante de outra maneira e ver para que ele serve.

Outro resultado que vamos usar agora, mesmo que não sejamos capazes de mostrar ainda é o seguinte:

\begin{lemma}
	Seja $A\in M_n(\R)$ matriz quadrada. Se existe matriz $B\in M_n(\R)$ tal que $AB=I_n$, então $B$ é inversa de $A$. Similarmente, se existe matriz $C\in M_n(\R)$ tal que $CA=I_n$, então $C$ é inversa de $A$.
\end{lemma}

Em outras palavras, para matrizes, basta checar se o produto em uma ordem dá a identidade, que isso é suficiente para concluir que o produto na outra ordem também dará.

Agora sim, vamos ver um resultado que conseguimos provar:

\begin{lemma}
	Uma matriz $A\in M_n(\R)$ é inversível se, e somente se, $A$ pode ser escalonada em $I_n$.
\end{lemma}
\begin{proof}
	Por um lado, é óbvio que a matriz identidade pode ser escalonada em si mesma (fazendo nada) e que a matriz identidade é inversível: $I_nI_n=I_n$ - de fato, ela é seu próprio inverso. Além disso, já vimos que se $A'$ pode ser obtida de $A$ via escalonamento, então o sistema $AX=B$ tem as mesmas soluções do sistema $A'X=B'$, em que $B'$ é obtida de $B$ pelo mesmo escalonamento que leva $A$ em $A'$. 
	
	Então, se $I_n$ pode ser obtida de $A$ por escalonamento, o sistema $AX=I_n$ tem as mesmas soluções do sistema $I_nX=C$, em que $C$ é obtida de $I_n$ pelo mesmo escalonamento que leva $A$ em $I_n$. Mas $I_nX=C$ nos diz que $C=X$ e, logo, $AC=I_n$. O lema acima agora nos garante que $CA=I_n$ e portanto $A$ é inversível.
	
	\bigskip
	Analogamente, suponha que $A$ é inversível - ou seja, o sistema $AX=I_n$ tem solução $B$ - em outras palavras, $X=B$. Mas podemos re-escrever isso como $I_nX=B$ e ver que esse sistema tem a mesma solução de $AX=I_n$, donde podemos concluir que $I_n$ pode ser obtido de $A$ por escalonamentos.
\end{proof}

Finalmente, vamos encerrar essa seção retomando as matrizes de escalonamento.

Como dissemos anteriormente, nem toda soma de matrizes $e^n_{i,j}$ é um escalonamento.

\begin{prop}
	Toda matriz invertível é um escalonamento.
\end{prop}

\begin{proof}
	Dado um sistema $AX=B$, se $E$ é inversível com inversa $E^{-1}$, tome $Z$ solução de $(EA)X=EB$. Vamos mostrar que $Z$ também é solução de $AX=B$ - e portanto, $E$ é escalonamento.
	
	De fato:
	
	\[AZ=I_n(AZ)=(E^{-1}E)(AZ)=E^{-1}((EA)Z)=E^{-1}(EB)=(E^{-1}E)B=I_nB=B,\]logo $Z$ é solução de $AX=B$ e, portanto, $E$ é um escalonamento.
\end{proof}

Gostaríamos de mostrar mais - gostaríamos de mostrar que todo escalonamento é invertível, mas não temos ferramentas para isso ainda. Contudo, para matrizes $2\times 2$ é fácil:

\begin{ex}
	Os possíveis escalonamentos são combinações de 
	\begin{itemize}
		\item Troca de linhas;
		\item Multiplicação de uma linha por um número;
		\item Soma de uma linha a outra linha.
	\end{itemize}

	Vamos exibir as matrizes que realizam cada operação:
	
	\begin{itemize}
		\item A matriz que troca as duas linhas de uma matriz $2\times 2$ é dada por $\begin{pmatrix}
		0&1\\1&0
		\end{pmatrix}$, de fato:
		\[\begin{pmatrix}
		0&1\\1&0
		\end{pmatrix}\begin{pmatrix}
		a&b\\c&d
		\end{pmatrix}=\begin{pmatrix}
		0\cdot a+1\cdot c&0\cdot b+1\cdot d\\
		1\cdot a+0\cdot c&1\cdot b+0\cdot d
		\end{pmatrix}=\begin{pmatrix}
		c&d\\a&b
		\end{pmatrix}.\]
		
		\item A matriz que multiplica a linha 1 por $\lambda\in \R$ é $\begin{pmatrix}
		\lambda&0\\0&1
		\end{pmatrix}$ e a matriz que multiplica a linha 2 por $\mu\in \R$ é $\begin{pmatrix}
		1&0\\0&\mu
		\end{pmatrix}$. De fato, 
		\[\begin{pmatrix}
			\lambda&0\\
			0&\mu
		\end{pmatrix}\begin{pmatrix}
		a&b\\c&d
		\end{pmatrix}=\begin{pmatrix}
		\lambda\cdot a+0\cdot c&\lambda\cdot b+0\cdot d\\
		0\cdot a+\mu\cdot c&0\cdot b+\mu\cdot d
		\end{pmatrix}=\begin{pmatrix}
		\lambda a&\lambda b\\\mu c&\mu d
		\end{pmatrix}.\]
		\item A matrix que soma a linha 1 na linha 2 é $\begin{pmatrix}
		1&0\\1&1
		\end{pmatrix}$ e a matriz que soma a linha 2 na linha 1 é $\begin{pmatrix}
		1&1\\0&1
		\end{pmatrix}$. De fato,
		\[\begin{pmatrix}
		1&0\\1&1
		\end{pmatrix}\begin{pmatrix}
		a&b\\c&d
		\end{pmatrix}=\begin{pmatrix}
		1\cdot a+0\cdot c&1\cdot b+0\cdot d\\
		1\cdot a+1\cdot c&1\cdot b+1\cdot d
		\end{pmatrix}=\begin{pmatrix}
		a & b\\
		a+c&b+d
		\end{pmatrix}\]e
		\[\begin{pmatrix}
		1&1\\0&1
		\end{pmatrix}\begin{pmatrix}
		a&b\\c&d
		\end{pmatrix}=\begin{pmatrix}
		1\cdot a+1\cdot c&1\cdot b+1\cdot d\\
		0\cdot a+1\cdot c&0\cdot b+1\cdot d
		\end{pmatrix}=\begin{pmatrix}
		a+c & b+d\\
		c&d
		\end{pmatrix}.\]
	\end{itemize}

	Mas todas essas matrizes são inversíveis: 
	\begin{itemize}
		\item O inverso da matriz $\begin{pmatrix}
		0&1\\1&0
		\end{pmatrix}$ é ela mesma, já que trocar as duas linhas duas vezes é a mesma coisa de não fazer nada.
		\item Os inversos das matrizes $\begin{pmatrix}
		\lambda &0\\0&1
		\end{pmatrix}$ e $\begin{pmatrix}
		1&0\\0&\mu
		\end{pmatrix}$ são as matrizes $\begin{pmatrix}
		\lambda^{-1}&0\\0&1
		\end{pmatrix}$ e $\begin{pmatrix}
		1&0\\0&\mu^{-1}
		\end{pmatrix}$, respectivamente, já que o inverso de ``multiplicar uma linha por $x$'' é ``dividir uma linha por $x$''.
		\item Os inversos das matrizes $\begin{pmatrix}
		1&0\\1&1
		\end{pmatrix}$ e $\begin{pmatrix}
		1&1\\0&1
		\end{pmatrix}$ são, respectivamente, as matrizes $\begin{pmatrix}
		1&0\\-1&1
		\end{pmatrix}$ e $\begin{pmatrix}
		1&-1\\0&1
		\end{pmatrix}$, já que o inverso de ``somar a linha $i$ na linha $j$'' é ``subtrair a linha $i$ da linha $j$''.
	\end{itemize}

	Finalmente, note que se $A$ e $B$ são inversíveis, então o produto $AB$ também é, pois $(AB)(B^{-1}A^{-1})=A((BB^{-1})A^{-1})=A(I_nA^{-1})=AA^{-1}=I_n$. Assim, como qualquer escalonamento é produto das matrizes acima, segue que qualquer escalonamento é invertível, já que cada uma delas é.
\end{ex}

\begin{exerc}
	Prove que os inversos que apresentamos acima são, de fato, inversos.
\end{exerc}