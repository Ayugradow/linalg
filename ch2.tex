\chapter{Real Linear Algebra}
\section{Introduction}
\subsection{Operations and other fun things to do with your friends and family}

To start working with vector spaces we first need to understand that the perspective is going to change a bit from the previous chapter. We're leaving the domain of \textbf{set theory} and jumping right in the domain of \textbf{algebra}.

\textit{Algebra }is the domain of mathematics that deals with operations and their properties.

\begin{df}
	Given any set $X$, a \textbf{(binary) operation} on $X$ is a function $f:X\times X \to X$.
\end{df}

\begin{ex}
	Let $\N$ be the set of natural numbers, as before. We have a few operations here:
	\[f,g,h:\N\times\N \to \N\]
	\[(n,m)\mapsto f(n,m):=n+m\]
	\[(n,m)\mapsto g(n,m):=nm\]
	\[(n,m)\mapsto h(n,m):=n^m\]and some of these operations have some properties that the others don't.
	
	For instance, all three functions satisfy the following property:
	
	\begin{itemize}
		\item Let $\phi$ be an operation on $X$. There is some $n_e\in X$ such that $\phi(n,n_e)=n$ for all $n\in X$.
	\end{itemize}

	In the case of $f$, if we choose $n_e:=0$, we see that $f(n,0)=n+0=n$, no matter which $n\in \N$ we chose, so $f$ satisfies the property above.
	
	In the case of $g$, if we choose $n_e:=1$, we see that $g(n,1)=n\cdot1=n$, no matter which $n\in\N$ we chose, so $g$ satisfies the property above.
	
	Finally, in the case of $h$, if we choose $n_e:=1$, we see that $h(n,1)=n^1=n$, no matter which $n\in\N$ we chose, so $h$ satisfies the property above.
	
	\bigskip
	Next up is the property:
	
	\begin{itemize}
		\item Let $\phi$ be an operation on $X$. There is some $n_e\in X$ such that $\phi(n_e,n)=n$ for all $n\in X$.
	\end{itemize}

	What can we say about $f,g,h$ in this case? Well, it's easy to see that for $f$ and $g$ it still holds true - and it does so for the same value of $n_e$ as before.
	
	However, for $h$ it fails. For instance, is there some number $x\in\N$ such that $h(x,2)=2$? Well, by definition of $h$ we would need to have $x^2=2$ and so $x=\sqrt{2}$ which is not in $\N$ - this tells us that there's no such $x\in\N$. It follows that this property fails for $h$.
	
	\bigskip
	Summing up all of these together, we get the following property:
	\begin{itemize}
		\item (Identity element) Let $\phi$ be an operation on $X$. There is some $n_e\in X$ such that $\phi(n,n_e)=n=\phi(n_e,n)$ for all $n\in X$.
	\end{itemize}
	And we see that $f$ and $g$ have what's called an \textit{identity element} - it's an element $n_e$ such that if you fix it in any input of your operation, then your operation is just the identity function.
	
	\bigskip
	Consider now the following property:
	\begin{itemize}
		\item (Associativity) Let $\phi$ be an operation on $X$. Then, for all $n,m,l\in X$ we have that $\phi(\phi(n,m),l)=\phi(n,\phi(m,l))$.
	\end{itemize}

	In the case of $f$ we can check
	\[f(f(n,m),l)=f(n+m,l)=(n+m)+l=n+(m+l)=f(n,m+l)=f(n,f(m,l))\]and see that $f$ is associative.
	
	In the case of $g$ we can check
	\[g(g(n,m),l)=g(nm,l)=(nm)l=n(ml)=g(n,ml)=g(n,g(m,l))\]and see that $g$ is associative.
	
	However, for $h$, once again, this property fails: For instance, let us compare $h(h(2,2),3)$ and $h(2,h(2,3))$:
	\[h(h(2,2),3)=h(2^2,3)=(2^2)^3=4^3=64\]
	\[h(2,h(2,3))=h(2,2^3)=2^{(2^3)}=2^8=256\]so they are clearly different, and $h$ is not associative.
	
	\bigskip
	One more:
	\begin{itemize}
		\item (Commutativity) Let $\phi$ be an operation on $X$. Then, for all $n,m\in X$ we have that $\phi(n,m)=\phi(m,n)$.
	\end{itemize}

	In the case of $f$ we can easily see that $f(n,m)=n+m=m+n=f(m,n)$.
	
	Similarly for $g$, we see that $g(n,m)=nm=mn=g(m,n)$.
	
	But, once again, $h(2,3)=8\neq 9=h(3,2)$, so $h$ is not commutative.
	
	\bigskip
	These are the most common operations in $\N$ and some of their properties. Now, let us show something that is \textbf{not} an operation:
	
	Consider the functions $$f',g':\N\times\N\to\N$$\[(n,m)\mapsto f'(n,m):=n-m\]
	\[(n,m)\mapsto g'(n,m):=n/m.\]
	
	Notice that I've just lied to you - these are \textbf{not} functions. To see that, take $f'$ and apply it on $(3,1)$. By definition of function, $f'(3,1)$ should lie on $\N$, the codomain of $f'$. But, by definition of $f'$, we see that $f'(3,1)=3-1=-2$, which is \textbf{not} in $\N$.
	
	Similarly, $g'$ isn't a function for the same reason: It should take, for instance, $(1,2)$ to a natural number - but it doesn't. It takes $(1,2)$ to $g'(1,2)=1/2$ which, once more, is not a natural number.
	
	However, for \textit{some} specific values of the input, $f'$ and $g'$ really have outputs in $\N$. For that reason, they are called \textbf{partial operations} and, sadly, won't be studied in this text, since we're mostly concerned with proper operations.
	
	If, however, you'd like to learn more about partial operations, you should click \href{https://ncatlab.org/nlab/show/groupoid}{here} or Google for ``groupoid'' - which is precisely the mathematical notion of a set with an associative partial operation.
\end{ex}

\begin{df}
	Given an operation $\phi:X\times X\to X$ we will say that
	\begin{itemize}
		\item (Identity element) $\phi$ admits an \textbf{identity element} if there is some $e\in X$ such that $\phi(x,e)=x=\phi(e,x)$ for all $x\in X$. In this case, $e$ is called an \textbf{identity element};
		\item (Associativity) $\phi$ is \textbf{associative} if $\phi(x,\phi(y,z))=\phi(\phi(x,y),z)$ for all $x,y,z\in X$;
		\item (Commutative) $\phi$ is \textbf{commutative} if $\phi(x,y)=\phi(y,x)$ for all $x,y\in X$;
		\item (Inverse element) $\phi$ admits \textbf{inverse elements} if for all $x\in X$ there is some $y\in X$ such that $\phi(x,y)=e=\phi(y,x)$ for some identity element $e\in X$.
	\end{itemize}
\end{df}

\begin{df}
	Let $X$ be a set with two operations, $f,g:X\times X\to X$. We say that \textbf{$f$ distributes over $g$ on the left} (resp. \textbf{on the right}) if $$f(x,g(y,z))=g(f(x,y),f(x,z))$$ (resp. $$f(g(x,y),z)=g(f(x,z),f(y,z)))$$ for all $x,y,z\in X$.
	
	If $f$ distributes over $g$ on both sides, we simply say that \textbf{$f$ distributes over $g$}.
\end{df}
\begin{ex}
	Following up on the previous example, we see that $g$ (the multiplication) distributes over $f$ (the addition):
	\[g(n,f(m,l))=g(n,m+l)=n(m+l)=nm+nl=f(nm,nl)=f(g(n,m),g(n,l))\]
	\[g(f(n,m),l)=g(n+m,l)=(n+m)l=nl+ml=f(nl,ml)=f(g(n,l),g(m,l))\]but $f$ doesn't distribute (on either side!) over $g$:
	\[1+(1\cdot 1)=1+1=2\neq 4=2\cdot 2=(1+1)\cdot(1+1)\]
	\[(1\cdot 1)+1=1+1=2\neq 4=2\cdot 2=(1+1)\cdot(1+1)\]
\end{ex}

All this talk now brings us to a very specific definition:

\begin{df}
	Let $X$ be a set with two operatios $A,M:X\times X\to X$. We will say that $(X,A,M)$ is a \textbf{field} if
	\begin{multicols}{3}
		\begin{enumerate}[(1)]
			\item $A$ is associative;
			\item $A$ is commutative;
			\item $A$ has an identity element;
			\item $A$ has inverses;
			\item $M$ is associative;
			\item $M$ is commutative;
			\item $M$ has an identity element;
			\item $M$ has inverses (excluding the additive identities);
			\item $M$ distributes over $A$.
		\end{enumerate}
	\end{multicols}
	

In this case, we call $A$ and $M$, respectively, the field's \textbf{addition} and \textbf{multiplication} operations, and denote them simply by $x+y:=A(x,y)$ and $xy:=M(x,y)$ for all $(x,y)\in X\times X$.
\end{df}

\begin{prop}
	The set $\R$ of real numbers with the usual addition and multiplication is a field.
\end{prop}
\begin{proof}
	This is immediate, since for every $x,y,z\in\R$ we have:
	\begin{multicols}{3}
		\begin{enumerate}[(1)]
			\item $x+(y+z)=(x+y)+z$;
			\item $x+y=y+x$;
			\item $x+0=0+x=x$;
			\item $x+(-x)=(-x)+x=0$;
			\item $x(yz)=(xy)z$;
			\item $xy=yx$;
			\item $x\cdot 1=1\cdot x=x$;
			\item $xx^{-1}=x^{-1}x=1$ if $x\neq 0$;
			\item $x(y+z)=xy+xz$ and $(x+y)z=xz+yz$.
		\end{enumerate}
	\end{multicols}	
\end{proof}

\begin{ex}
	Notice, however, that the sets $\N$ and $\Z$, of the naturals and integers, respectively, are \textbf{not} fields: $\N$ doesn't have either additive or multiplicative inverses (so it fails properties (4) and (8)), and $\N$ doesn't have multiplicative inverses (so it fails property (8)).
	
	On the other hand, it's easy to see that $\Q$, the set of rational numbers, is indeed a field. It is actually constructed to be, in some sense, ``the smallest field which extends $\Z$/$\N$''.
	
	Finally, the set $\C$ of complex numbers is also a field if you define the inverse of $z=x+iy$ to be $z^{-1}:=\dfrac{x-iy}{x^2+y^2}$. Indeed:
	
	\[zz^{-1}=(x+iy)\left(\frac{x-iy}{x^2+y^2}\right)=\frac{x^2+y^2}{x^2+y^2}=1\]so it is indeed an inverse for $z$.
\end{ex}

Let us show some properties of fields:

\begin{lemma}
	Let $(k,+,\cdot)$ be a field. Then the following hold:
	\begin{enumerate}[(a)]
		\item There's a unique additive identity;
		\item For each $x\in k$, there's a unique additive inverse;
		\item There's a unique multiplicative identity;
		\item For each $x\in k$, there's a unique multiplicative inverse;
		\item Let $0$ be an additive identity of $k$. Then $0x=0$ for all $x\in k$.
		\item Let $1$ be a multiplicative identity of $k$. Then $-x=(-1)x$, where $(-1)+1=0$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{enumerate}[(a)]
		\item Let $0$ and $0'$ be two additive identities of $k$. Then
		\[0=0+0'=0'\]where the leftmost equality holds since $0'$ is additive identity, and the rightmost equality holds since $0$ is additive identity, and so $0=0'$.
		
		\item Given $x\in k$, let $x'$ and $x''$ be two additive inverses to $x$. Then
		\[x'=x'+0=x'+(x+x'')=(x'+x)+x''=0+x''=x''\]and so $x'=x''$.
		
		\item Let $1$ and $1'$ be two multiplicative identities of $k$. Then
		\[1=1\cdot1'=1'\]where the leftmost equality holds since $1'$ is a multiplicative identity, and the rightmost equality holds since $1$ is a multiplicative identity, so $1=1'$.
		
		\item Given $x\in k$, let $x''$ and $x''$ be two multiplicative inverses to $x$. Then
		\[x'=x'\cdot1=x'(xx'')=(x'x)x''=1\cdot x''=x''\]and so $x'=x''$.
		
		\item Given $x\in k$, we have that
		\[0x=(0+0)x=0x+0x\]since $k$ is a field and $0$ is the additive identity. Let $y\in k$ be the additive inverse of $0x$.
		
		Then, since the above is true, we can see that $(0x)+y=(0x+0x)+y$ is also true. But the LHS is just 0, since $y$ is the additive inverse of $0x$, and the RHS is just $(0x+0x)+y=0x+(0x+y)=0x+0=0x$, so the above equation evaluates to $0=0x$.
		
		\item Given $x\in k$, we have that $0x=(1+(-1))x$ since $1+(-1)=0$. But now, by the distributive property of fields we see that $0x=(1+(-1))x=(1)x+(-1)x$.
		
		But $0x=0$ and $1x=x$, so this is just $0=x+(-1)x$. Since additive inverses are unique, we see that $(-1)x=-x$.
	\end{enumerate}
\end{proof}

This result basically tells us that every field is ``similar'' to $\R$, in some sense.

\begin{rmk}
	The reason why we require that the multiplication has inverses for all elements \textit{except for 0} is precisely because of item (e) above. Since $0x=0$ for all $x$, if we could have some $0^{-1}$, then $0=00^{-1}=1$ so we would have $0=1$.
	
	But since $1x=x$ for all $x$, this would imply that $x=1x=0x=0$, so \textbf{every element of the field would have to be 0 for it to be consistent}.
	
	In other words, the only set that satisfies all the properties of a field and also has a multiplicative inverse to 0 is the set $\{0\}$.
	
	In fact:
\end{rmk}

\begin{prop}
	The set $1=\{0\}$ with addition and multiplication being equal and given by $0+0=0\cdot 0=0$ is a field. Its multiplicative and additive identities are $0$, who is also the inverse of $0$.
\end{prop}
\begin{proof}
	There's literally nothing to prove.
\end{proof}

Finally, to end this section, let us give some examples of fields that aren't $1$, $\Q$, $\R$ or $\C$.

\begin{ex}
	Let $p\in \N$ be a prime number (that is, there are only two ways to write $p=nm$: $n=p, m=1$ and $n=1, m=p$). Consider the set $p\in\N$ - that is, $p=\{0,1,2,\cdots,p-1\}$. We will give a field structure to $p$ as follows:
	
	For any $x,y\in p$, define:
	\begin{itemize}
		\item $x+y$ is the remainder of the division of $x+y$ in $\N$ by $p$;
		\item $xy$ is the remainder of the division of $xy$ in $\N$ by $p$.
	\end{itemize}

	We claim that $p$ with those two operations is a field, which will be denoted by either $\Z_p$, $\Z/p\Z$ or $\mathds F_p$.
	
	For instance, let us do some computations with $p=3$.
	
	In this case, $p=\{0,1,2\}$, and so we have the following tables of operations:
	
	\begin{center}
		\begin{tabu}{|c|[2pt]c|c|c|}
			\hline+&0&1&2\\
			\tabucline[2pt]{-} 0&0&1&2\\
			\hline 1&1&2&0\\
			\hline 2&2&0&1\\
			\hline 
		\end{tabu} and \begin{tabu}{|c|[2pt]c|c|c|}
			\hline $\times$&0&1&2\\
			\tabucline[2pt]{-} 0&0&0&0\\
			\hline 1&0&1&2\\
			\hline 2&0&2&1\\
			\hline 
		\end{tabu}
	\end{center}
	
	It is, then, readily seen that $0$ and $1$ are, respectively, the additive and multiplicative identities of $\mathds{F}_3$.
	
	We can also see that $1+2=0$ so 1 and 2 are additive inverses to each other. Similarly, we see that $1\cdot 1=1=2\cdot 2$ so both 1 and 2 are multiplicative inverses to themselves.
	
	This shows that $\mathds{F}_3$ is a field.
	
	Building similar tables of operations we can prove that any $\mathds{F}_p$ is a field.
	
	Let us now show the necessity of $p$ being a prime.
	
	\bigskip
	Let $4=\{0,1,2,3\}$. Let's try building the same operations:
	
	\begin{center}
		\begin{tabu}{|c|[2pt]c|c|c|c|}
			\hline
			+&0&1&2&3\\\tabucline[2pt]{-}
			0&0&1&2&3\\\hline
			1&1&2&3&0\\\hline
			2&2&3&0&1\\\hline
			3&3&0&1&2\\\hline 
		\end{tabu} and \begin{tabu}{|c|[2pt]c|c|c|c|}
			\hline
			$\times$&0&1&2&3\\\tabucline[2pt]{-}
			0&0&0&0&0\\\hline
			1&0&1&2&3\\\hline
			2&0&2&0&2\\\hline
			3&0&3&2&1\\\hline
		\end{tabu}
	\end{center}but this shows that $2$ doesn't have any multiplicative inverses: $2\cdot 0=0$, $2\cdot 1=2$, $2\cdot 2=0$ and $2\cdot 3=2$.
	
	But by definition of a field, the only element that has no multiplicative inverse is 0. But clearly $2\neq 0$ (since $1+2\neq 1$), so $\Z/4\Z$ cannot be a field.
	
	This happens precisely because $4$ can be written as $4=nm$ in \textit{three} different ways: $4=4\cdot1=1\cdot 4=2\cdot 2$.
	
	Since this isn't supposed to be a course on field theory, we won't go into much detail on how to prove that $\Z/n\Z$ is a field if, and only if, $n$ is prime.
\end{ex}

\newpage
\subsection{$\R^2$D2}

Let us start this section with the set that will be the focus of most, if not all, of this chapter: $\R^2$.

By definition, $\R^2=\R\times \R$ is the set of ordered pairs of real numbers.

\begin{df}
	We're going to define the \textbf{addition} $A:\R^2\times \R^2\to \R^2$ to be given by $A((x,y),(z,w)):=(x+z,y+w)$ for any $(x,y),(z,w)\in\R^2$.
\end{df}

\begin{prop}
	The addition $A$ we've just defined satisfies the following properties:
	\begin{enumerate}[(i.)]
		\item $A$ is associative;
		\item $A$ is commutative;
		\item $A$ admits an identity element;
		\item $A$ admits inverses.
	\end{enumerate}
\end{prop}
\begin{proof}
	Choose any three elements $(a,b),(c,d),(e,f)\in\R^2$. Then:
	\begin{enumerate}[(i.)]
		\item \begin{align*}
			A\left(A\left((a,b),(c,d)\right),(e,f)\right)&=A((a+c,b+d),(e,f))\\
			&=((a+c)+e,(b+d)+f)\\
			&=(a+(c+e),b+(d+f))\\
			&=A((a,b),(c+e,d+f))=A((a,b),A((c,d),(e,f))),
		\end{align*}so $A$ is associative;
		
		\item \begin{align*}
		A((a,b),(c,d))&=(a+c,b+d)=(c+a,d+b)=A((c,d),(a,b))
		\end{align*}so $A$ is commutative;
		
		\item $A((a,b),(0,0))=(a+0,b+0)=(a,b)=(0+a,0+b)=A((0,0),(a,b))$ so $A$ has identity $(0,0)$;
		
		\item $A((a,b),(-a,-b))=(a-a,b-b)=(0,0)=(-a+a,-b+b)=A((-a,-b),(a,b))$ so $A$ has inverses.
	\end{enumerate}
\end{proof}

\begin{rmk}
	From now on, we're gonna denote $A((a,b),(c,d))$ by $(a,b)+(c,d)$ for any $(a,b),(c,d)\in \R^2$ since, by the preceding proposition, it behaves well-enough like number addition.
\end{rmk}

And it seems there's not much else we can do with $\R^2$ for now.

To proceed with our studies, then, we're gonna need a new approach.

\begin{df}
	We'll denote $E_*$ the \textbf{pointed Euclidean plane}. That is, $E$ is the set of all points in the Euclidean plane, and $*$ is a distinguished point.
\end{df}

\begin{ex}
	For instance,
	
	\[\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\clip(-0.5,-2.) rectangle (7.,6.);
	\begin{scriptsize}
	\draw [fill=ududff] (0.94,0.96) circle (2.5pt);
	\draw[color=ududff] (1.08,1.33) node {$A$};
	\draw [fill=ududff] (5.06,2.92) circle (2.5pt);
	\draw[color=ududff] (5.2,3.29) node {$B$};
	\end{scriptsize}
	\end{tikzpicture}\] we can think of the set $E_A$ (whose elements are all points in the plane, including $A$ and $B$, but distinguishing $A$) and the set $E_B$ (whose elements are all points in the plane, including $A$ and $B$, but distinguishing $B$).
\end{ex}

\begin{df}
	Given a pointed Euclidean plane $E_*$, we define a \textbf{vector in $E_*$} to be any directed segment starting in $*$.
\end{df}

\begin{ex}
	Continuing the above example,
	\[
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\clip(-3.18,-2.52) rectangle (6.,4.75);
	\draw [->] (0.94,0.96) -- (2.2,3.56);
	\draw [->] (0.94,0.96) -- (-2.32,1.66);
	\draw [->] (0.94,0.96) -- (-0.06,-1.7);
	\draw [->] (0.94,0.96) -- (5.5,0.1);
	\draw [->] (0.94,0.96) -- (4.34,2.82);
	\draw [->] (5.06,2.92) -- (4.34,2.82);
	\draw [->] (5.06,2.92) -- (5.5,0.1);
	\draw [->] (5.06,2.92) -- (6,0);
	\begin{scriptsize}
	\draw [fill=ududff] (0.94,0.96) circle (2.5pt);
	\draw[color=ududff] (0.72,1.43) node {$A$};
%	\draw [fill=ududff] (4.34,2.82) circle (2.5pt);
%	\draw [fill=ududff] (5.5,0.1) circle (2.5pt);
%	\draw [fill=ududff] (-0.06,-1.7) circle (2.5pt);
%	\draw [fill=ududff] (-2.32,1.66) circle (2.5pt);
%	\draw [fill=ududff] (2.2,3.56) circle (2.5pt);	
	\draw [fill=ududff] (5.06,2.92) circle (2.5pt);
	\draw[color=black] (1.46,2.47) node {$s$};
	\draw[color=black] (-0.66,1.17) node {$v$};
	\draw[color=black] (0.62,-0.23) node {$w$};
	\draw[color=black] (3.28,0.79) node {$u$};
	\draw[color=black] (2.64,2.15) node {$t$};
	\draw[color=ududff] (5.2,3.29) node {$B$};
	\draw[color=black] (4.8,2.7) node {$x$};
	\draw[color=black] (5,1.5) node {$y$};	
	\draw[color=black] (5.8,1.5) node {$z$};
	\end{scriptsize}
	\end{tikzpicture}
	\]we see that $s,t,u,v,w$ are vectors in $E_A$, but not in $E_B$, whereas $x,y,z$ are vectors in $E_B$, but not in $E_A$.
\end{ex}

\begin{lemma}
	Let $V_A$ denote the set of all vectors in $E_A$. Then $E_A\iso V_A$.
\end{lemma}
\begin{proof}
	Consider the function $t:V_A\to E_A$ that takes any vector $v\in V_A$ to its endpoint $t(v)$, and takes the null vector to $A$.
	
	\begin{itemize}
		\item $t$ is injective:
		
		If $t(v)=t(u)$ for some $v,u\in V_A$, then $v$ and $u$ have the same endpoints. Since they also have the same starting points (by definition), they are equal - hence, $v=u$ and $t$ is injective.
		
		\item $t$ is surjective:
		
		Let $P\in E_A$ be a point. Consider the directed segment $\overrightarrow{AP}$. It is, by definition, a vector in $V_A$ whose endpoint is $P$, and, hence, $t(\overrightarrow{AP})=P$, and so $t$ is surjective.
	\end{itemize}

\begin{df}
	Given any vector $v\in V_A$, we define its \textbf{magnitude} or \textbf{size} or \textbf{norm} to be $\lVert v\rVert:=\lvert AP\rvert$, where $v=\overrightarrow{AP}$.
\end{df}

Since $t$ is both injective and surjective, it is a bijection. This proves the result.
\end{proof}

Let now $V^{r,s}_A$ be the following set: Take any two perpendicular lines $r$ and $s$ through $A$. Then, $V^{r,s}_A$ will be the set $(V_A\cap r)\times (V_A\cap s)$ - that is, the set of all pairs vectors of $E_A$ such that the first vector lies entirely in $r$ and the second vector lies entirely in $s$.

\begin{lemma}
	For any $E_A$, we have that $V_A\iso V^{r,s}_A$.
\end{lemma}
\begin{proof}
	Let $f:V_A\to V^{r,s}_A$ be the following function: For any vector $\overrightarrow{AP}$ in $V_A$, we can consider the parallel to $r$ through $P$, $r_P$ and the parallel to $s$ through $P$, $s_P$.
	
	Since $r\parallel r_P$, we have that $r\cap r_P=\varnothing$, and similarly we have that $s\cap s_P=\varnothing$.
	
	But since $r\nparallel s_P$ and they are both lines, we have that $r\cap s_P$ is a single point - let's call it $P_r$. Similarly, we see that $s\cap r_P$ is a single point - let's call it $P_s$.
	
	But now, $P_r\in r$, by definition. So $\overrightarrow{AP_r}$ is a vector which lies entirely in $r$. Similarly, $P_s\in s$ so $\overrightarrow{AP_s}$ is a vector which lies entirely in $s$.
	
	We then define $f(\overrightarrow{AP}):=(\overrightarrow{AP_r},\overrightarrow{AP_s})$.
	\[\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\clip(-0.86,0.62) rectangle (8.56,7.02);
	\draw [->] (0.5,3.04) -- (7.5,4.08);
	\draw [->] (0.5,3.04) -- (1.0909987984666145,5.574475616501058);
	\draw [->] (0.5,3.04) -- (6.909001201533385,1.5455243834989414);
	\draw [domain=-0.86:8.56] plot(\x,{(--28.1568-2.08*\x)/8.92});
	\draw [domain=-0.86:8.56] plot(\x,{(--1.8632--8.92*\x)/2.08});
	\draw [dotted,domain=-0.86:8.56] plot(\x,{(-58.4136--8.92*\x)/2.08});
	\draw [dotted,domain=-0.86:8.56] plot(\x,{(-51.9936--2.08*\x)/-8.92});
	\begin{scriptsize}
	\draw [fill=ududff] (0.5,3.04) circle (2.5pt);
	\draw[color=ududff] (0.76,2.71) node {$A$};
	\draw[color=black] (-5.62,4.33) node {$r$};
	\draw[color=black] (2.16,9.29) node {$s$};
	\draw [fill=ududff] (7.5,4.08) circle (2.5pt);
	\draw[color=ududff] (7.76,4.37) node {$P$};
	\draw [fill=uuuuuu] (1.0909987984666145,5.574475616501058) circle (2.0pt);
	\draw[color=uuuuuu] (1.39,5.92) node {$P_s$};
	\draw [fill=uuuuuu] (6.909001201533385,1.5455243834989414) circle (2.0pt);
	\draw[color=uuuuuu] (7.29,1.94) node {$P_r$};
	\draw[color=uuuuuu] (3.29,2) node {$r$};
	\draw[color=uuuuuu] (0.5,4.5) node {$s$};
	\end{scriptsize}
	\end{tikzpicture}\]
	
	\begin{itemize}
		\item $f$ is injective:
		
		Let $v,u$ be vectors in $V_A$ such that $f(v)=f(u)$. This means that $v=\overrightarrow{AP}$ and $u=\overrightarrow{AQ}$ for some uniquely determined points $P,Q$ in $E_A$. Now, by definition, $f(v)=f(\overrightarrow{AP})=(\overrightarrow{AP_r},\overrightarrow{AP_s})$ and $f(u)=f(\overrightarrow{AQ})=(\overrightarrow{AQ_r},\overrightarrow{AQ_s})$.
		
		But $f(v)=f(u)$ implies $\overrightarrow{AP_r}=\overrightarrow{AQ_r}$ and $\overrightarrow{AP_s}=\overrightarrow{AQ_s}$, by definition of set product.
		
		But then this means that $P_r=Q_r$ and $P_s=Q_s$.
		
		This means that both $P$ and $Q$ belong to both of $r_P$ and $s_P$. But $r_P$ and $s_P$ are lines, so their intersection has, at most, one point. This means that $P=Q$, and so $f$ is injective.
		
		\item $f$ is surjective:
		
		Take any $(v,u)\in V^{r,s}_A$. Then, there are uniquely determined $P,Q\in E_A$ such that $v=\overrightarrow{AP}$ and $u=\overrightarrow{AQ}$.
		
		Now, consider the lines $r_Q$ and $s_P$ defined, respectively, to be the lines parallel to $r$ through $Q$, and parallel to $s$ through $P$.
		
		Since $s_P\parallel s\nparallel r\parallel r_Q$, we see that $s_P\nparallel r_Q$ and so $s_P\cap r_Q$ is a single point - T.
		
		It is, then, easy tos see that $f(\overrightarrow{AT})=(v,u)$, so $f$ is surjective.
	\end{itemize}

It follows then that $f$ is indeed a bijection, which ends the proof.
\end{proof}

Finally, we can prove the result we've all been waiting for:

\begin{theorem}
	Let $E_A$ be a pointed Euclidean plane. Then $V^{r,s}_A\iso \R^2$.
\end{theorem}
\begin{proof}
	Take any two points $R\in r$ and $P\in p$, respectively, both different from $A$.
	
	Now, given any $v\in r$ a vector lying entirely in $r$, we say that $v$ is \textbf{positive} if the endpoint of $v$ lies in the same side of the semiplane defined by $s$ as $R$, \textbf{zero} if $v=\overrightarrow{AA}$ the null vector, and \textbf{negative} otherwise.
	
	Similarly, given $u\in s$ a vector lying entirely in $s$, we say that $u$ is \textbf{positive} if the endpoint of $u$ lies in the same side of the samiplane defined by $r$ as $S$, \textbf{zero} if $u=\overrightarrow{AA}$ the null vector, and \textbf{negative} otherwise.
	
	This can be seen as a function $\mathrm{sgn}:V^{r,s}_A\to\{0,1,-1\}$.
	
	That said, we're gonna define a function $f:V^{r,s}_A\to \R^2$ by putting $$f(v,u):=(\mathrm{sgn}(v)\lVert v\rVert,\mathrm{sgn}(u)\lVert u\rVert).$$ We claim that $f$ is the bijection we're looking for.
	
	\begin{itemize}
		\item $f$ is injective:
		
		Let $(v,u),(v',u')\in V^{r,s}_A$ be two elements such that $f(v,u)=f(v',u')$. This means that $\mathrm{sgn}(v)\norm{v}=\mathrm{sgn}(v')\norm{v'}$ and $\mathrm{sgn}(u)\norm{u}=\mathrm{sgn}(u')\norm{u'}$. Since $\norm-$ is always a non-negative number, we see that $\mathrm{sgn}(v)\norm{v}=\mathrm{sgn}(v')\norm{v'}$ if, and only if, $\mathrm{sgn}(v)=\mathrm{sgn}(v')$, and similarly for $\mathrm{sgn}(u)=\mathrm{sgn}(u')$.
		
		Now this implies, together with the equations $\mathrm{sgn}(v)\norm{v}=\mathrm{sgn}(v')\norm{v'}$ and $\mathrm{sgn}(u)\norm{u}=\mathrm{sgn}(u')\norm{u'}$, that $\norm{v}=\norm{v'}$ and $\norm{u}=\norm{u'}$. But since the norm is entirely determined by the endpoint (since the starting point is fixed, being $A$), we see that $\norm{v}=\norm{v'}$ if, and only if, $v$ and $v'$ have the same endpoints. Similarly, we see that $u$ and $u'$ have the same endpoints.
		
		Finally, since $v$ and $v'$ have the same endpoints they must be equal, and the same goes for $u$ and $u'$.
		
		It follows that $(v,u)=(v',u')$ and so $f$ is indeed injective.
		
		\item $f$ is surjective:
		
		Take any $(x,y)\in \R^2$. Now draw two circles centered in $A$: one with radius $\abs{x}$ and one with radius $\abs{y}$. Call these circles $C_x$ and $C_y$, resp.
		
		Since $C_x$ is a circle and $r$ is a line which contains a point inside the circle, we know that $C_x\cap r$ is precisely two points - $R_1,R_2$. Similarly, we know that $C_y\cap s$ is precisely two points - $S_1,S_2$.
		
		Now, we take $\overrightarrow{x}$ to be the vector ending at either $R_1$ or $R_2$ such that $\mathrm{sgn}(\overrightarrow{x})=\mathrm{sgn}(x)$. Similarly, we define $\overrightarrow{y}$ to be the vector ending at either $S_1$ or $S_2$ such that $\mathrm{sgn}(\overrightarrow{y})=\mathrm{sgn}(y)$.
		
		Now, by construction, $(\overrightarrow{x},\overrightarrow{y})\in V^{r,s}_A$ is such that $\mathrm{sgn}(\overrightarrow{x})\norm{\overrightarrow{x}}=x$ and $\mathrm{sgn}(\overrightarrow{y})\norm{\overrightarrow{y}}=y$, so $f(\overrightarrow{x},\overrightarrow{y})=(x,y)$ and we see that $f$ is surjective.
	\end{itemize}

Finally, we can conclude that $f$ is the bijection we were looking for. This ends the proof of the theorem.
\end{proof}
\begin{cor}
	By composition of isomorphisms we have:
	
	\[E_A\iso V_A\iso V^{r,s}_A\iso\R^2,\]so the elements of $\R^2$ can be thought of as vectors in the pointed Euclidean plane $E_A$.
\end{cor}

\begin{lemma}
	In the above bijection $f:V^{r,s}_A\to\R^2$, the image of $r$ is the set
	\[\R\times\{0\}:=\{(x,y)\in \R^2\mid y=0\}\]and the image of $s$ is the set
	\[\{0\}\times \R:=\{(x,y)\in\R^2\mid x=0\}.\]
\end{lemma}
\begin{proof}
	Take $(v,\overrightarrow{AA})\in V^{r,s}_A$ any vector lying entirely on $r$. Then, by definition,
	\[f(v,\overrightarrow{AA})=(\mathrm{sgn}(v)\norm{v},\mathrm{sgn}(\overrightarrow{AA})\norm{\overrightarrow{AA}}),\]but both $\mathrm{sgn}(\overrightarrow{AA})$ and $\norm{\overrightarrow{AA}}$ equal 0. So $f(v,\overrightarrow{AA})=(\mathrm{sgn}(v)\norm{v},0)$.
	
	This shows that $f(r)\subseteq \R\times\{0\}$.
	
	Conversely, take any $(x,0)\in \R\times\{0\}$. It is clearly the image of some vector pair $(\overrightarrow{x},\overrightarrow{AA})$, and so $\R\times\{0\}\subseteq f(r)$.
	
	We can argue analogously and show that $f(s)=\{0\}\times\R$.
	
	This ends the proof.
\end{proof}

\begin{df}
	We will denote the sets $\R\times\{0\}$ and $\{0\}\times \R$ the \textbf{$X$-axis} and the \textbf{$Y$-axis}, respectively. Sometimes we'll also denote them, respectively, by $\mb{X}$ and $\mb Y$.
\end{df}

\begin{rmk}
	Notice that, as sets, both the $X$- and the $Y$-axis are in bijection with $\R$, and with a line (the $X$-axis is in bijection with $r$ and the $Y$-axis is in bijection with $s$).
	
	Therefore, $\R$ is in bijection with a line. So it makes sense to think of the set of real numbers as a \textit{line}, and call it the \textbf{real line}.
\end{rmk}

Finally, to end this section, one last result:

\begin{lemma}
	Take $t\in E_A$ any line through $A$. Then there is some point $(x_t,y_t)\in \R^2$ such that $f(t)=\{(x,y)\in\R^2\mid x=\lambda x_t \mbox{ and }y=\lambda y_t,\mbox{ for some }\lambda\in\R\}$.
\end{lemma}
\begin{proof}
	Let $\R(x_t,y_t)$ denote the set $\{(x,y)\in\R^2\mid x=\lambda x_t \mbox{ and }y=\lambda y_t,\mbox{ for some }\lambda\in\R\}$.
	
	Draw $\mc C(A,1)$ the circle of radius $1$ centered at $A$. Since $t$ contains a point in the inside of the circle, $t\cap \mc C(A,1)=2$. Choose any of those two points and call it $P$.
	
	Now, let us define $(x_t,y_t):=f(\overrightarrow{AP_r},\overrightarrow{AP_s})$.
	
	Now take any point $T\in t$, and look at the vectors $\overrightarrow{AT_r}$ and $\overrightarrow{AT_s}$. Now, by using similar triangles we see that 
	\[\dfrac{\mathrm{sgn}(\overrightarrow{AT_r})\norm{\overrightarrow{AT_r}}}{x_t}=\dfrac{\mathrm{sgn}(\overrightarrow{AT_s})\norm{\overrightarrow{AT_s}}}{y_t}=\frac{\mathrm{sgn(\overrightarrow{AT})\norm{\overrightarrow{AT}}}}{\mathrm{sgn}(\overrightarrow{AP})\norm{\overrightarrow{AP}}}\]
	\[\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-1.2799999999999998,
	xmax=6.560000000000002,
	ymin=-1.3500000000000016,
	ymax=4.3500000000000005,
	yticklabels={},	
	xticklabels={},	
	xlabel=$r$,
	ylabel=$s$,]
	\clip(-1.28,-1.35) rectangle (6.56,4.35);
	\draw [domain=-1.28:6.56] plot(\x,{(-0.--1.67*\x)/3.04});
	\draw [dotted] (5.261710485848469,-1.35) -- (5.261710485848469,4.35);
	\draw [dotted] (3.04,-1.35) -- (3.04,4.35);
	\draw [dotted,domain=-1.28:6.56] plot(\x,{(-1.67-0.*\x)/-1.});
	\draw [dotted,domain=-1.28:6.56] plot(\x,{(-2.890479115581231-0.*\x)/-1.});
	\draw [->] (0.,0.) -- (3.04,0.);
	\draw [->] (0.,0.) -- (5.261710485848469,0.);
	\draw [->] (0.,0.) -- (0.,1.67);
	\draw [->] (0.,0.) -- (0.,2.890479115581231);
	\draw [->] (0.,0.) -- (3.04,1.67);
	\draw [->] (0.,0.) -- (5.261710485848469,2.890479115581231);
	\begin{scriptsize}
	\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	\draw[color=uuuuuu] (0.22,-0.28) node {$A$};
	\draw [fill=ududff] (3.04,1.67) circle (2.5pt);
	\draw[color=ududff] (3.24,1.48) node {$P$};
	\draw [fill=xdxdff] (5.261710485848469,2.890479115581231) circle (2.5pt);
	\draw[color=xdxdff] (5.54,2.66) node {$T$};
	\draw [fill=uuuuuu] (3.04,0.) circle (2.0pt);
	\draw[color=uuuuuu] (3.27,-0.4) node {$P_r$};
	\draw [fill=uuuuuu] (0.,1.67) circle (2.0pt);
	\draw[color=uuuuuu] (-0.35,1.9) node {$P_s$};
	\draw [fill=uuuuuu] (0.,2.890479115581231) circle (2.0pt);
	\draw[color=uuuuuu] (-0.33,3.3) node {$T_s$};
	\draw [fill=uuuuuu] (5.261710485848469,0.) circle (2.0pt);
	\draw[color=uuuuuu] (5.51,-0.4) node {$T_r$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]
	
	So we pick $\lambda_T$ to be any of those (since they're all equal).
	
	Clearly, then, by definition of $\lambda_T$, we have that $$f(\overrightarrow{AT_r},\overrightarrow{AT_s})=(\mathrm{sgn}(\overrightarrow{AT_r})\norm{AT_r},\mathrm{sgn}(\overrightarrow{AT_s})\norm{AT_s})=(\lambda_Tx_t,\lambda_Ty_t).$$
	
	Since this holds true for any $T\in t$, we have that $f(t)\subseteq \R(x_t,y_t)$.
	
	\bigskip
	Conversely, take some $(x_t,y_t)\in\R^2$ fixed.
	
	Now, let $P$ be the inverse image of $(x_t,y_t)$ under $f$ (it can be done uniquely since $f$ is bijective), and let $t$ be the line through $A$ that also passes through $P$ (it is also unique, since a line is uniquely determined by two distinct points).
	
	By the same reasoning as before, if we now take the inverse image of $(\lambda x_t,\lambda y_t)$ for any real $\lambda\in \R$ to be some $P^\lambda$, we see that the triangles $AP_rP_s$ and $AP^\lambda_rP^\lambda_s$ are similar, since 
	\[\frac{\overline{AP_r}}{\overline{AP^\lambda_r}}=\frac{\overline{AP_s}}{\overline{AP^\lambda_s}}=\frac{\overline{P_sP_r}}{\overline{P^\lambda_sP^\lambda_r}}=\lambda,\]so $P^\lambda$ lies in $t$.
	
	This shows that $\R(x_t,y_t)\subseteq f(t)$.
	
	Finally, we can conclude that $f(t)=\R(x_t,y_t)$, which ends the proof.
\end{proof}

This shows that the subsets of $\R^2$ where each element is obtained simply by multiplying both coordinates of a fixed vector by a fixed number are \textbf{lines}.

With this we can finally define:

\begin{df}
	We're gonna define the \textbf{multiplication of a point in $\R^2$ by a real number} to be as such:
	\[\lambda(x,y):=(\lambda x,\lambda y)\]for any $\lambda\in \R$ and $(x,y)\in\R^2$.
\end{df}

\begin{rmk}
	By the preceding theorem, this multiplication is simply scaling the vector $(x,y)\in\R^2$ in a straight line to the center to become the same size as $\lambda$ (notice that if $\lambda <0$, this means that the vector will also change sign).
	
	For this reason, this operation is more often than not called the \textbf{scalar multiplication} or \textbf{multiplication by a scalar}.
\end{rmk}

\begin{cor}
	Every line through the point $(0,0)$ in $\R^2$ is just the subset of $\R^2$ formed by all scalar multiples of a fixed vector.
\end{cor}

And to wrap things up, some properties of scalar multiplication:

\begin{prop}
	Let $v,u\in \R^2$ and $\lambda,\mu\in \R$. Then, the following hold:
	\begin{enumerate}[(1)]
		\item (Associative) $(\mu\lambda)v=\mu(\lambda v)$;
		\item (Commutative) $\lambda v=v\lambda$;
		\item (Identity element) There is some $\epsilon\in \R$ such that $\epsilon v=v\epsilon=v$;
		\item (Scalar product distributes over vector sum) $\lambda(v+u)=\lambda v+\lambda u$ and $(v+u)\mu=v\mu+u\mu$;
		\item (Scalar product distributes over scalar sum) $(\mu+\lambda)v=\mu v+\lambda v$.
	\end{enumerate}
\end{prop}
\begin{proof}
	Write, once and for all, $v=(v_1,v_2)$ and $u=(u_1,u_2)$.
	\begin{enumerate}[(1)]
		\item This follows from the fact that real number multiplication is associative:
		\begin{align*}
			(\mu\lambda)v&=(\mu\lambda)(v_1,v_2)\\
			&=((\mu\lambda)v_1,(\mu\lambda)v_2)\\
			&=(\mu(\lambda v_1),\mu(\lambda v_2))\\
			&=\mu(\lambda v_1,\lambda v_2)\\
			&=\mu(\lambda (v_1,v_2))=\mu(\lambda v)
		\end{align*}which holds true since $\mu,\lambda,v_1,v_2$ are real numbers.
		
		\item This follows directly from the fact that real number multiplication is commutative:
		\begin{align*}
			\lambda v&=\lambda(v_1,v_2)\\
			&=(\lambda v_1,\lambda v_2)\\
			&=(v_1\lambda,v_2\lambda)\\
			&=(v_1,v_2)\lambda=v\lambda
		\end{align*}which holds true since $\lambda,v_1,v_2$ are real numbers.
		
		\item This follows from the fact that real number multiplication has an identity:
		\[1v=1(v_1,v_2)=(1v_1,1v_2)=(v_1,v_2)=v.\]
		
		\item This follows from the fact that real number multiplication distributes over real number sums:
		
		\begin{align*}
			\lambda(v+u)&=\lambda((v_1,v_2)+(u_1,u_2))\\
			&=\lambda(v_1+u_1,v_2+u_2)\\
			&=(\lambda(v_1+u_1),\lambda(v_2+u_2))\\
			&=(\lambda v_1+\lambda u_1,\lambda v_2+\lambda u_2)\\
			&=(\lambda v_1,\lambda v_2)+(\lambda u_1+\lambda u2)\\
			&=\lambda (v_1,v_2)+\lambda(u_1,u_2)=\lambda v+\lambda u
		\end{align*}which holds true since $\lambda, v_1,v_2,u_1,u_2$ are real numbers.
		
		Distribution on the right-side holds since we've already proven (2), so $\lambda(v+u)=(v+u)\lambda$ and $\lambda v+\lambda u=v\lambda+u\lambda$.
		
		\item Similar to (4):
		
		\begin{align*}
			(\mu+\lambda)v&=(\mu+\lambda)(v_1,v_2)\\
			&=((\mu+\lambda)v_1,(\mu+\lambda) v_2)\\
			&=(\mu v_1+\lambda v_1,\mu v_2+\lambda v_2)\\
			&=(\mu v_1,\mu v_2)+(\lambda v_1,\lambda v_2)\\
			&=\mu(v_1,v_2)+\lambda(v_1,v_2)=\mu v+\lambda v
		\end{align*}which holds true since $\mu,\lambda,v_1,v_2$ are real numbers.
		
		Distribution on the right follows, again, from (2).
	\end{enumerate}
\end{proof}

So from here onwards, vectors, points and elements of $\R^2$ will be taken as being the same thing, since everything we did in this section was to show that $\R^2$ is a great model of the pointed Euclidean plane.

To end this section, then, let us define:

\begin{df}
	We will call the point $(0,0)\in \R^2$ of the \textbf{origin} or the \textbf{zero} point of $\R^2$, and denote it simply by $0$ when there's no ambiguity.
\end{df}

\begin{prop}
	For any vector $v\in \R^2$ we have that $0v=0$.
\end{prop}
\begin{proof}
	Take any $v\in \R^2$. Now, by what we've already shown, we have:
	\[0v=(0+0)v=0v+0v\]and so, by subtracting $0v$ on both sides, we get $0v=0$, just as we wanted.
\end{proof}

\newpage
\section{The realest plane of vectors}
\subsection{Getting into shape}

So, now that we know what $\R^2$ is, we're gonna study how functions interact with it.

\begin{ex}
	Consider the two following functions $f,g:\R^2\to\R^2$ given by $f(x,y):=(x,x^2)$ and $g(x,y):=(2x-y,x+y)$ respectively.
	
	It's not hard to see that the first function takes the line $y=0$ (that is, the $X$-axis) into the parabola $y=x^2$. In particular, notice that this function doesn't preserve \textit{any} lines through zero.
	
	Conversely, we claim that $g$ does preserve \textit{every} line through zero. To see this, we will show that $g$ preserves scalar multiplication. That suffices, by definition of lines through zero:
	
	Indeed, if $r$ is the line of all scalar multiples of a vector $v$, then any vector $w\in r$ is of the form $\lambda v$ for some $\lambda \in \R$. Let $g(v)=v'\in\R^2$ be the image of $v$ under $g$. If we can prove that $g$ preserves scalar multiplication, then $g(w)=g(\lambda v)=\lambda g(v)=\lambda v'$, and so every point in the line of the multiples of $v$ goes to the line of the multiples of $v'$.
	
	So it remains to show that $g$ preserves scalar multiplication. But that's easy: Take any $(x,y)\in \R^2$ and any $\lambda\in\R$. Then:
	\begin{align*}
		g(\lambda(x,y))&=g(\lambda x,\lambda y)\\
		&=(2(\lambda x)-(\lambda y),(\lambda x)+(\lambda y))\\
		&=(2\lambda x-\lambda y,\lambda x+\lambda y)\\
		&=(\lambda(2x-y),\lambda(x+y))\\
		&=\lambda(2x-y,x+y)=\lambda g(x,y)
	\end{align*}and so we see that $g$ preserves scalar multiplication - and, therefore, preserves any line through the origin.
	
	\bigskip
	There's only one slight issue: Indeed, for a function to preserve lines it suffices for it to preserve scalar multiplication, but there's no telling what happens to the origin during this process. Think about it: Just because every line becomes another line after applying a certain function, that doesn't mean that every point must stand still.
	
	For instance, consider the function $h:\R^2\to\R^2$ defined by $h(x,y):=(x,x)$ if $y=0$ and $h(x,y):=(y,y)$ otherwise. Then:
	\[h(\lambda(x,0))=h(\lambda x,\lambda\cdot0)=(\lambda x,\lambda x)=\lambda(x,x)=\lambda h(x,0)\]
	\[h(\lambda(x,y))=h(\lambda x,\lambda y)=(\lambda y,\lambda y)=\lambda(y,y)=\lambda h(x,y)\]and so $h$ preserves scalar multiplication - and hence preserves lines. However, as we'll see further ahead, this function is still not good enough - for one simple reason: It might preserve lines, but it doesn't preserve parallelograms.
\end{ex}

\begin{prop}
	Given any two vectors $v,u\in \R^2$, consider the following construction:
	\begin{enumerate}
		\item Draw the two vectors $v,u\in \R^2$;
		\item Take $r_v$ the line parallel to $v$ and containing the endpoint of $u$ (there's a unique such line);
		\item Take $r_u$ the line parallel to $u$ and containing the endpoint of $v$ (there's a unique such line).
	\end{enumerate}
\[\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\begin{axis}[
x=1.0cm,y=1.0cm,
axis lines=middle,
xmin=-0.9199999999999992,
xmax=9.040000000000004,
ymin=-1.0600000000000016,
ymax=5.980000000000001,
yticklabels={},	
xticklabels={},]
\clip(-0.92,-1.06) rectangle (9.04,5.98);
\draw [->] (0.,0.) -- (2.,4.);
\draw [->] (0.,0.) -- (6.,1.);
\draw [dotted,domain=-0.92:9.04] plot(\x,{(-22.--4.*\x)/2.});
\draw [dotted,domain=-0.92:9.04] plot(\x,{(--22.--1.*\x)/6.});
\draw [->,dash pattern=on 3pt off 3pt] (0.,0.) -- (8.,5.);
\begin{scriptsize}
\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
\draw [fill=ududff] (2.,4.) circle (2.5pt);
\draw[color=black] (0.86,2.37) node {$v$};
\draw [fill=ududff] (6.,1.) circle (2.5pt);
\draw[color=black] (3.5,0.4) node {$u$};
\draw [fill=uuuuuu] (8.,5.) circle (2.0pt);
\draw[color=black] (3.86,4.8) node {$r_u$};

\draw[color=black] (3.86,2.81) node {$v+u$};

\draw[color=black] (7.5,2.81) node {$r_v$};
\end{scriptsize}
\end{axis}
\end{tikzpicture}\]

	Then, $v+u=r_u\cap r_v$.
\end{prop}

\begin{proof}
	First, how do we know that $r_v\cap r_u$ is non-empty? Well, by construction, $r_v\parallel v$ and $r_u\parallel u$. But $v$ and $u$ meet at 0, so $r_v$ and $r_u$ must also meet.
	
	Second, since $r_v$ and $r_u$ are lines, they have, at most, a single meet point. Since we already know they have a non-empty intersection, then we know that they meet at a single point.
	
	Call this point $P\in \R^2$.
	
Consider now the following triangles:

\[\definecolor{zzttqq}{rgb}{0.6,0.2,0.}
\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\begin{axis}[
x=1.0cm,y=1.0cm,
axis lines=middle,
xmin=-0.9399999999999992,
xmax=9.020000000000005,
ymin=-0.9400000000000015,
ymax=5.980000000000001,
yticklabels={},	
xticklabels={},]
\clip(-0.94,-0.94) rectangle (9.02,5.98);
\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (0.,0.) -- (2.,0.) -- (2.,4.) -- cycle;
\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (6.,1.) -- (8.,1.) -- (8.,5.) -- cycle;
\draw [->] (0.,0.) -- (2.,4.);
\draw [->] (0.,0.) -- (6.,1.);
\draw [dotted,domain=-0.94:9.02] plot(\x,{(-22.--4.*\x)/2.});
\draw [dotted,domain=-0.94:9.02] plot(\x,{(--22.--1.*\x)/6.});
\draw [->,dash pattern=on 3pt off 3pt] (0.,0.) -- (8.,5.);
\draw [dotted] (6.,-0.94) -- (6.,5.98);
\draw [dotted] (8.,-0.94) -- (8.,5.98);
\draw [dotted] (2.,-0.94) -- (2.,5.98);
\draw [color=zzttqq] (0.,0.)-- (2.,0.);
\draw [color=zzttqq] (2.,0.)-- (2.,4.);
\draw [color=zzttqq] (2.,4.)-- (0.,0.);
\draw [dotted] (-1,1) -- (9,1);
\draw [color=zzttqq] (6.,1.)-- (8.,1.);
\draw [color=zzttqq] (8.,1.)-- (8.,5.);
\draw [color=zzttqq] (8.,5.)-- (6.,1.);
\begin{scriptsize}
\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
\draw [fill=ududff] (2.,4.) circle (2.5pt);
\draw[color=black] (0.86,2.37) node {$v$};
\draw [fill=ududff] (6.,1.) circle (2.5pt);
\draw[color=black] (3.5,0.2) node {$u$};
\draw[color=black] (9.39,8.32) node {$r_v$};
\draw[color=black] (-3.51,2.98) node {$r_u$};
\draw [fill=uuuuuu] (8.,5.) circle (2.0pt);
\draw[color=uuuuuu] (8.26,5.29) node {$P$};
\draw [fill=uuuuuu] (2.,0.) circle (2.0pt);
\draw[color=uuuuuu] (2.25,-0.25) node {$v_1$};
\draw [fill=uuuuuu] (8.,1.) circle (2.0pt);
\draw[color=uuuuuu] (8.2,1.33) node {$u'$};
\draw [fill=uuuuuu] (6.,0.) circle (2.0pt);
\draw[color=uuuuuu] (6.23,-0.30) node {$u_1$};
\draw [fill=uuuuuu] (8.,0.) circle (2.0pt);
\draw[color=uuuuuu] (8.27,-0.28) node {$P_1$};
\end{scriptsize}
\end{axis}
\end{tikzpicture}\]

First, notice that segments $u_1P_1$ and $uu'$ are congruent (since $uu'P_1u_1$ is a rectangle, by construction).

Now we prove that the segments $0v_1$ and $uu'$ are congruent. To do that, we use the triangles $0v_1v$ and $uu'P$. We claim they are congruent: Indeed, both are right triangles (on $v_1$ and $u'$, respectively), the angles $v0v_1$ and $Puu'$ are the same (since the segment $Pu$ is parallel to $v0$, by construction) and the segments $0v$ and $Pu$ are congruent (once again, by construction).

This implies that the triangles are congruent and, therefore, $vv_1$ is congruent to $Pu'$, and $0v_1$ is congruent to $uu'$.

Now, it's easy to see that the segment $0P_1$ is just the concatenation of the segments $0u_1$ and $u_1P_1$, so the length of $0P_1$ is the sum of the lengths of $0u_1$ and $u_1P_1$.

But the length of $0u_1$ is, by definition, $u_1$, and the length of $u_1P_1$ is the length of $uu'$, which, as we've shown, is the length of $0v_1$ - which, once again by definition, is just $v_1$.

So the length of $0P_1$ - which is just $P_1$ - is the sum of $u_1$ and $v_1$ - that is, $P_1=u_1+v_1$.

\bigskip
We can proceed analogously on the $Y$-axis and show that $P_2=u_2+v_2$, which finally shows that $P=v+u$, just as stated.
\end{proof}

\begin{cor}
	Any line $r\subseteq \R^2$ is of the form $r=v+r'$ where $v$ is a fixed vector and $r'$ is a line through zero which is parallel to $r$.
\end{cor}
\begin{proof}
	There are two possible cases:
	\begin{enumerate}
		\item \underline{$r\cap\mb Y\neq\varnothing$}.
		
		In this case, take $v:=r\cap\mb Y$ and draw $r'\parallel r$ through $0$.
		
		Now given any $w\in r$, we can take a line parallel to $v$ through $w$, and since $v$ cuts $r'$, this new line will also cut $r'$. Call this new point $u$.
		
		\[\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
		\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=.4cm,y=.4cm]
		\begin{axis}[
		x=.7cm,y=.7cm,
		axis lines=middle,
		xmin=-0.8999999999999992,
		xmax=7.0000000000000036,
		ymin=-1.0000000000000016,
		ymax=7.000000000000002,
		yticklabels={},	
		xticklabels={},]
		\clip(-0.9,-1.) rectangle (7.,7.);
		\draw [domain=-0.9:7.] plot(\x,{(--34.2344--3.*\x)/8.84});
		\draw [domain=-0.9:7.] plot(\x,{(-0.--3.*\x)/8.84});
		\draw [->] (0.,0.) -- (0.,3.872669683257919);
		\draw [->] (0.,0.) -- (6.272729409172696,6.001424007637792);
		\draw [dash pattern=on 2pt off 2pt] (6.272729409172696,-1.) -- (6.272729409172696,7.);
		\draw [->] (0.,0.) -- (6.272729409172696,2.1287543243798743);
		\begin{scriptsize}
		\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
		\draw [fill=uuuuuu] (0.,3.872669683257919) circle (2.0pt);
		\draw[color=black] (-0.32,1.81) node {$v$};
		\draw [fill=xdxdff] (6.272729409172696,6.001424007637792) circle (2.5pt);
		\draw[color=black] (3.02,3.25) node {$w$};
		\draw [fill=uuuuuu] (6.272729409172696,2.1287543243798743) circle (2.0pt);		
		\draw[color=black] (3.02,1.5) node {$u$};
		\draw[color=black] (6.7,5.8) node {$r$};
		\draw[color=black] (6.7,1.8) node {$r'$};
		\end{scriptsize}
		\end{axis}
		\end{tikzpicture}\]Clearly, then, by construction, we see that $w=v+u$. 
		
		\item \underline{$r\cap\mb Y=\varnothing$}.
		
		In this case, just take $v:=r\cap\mb X$. We know this point exists because $\mb X\perp\mb Y$ and $r\parallel\mb Y$ together imply $r\perp\mb X$, and so $r\cap\mb X\neq\varnothing$.
		
		Now we can just proceed as in the previous case. We'll skip the details of the proof.
	\end{enumerate}

This shows that every point in $r$ can be written as $v+r'$, which ends the proof.
\end{proof}
\begin{cor}
	Every line in $\R^2$ is of the form $v+\R u$ for some $v,u\in \R^2$.
\end{cor}

\begin{ex}
	Consider the vectors $v=(2,1)$, $u=(1,3)$.  Now, we can construct various lines: $\R v$, $\R u$, $v+\R u$, $u+\R v$ and $\R(v+u)$, for instance.
	
	By the previous discussion, we know that $v+\R u$ and $u+\R v$ don't pass through zero, and all the other lines do.
	
	Here's a sketch of these lines:
	\[\definecolor{yqqqyq}{rgb}{0.5019607843137255,0.,0.5019607843137255}
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\definecolor{qqqqff}{rgb}{0.,0.,1.}
	\definecolor{ffqqqq}{rgb}{1.,0.,0.}
	\definecolor{ffdxqq}{rgb}{1.,0.8431372549019608,0.}
	\definecolor{qqwuqq}{rgb}{0.,0.39215686274509803,0.}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-0.9177598207939823,
	xmax=4.01551855273497,
	ymin=-0.9650502335857208,
	ymax=5.001090829717888,
	yticklabels={},	
	xticklabels={},]
	\clip(-0.9177598207939823,-0.9650502335857208) rectangle (4.01551855273497,5.001090829717888);
	\draw [->] (0.,0.) -- (1.,3.) node [midway, above, sloped] {$u$};
	\draw [->] (0.,0.) -- (2.,1.) node [midway, above, sloped] {$v$};
	\draw [color=green,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(-0.--1.*\x)/2.});
	\draw [color=yellow,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(-0.--3.*\x)/1.});
	\draw [color=red,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(--5.--1.*\x)/2.});
	\draw [color=blue,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(-5.--3.*\x)/1.});
	\draw [->] (0.,0.) -- (3.,4.) node [midway, above, sloped] {$v+u$};
	\draw [color=purple,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(-0.--4.*\x)/3.});
	\begin{scriptsize}
	\draw [fill=xdxdff] (0.,0.) circle (2.5pt);
	\draw [fill=ududff] (2.,1.) circle (2.5pt);
	\draw [fill=ududff] (1.,3.) circle (2.5pt);
	\draw [fill=uuuuuu] (3.,4.) circle (2.0pt);	
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]with the following coloring: {\color{green}{$\R v$}}, {\color{yellow}{$\R u$}}, {\color{red}{$\R v+u$}}, {\color{blue}{$v+\R u$}} and {\color{purple}{$\R(v+u)$}}.
\end{ex}

\begin{ex}
	Now let's resume the discussion from the previous example: Just to refresh, we're considering the function $h:\R^2\to \R^2$ defined by
	\[h(x,y):=\begin{cases}
	(x,x),&\mbox{ if }y=0\\
	(y,y),&\mbox{ otherwise}
	\end{cases}\]and we've seen that this function does preserve lines through 0. But what does it do with parallelograms?
	
	For instance, take the line points $v=(2,4)$ and $u=(2,-4)$. Then, by what we've already shown, the point $v+u=(4,0)$ is precisely the remaining vertex of the parallelogram determined by $v$ and $u$. But look what happens if we try applying the same rule to $h$:
	\[h(v+u)=h(4,0)=(4,4)\]but
	\[h(v)+h(u)=h(2,4)+h(2,-4)=(4,4)+(-4,-4)=(0,0)\]so $h$ didn't preserve our parallelogram!
	
	\[\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-4.948163022910453,
	xmax=4.998779844455295,
	ymin=-4.9803933515895045,
	ymax=4.949458548650205,
	yticklabels={},	
	xticklabels={},]
	\clip(-4.948163022910453,-4.9803933515895045) rectangle (4.998779844455295,4.949458548650205);
	\draw [->] (0.,0.) -- (2.,-4.) node[midway, above, sloped] {$u$};
	\draw [->] (0.,0.) -- (2.,4.) node[midway, above, sloped] {$v$};
	\draw [->] (0.,0.) -- (4.,0.) node[midway, above, sloped] {$v+u$};
	\draw [line width=0.8pt,dotted,domain=-4.948163022910453:4.998779844455295] plot(\x,{(-16.--4.*\x)/2.});
	\draw [line width=0.8pt,dotted,domain=-4.948163022910453:4.998779844455295] plot(\x,{(--16.-4.*\x)/2.});
	\draw [->] (0.,0.) -- (4.,4.) node[midway, below, sloped] {$h(v)=h(v+u)$};
	\draw [->] (0.,0.) -- (-4.,-4.) node[midway, above, sloped] {$h(u)$};
	\begin{scriptsize}
	\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	\draw [fill=ududff] (2.,-4.) circle (2.5pt);
	\draw [fill=ududff] (2.,4.) circle (2.5pt);
	\draw [fill=xdxdff] (4.,0.) circle (2.5pt);
	\draw [fill=ududff] (4.,4.) circle (2.5pt);
	\draw [fill=ududff] (-4.,-4.) circle (2.5pt);
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]
	
	This is why $h$ is bad - it is distorting our drawings. We want our functions not only to preserve lines, but also to preserve \textit{parallelograms}. This is the motivation for the following definition, which, I might add, is the single most important definition in this whole chapter.
\end{ex}

\begin{df}
	A function $f:\R^2\to\R^2$ is said to be \textbf{linear} if for every $v,u\in \R^2$ and $\lambda\in \R$ we have:
	\begin{itemize}
		\item $f(v+u)=f(v)+f(u)$;
		\item $f(\lambda v)=\lambda f(v)$.
	\end{itemize}

	We'll denote the set of all linear functions in $\R^2$ by $\hom_\R(\R^2,\R^2)$.
\end{df}

\begin{rmk}
	This notation is, and should be, somewhat enigmatic at this point. Just trust us that it's going to make sense further on.
	
	Much further on.
\end{rmk}

This is precisely the concept we've just built in the example: The first item tells us that a linear function preserves parallelograms, and the second item tells us that a linear function preserves lines.

\begin{ex}
	Finishing up the motivating example, we claim that the function $g:\R^2\to R^2$ we defined before as being $g(x,y)=(2x-y,x+y)$ is a linear function.
	
	We've already proven, when we first introduced it, that $g$ preserves scalar multiplication. Let us then prove that it preserves summation:
	
	\begin{align*}
		g((a,b)+(c,d))&=g(a+c,b+d)\\
		&=(2(a+c)-(b+d),(a+c)+(b+d))\\
		&=(2a+2c-b-d,a+c+b+d)\\
		&=((2a-b)+(2c-d),(a+b)+(c+d))\\
		&=(2a-b,a+b)+(2c-d,c+d)=g(a,b)+g(c,d)
	\end{align*}for any $(a,b),(c,d)\in \R^2$, and so $g$ preserves summation and scalar multiplication - and is, therefore, a linear transformation.
\end{ex}

\begin{ex}
	Let us give some more examples of functions. Let $f,g,h:\R^2\to\R^2$ be given by $f(x,y)=(x,y)$, $g(x,y)=(y,x)$ and $h(x,y)=(x+y-4,y)$. Which ones of these are linear? Well, let's test: For this, fix, once and for all, $(a,b),(c,d)\in \R^2$ and $\lambda\in \R$. Then:
	\begin{align*}
		f((a,b)+(c,d))&=f((a+c,b+d))=(a+c,b+d)=(a,b)+(c,d)=f(a,b)+f(b,d)
	\end{align*}and
	\[f(\lambda (a,b))=f(\lambda a,\lambda b)=(\lambda a, \lambda b)=\lambda (a,b)=\lambda f(a,b)\]so $f$ is indeed linear.
	
	\begin{align*}
		g((a,b)+(c,d))&=g(a+c,b+d)=(b+d,a+c)=(b,a)+(d,c)=g(a,b)+g(c,d)
	\end{align*}and
	\[g(\lambda (a,b))=g(\lambda a,\lambda b)=(\lambda b,\lambda a)=\lambda (b,a)=\lambda g(a,b)\]so $g$ is indeed linear.
	
	$h$, however, isn't linear. It actually fails both checks: To see that, assume $h$ \textit{was} linear. Then $h(\lambda (a,b))=\lambda h(a,b)$ for all $\lambda \in \R$ and $(a,b)\in \R^2$.
	
	In particular, $h(3(1,1))=h(3,3)=(3+3-4,3)=(2,3)$, while $3h(1,1)=3(1+1-4,1)=3(-2,1)=(-6,3)$ and clearly $2\neq -6$ so they can't be equal.
	
	Similarly, $h((1,1)+(1,1))=h(2,2)=(2+2-4,2)=(0,2)$, while $h(1,1)+h(1,1)=(1+1-4,1)+(1+1-4,1)=(-4,2)$ and clearly $0\neq -4$ so they can't be equal.
	
	\bigskip
	It would be great then if we could tell at a glance whether a function is linear or not.
\end{ex}

\begin{lemma}
	Let $f:\R^2\to\R^2$ be a function. Then $f$ is linear if, and only if, there are real numbers $a,b,c,d\in \R$ such that $f(x,y)=(ax+cy,bx+dy)$ for all $(x,y)\in \R^2$.
\end{lemma}
\begin{proof}
	Let $f$ be linear. We want to show that there exists some real numbers $a,b,c,d\in\R$ such that $f(x,y)=(ax+cy,bx+dy)$.
	
	We can start off by seeing that $(x,y)=x(1,0)+y(0,1)$ and so, since $f$ is linear, we get $$f(x,y)=f(x(1,0)+y(0,1))=f(x(1,0))+f(y(0,1))=xf(1,0)+yf(0,1).$$
	
	But now, $f(1,0),f(0,1)\in\R^2$ and so there exist some real numbers $a,b,c,d\in \R$ such that $f(1,0)=(a,b)$ and $f(0,1)=(c,d)$. This allows us to go back and proceed with calculations:
	\[xf(1,0)+yf(0,1)=x(a,b)+y(c,d)=(xa,xb)+(yc,yd)=(xa+yc,xb+yd)\]and finally we see that, since real number multiplication is commutative, we can conclude that $f(x,y)=(ax+cy,bx+dy)$, as previously stated.
	
	\bigskip
	Conversely, let $f:\R^2\to\R^2$ be a function defined by $f(x,y)=(ax+cy,bx+dy)$ for all $(x,y)\in \R^2$, for some $a,b,c,d\in\R$ fixed. We claim that such an $f$ is linear.
	
	\begin{itemize}
		\item \begin{align*}
			f((x,y)+(x',y'))&=f(x+x',y+y')\\
			&=(a(x+x')+c(y+y'),b(x+x')+d(y+y'))\\
			&=(ax+ax'+cy+cy',bx+bx'+dy+dy')\\
			&=((ax+cy)+(ax'+cy'),(bx+dy)+(bx'+dy'))\\
			&=(ax+cy,bx+dy)+(ax'+cy',bx'+dy')=f(x,y)+f(x',y')
		\end{align*}so $f$ preserves sums.
		
		\item \begin{align*}
			f(\lambda (x,y))&=f(\lambda x,\lambda y)\\
			&=(a(\lambda x)+c(\lambda y),b(\lambda x)+d(\lambda y))\\
			&=(a\lambda x+c\lambda y,b\lambda x+d\lambda y)\\
			&=(\lambda (ax+by),\lambda(bx+dy))\\
			&=\lambda(ax+cy,bx+dy)=\lambda f(x,y)
		\end{align*}so $f$ preserves scalar multiplication.
	\end{itemize}

It follows that $f$ is linear, by definition, which ends the proof.
\end{proof}

This proof, however, far from only telling us that all linear functions are of such-and-such form, give us a very powerful tool for dealing with linear functions and vector spaces as a whole.

Re-analyze the proof above. Where did the $a,b,c,d$ come from? They are precisely the images of $(1,0)$ and $(0,1)$ under $f$. What we've shown, then, to some extent, is that the image of any point under a linear function is entirely determined by the images of $(1,0)$ and $(0,1)$.

\begin{theorem}
	Let $f:\R^2\to\R^2$ be a linear function. Then $f$ is uniquely determined by $f(1,0)$ and $f(0,1)$.
\end{theorem}
\begin{proof}
	Basically the same as above: Let's compute the image of $(x,y)$:
	\[f(x,y)=f(x(1,0)+y(0,1))=xf(1,0)+yf(0,1)\]and so if we call $f(1,0)=v$ and $f(0,1)=u$, we see that for any $(x,y)\in\R^2$, we have that $f(x,y)=xv+yu$.
	
	The result now follows.
\end{proof}

This has the following interesting consequence:
\begin{cor}
	Let $f:\{(1,0),(0,1)\}\to\R^2$ be any function. Then there is a unique linear function $\phi:\R^2\to\R^2$ such that $\phi(x,y):=xf(1,0)+yf(0,1)$.
\end{cor}
\begin{proof}
	All that remains is to show that $\phi$ is indeed linear, but even that is pointless: Since $f$ takes both $(1,0)$ and $(0,1)$ into $\R^2$, let $(a,b)=f(1,0)$ and $(c,d)=f(0,1)$. Then we readily see that $\phi(x,y)$ simply becomes
	\[\phi(x,y)=(ax+cy,bx+dy)\]which we've just shown is a linear function.
\end{proof}

This shows that the vectors $(1,0)$ and $(0,1)$ are very special. If you know what to do with them, you know what to do with literally everyone else.

\begin{df}
	We'll denote the vectors $(1,0)$ and $(0,1)$ by $e_1$ and $e_2$, respectively.
\end{df}

\begin{df}
	A finite set $X=\{x_1,x_2,\cdots, x_n\}\subseteq \R^2$ is called a \textbf{base of $\R^2$} or a \textbf{basic set} if $$\hom(X,\R^2)\iso\hom_\R(\R^2,\R^2)$$ that is, given any linear function $f:\R^2\to\R^2$ then for each $(a,b)\in \R^2$ there is a unique choice of $\lambda_1,\lambda_2,\cdots,\lambda_n\in R$ such that $f(a,b)=\lambda_1f(x_1)+\lambda_2f(x_2)+\cdots+\lambda_n f(x_n)$.
\end{df}

\begin{rmk}
	It follows trivially from this definition, taking the linear function $\id_{\R^2}:\R^2\to\R^2$, that given a base $X=\{x_1,x_2,\cdots,x_n\}$, then for any point $v\in\R^2$ there is a unique choice of $\lambda_1,\lambda_2,\cdots,\lambda_n\in\R$ such that $v=\lambda_1x_1+\lambda_2x_2+\cdots+\lambda_nx_n$. 
	
	In other words, every vector can be written uniquely as a linear combination of a fixed base.
\end{rmk}

\begin{ex}
	We claim that $\{(1,1),(1,-1)\}$ is a base.
	
	To see this, take any linear function $f:\R^2\to\R^2$.
	
	We want to write $f(x,y)=\lambda_1 f(1,1)+\lambda_2 f(1,-1)$ for some $\lambda_1,\lambda_2\in\R$. We claim that $\lambda_1:=\dfrac{x+y}{2}$ and $\lambda_2:=\dfrac{x-y}{2}$ are the ones we're looking for.
	
	Indeed:
	\begin{align*}
		\lambda_1 f(1,1)+\lambda_2 f(1,-1)&=\frac{x+y}{2}f(1,1)+\frac{x-y}{2}f(1,-1)\\
		&=f\left(\frac{x+y}{2},\frac{x+y}{2}\right)+f\left(\frac{x-y}{2},\frac{y-x}{2}\right)\\
		&=f\left(\frac{x+y}{2}+\frac{x-y}{2},\frac{x+y}{2}+\frac{y-x}{2}\right)=f(x,y)
	\end{align*}
	
	Assume now that $\lambda_1',\lambda_2'\in \R$ are other such numbers - that is, $f(x,y)=\lambda_1'f(1,1)+\lambda_2'f(1,-1)$. We need to show that $\lambda_1=\lambda_1'$ and $\lambda_2=\lambda_2'$. But:
	
	\begin{align*}
		f(x,y)&=\lambda_1'f(1,1)+\lambda_2'f(1,-1)\\
		&=f(\lambda_1',\lambda_1')+f(\lambda_2',-\lambda_2')\\
		&=f(\lambda_1'+\lambda_2',\lambda_1'-\lambda_2')
	\end{align*}for all linear functions. In particular, for the identity function we see that 
	\[(x,y)=(\lambda_1'+\lambda_2',\lambda_1'-\lambda_2')\]and so $x=\lambda_1'+\lambda_2'$ and $y=\lambda_1'-\lambda_2'$.
	
	Solving this, we see that $\lambda_1'=\dfrac{x+y}{2}=\lambda_1$ and $\lambda_2'=\dfrac{x-y}{2}=\lambda_2$.
	
	This proves that $\lambda_1,\lambda_2$ are indeed unique. 
	
	It follows that $\{(1,1),(1,-1)\}$ is a base.
\end{ex}

\begin{ex}
	For instance, the vector $(2,3)$ can be written in the base $\{(1,1),(1,-1)\}$ uniquely as $(2,3)=\dfrac{5}{2}(1,1)+\dfrac{-1}{2}(1,-1)$.
	
	Indeed:
	\[\dfrac{5}{2}(1,1)+\dfrac{-1}{2}(1,-1)=\left(\dfrac{5}{2},\dfrac{5}{2}\right)+\left(\dfrac{-1}{2},\dfrac{1}{2}\right)=\left(\dfrac{4}{2},\dfrac{6}{2}\right)=(2,3).\]
	
	Similarly, $(1,0)$ written in the base $\{(1,1),(1,-1)\}$ is just $(1,0)=\dfrac{1}{2}(1,1)+\dfrac{1}{2}(1,-1)$.
\end{ex}

The corollary above is telling us that the set $\{e_1,e_2\}$ is a base of $\R^2$.

\begin{df}
	The set $E:=\{e_1,e_2\}$ will be called the \textbf{canonical base} of $\R^2$.
\end{df}

\begin{ex}
	Let $f:E\to\R^2$ be defined by $f(e_1):=(3,-\pi)$ and $f(e_2):=(4,0)$. Then there's a unique linear function $\phi:\R^2\to\R^2$ defined by $\phi(x,y):=xf(e_1)+yf(e_2)$. Let us compute some images of $\phi$:
	
	$$\phi(1,1)=1f(e_1)+1f(e_2)=1(3,-\pi)+1(4,0)=(3,-\pi)+(4,0)=(7,-\pi)$$
	$$\phi(1,0)=1f(e_1)+0f(e_2)=1(3,-\pi)+0(4,0)=(3,-\pi)=f(e_1)$$
	$$\phi(4,5)=4f(e_1)+5f(e_2)=4(3,-\pi)+5(4,0)=(12,-4\pi)+(20,0)=(32,-4\pi)$$
	
	Note that this is the same as defining $\phi(x,y)=(3x+4y,-\pi x)$.
\end{ex}

So for now, what you need to keep in mind regarding bases is that they are important precisely because of this property.

This will allow us to prove many results about vector spaces by simply stating a parallel result in terms of the base, and then using some previously proven set-theory result.

In order to get to those results, however, we'll need to keep on advancing.

\newpage
\subsection{Subspace emissary}

\begin{ex}
	Let $r\subseteq \R^2$ be the line $\R(2,3)$. That is, $v\in r$ if, and only if, $v=\lambda (2,3)$ for some $\lambda\in \R$.
	
	Take then $v,u\in r$. Then, by what we've just said, there are some $\lambda,\mu\in \R$ such that $v=\lambda(2,3)$ and $u=\mu(2,3)$.
	
	We can then ask the question: Is $v+u$ also in $r$? Well, $v+u=\lambda(2,3)+\mu(2,3)=(\lambda+\mu)(2,3)$, and since both $\lambda,\mu$ are real numbers, so is $\lambda+\mu$. This shows that if $v,u\in r$ then so is $v+u\in r$..
	
	Similarly we can ask: Is $\alpha v\in r$ for any $\alpha\in \R$? Once again, $\alpha v=\alpha(\lambda (2,3))=(\alpha\lambda)(2,3)$ and since both $\alpha$ and $\lambda$ are real numbers, so is $\alpha\lambda$. This shows that if $v\in r$, then so is $\alpha v\in r$ for any $\alpha\in \R$.
	
	What this tells us is that lines through zero are \textbf{closed} under addition and scaling - that is, if we add two points on the same line, then they remain on that line, and if we scale any point on a line, it also remains there.
\end{ex}

\begin{prop}
	Let $v\in \R^2$ be any vector, and consider the line $r:=\R v$. Take now any $u\in r$. Then $r=\R u$.
\end{prop}
\begin{proof}
	Let $u\in \R v$ - that is, $u=\mu v$ for some $\mu\in \R$.
	
	Take now $w\in \R v$ - that is, $w=\omega v$ for some $\omega \in \R$. We want to show that $w=\omega' u$, for some $\omega'\in \R$, and therefore $w\in \R u$.
	
	\bigskip
	This is simple: Take $\omega':=\frac{\omega}{\mu}\in \R$, which we know is a real number, since both $\omega$ and $\mu$ are real numbers.
	
	Now \[\omega'u=\omega'\left(\mu v\right)=(\omega'\mu)v=\left(\frac{\omega}{\mu}\mu\right)v=\omega v=w\]shows that $w\in \R u$.
	
	We've just shown that every point in $\R v$ is also in $\R u$. We can proceed analogously and prove that every point in $\R u$ is also in $\R v$.
	
	By definition of set equality, it follows that $\R v=\R u$, which ends the proof.
\end{proof}

\begin{df}
	If $v,u\in \R^2$ are such that $\R v=\R u$ then we say that \textbf{$v$ and $u$ are parallel vectors}, which shall be denoted as $v\parallel u$.
\end{df}
\begin{cor}
	Two vectors $v,u\in \R^2$ are parallel if, and only if, there's some real number $\lambda\in \R$ such that $v=\lambda u$.
\end{cor}

\begin{ex}
	Let $v=(1,1)$ and $u=(-3,-3)$. We claim that $v\parallel u$.
	
	That's easy to see, since $u=-3v$.
	
	Now this shows us immediately that the lines $\R v$ and $\R u$ are the same line: Take any $w\in \R v$. That means that $w=\omega(1,1)=(\omega,\omega)$ for some $\omega\in \R$.
	
	 But now, taking $\omega':=\dfrac{\omega}{-3}$, we see that 
	 \[\omega' u=\left(\frac{\omega}{-3}\right)u=\omega\left(\frac{u}{-3}\right)=\omega v=w\]and we see that $w\in \R u$, just as stated.
\end{ex}

We can now ask the following question: We know that lines through zero are closed under addition and scaling. Are there any other subsets of $\R^2$ that are like that? If so, what are they? If not, then why?

Let us give a name to that property, because it's a mouthful:

\begin{df}
	Let $X\subseteq \R^2$ be a set satisfying
	\begin{itemize}
		\item For all $x,y\in X$, $x+y\in X$;
		\item For all $x\in X$ and $\lambda\in \R$, $\lambda x\in X$.
	\end{itemize}

Then $X$ will be called a \textbf{subspace of $\R^2$} which will be denoted as $X\leq \R^2$.
\end{df}

\begin{ex}
	Clearly, since lines through zero are the motivating example for this definition, we know that for every vector $v\in \R^2$, the line $\R v$ is a subspace of $\R^2$ - that is, $\R v\leq \R^2$.
	
	But, if you think about it, $\R^2\subseteq \R^2$ is a subset of $\R^2$ which is also closed under additions and scalar multiplications (duh). So $\R^2\leq \R^2$.
	
	\bigskip
	Conversely, \textbf{any} line which doesn't contain zero cannot be a subspace: To see this, take any two vectors $v,u\in \R^2$ and consider the line $\R v+u$, which does not contain zero.
	
	Take now $w\in \R v+u$ and any other real number - for instance, 0.
	
	If $\R v+u$ was a subspace, it would be closed under scalar multiplication, but $0w=0\notin\R v+u$, so it cannot be a subspace.
	
	\bigskip
	What else then can be a subspace?
\end{ex}
This example gives us a very nice idea for how to approach subspaces:
\begin{prop}
	Let $X\subseteq \R^2$ be a subspace. Then $0\in X$.
\end{prop}
\begin{proof}
	This can be seen in many ways.
	
	Take $x\in X$. Since $X$ is subspace it is closed under scalar multiplication, so $-x\in X$.
	
	But since $X$ is a subspace it is also closed under addition, so $x+(-x)\in X$.
	
	But $x+(-x)=0$ so $0\in X$, as claimed.
\end{proof}

Now let's try building new subspaces from existing ones:

\begin{lemma}
	If $X,Y\subseteq \R^2$ are two subspaces, then $X\cap Y\subseteq \R^2$ is also a subspace.
\end{lemma}
\begin{proof}
	This is easy to see:
	\begin{itemize}
		\item Take $v,u\in X\cap Y$. This means that $v,u\in X$ and $v,u\in Y$, by definition of intersection. But since $v,u\in X$ and $X$ is a subspace, then $v+u\in X$. Similarly, since $v,u\in Y$ and $Y$ is a subspace, then $v+u\in Y$.
		
		Finally, using once more the definition of intersection, we see that $v+u\in X$ and $v+u\in Y$ implies $v+u\in X\cap Y$, and so $X\cap Y$ is closed under addition.
		
		\item Take $v\in X\cap Y$ and $\lambda\in \R$. Once again, $v\in X\cap Y$ implies $v\in X$ and $v\in Y$, and since both are subspaces, this implies $\lambda v\in X$ and $\lambda v\in Y$.
		
		Finally, using once more the definition of intersection, we see that $\lambda v\in X$ and $\lambda v\in Y$ implies $\lambda v\in X\cap Y$, and so $X\cap Y$ is closed under scalar multiplication.
	\end{itemize}
This shows that $X\cap Y$ is a subspace, which ends the proof.
\end{proof}

With this we can already infer an important result:

\begin{cor}
	The set containing only the origin (called the \textbf{zero} set $0:=\{(0,0)\}$) is a subspace.
\end{cor}

We could check this directly, but that would imply that we even considered it a possible candidate for subspace before. Following this result, however, since lines through zero are subspaces, and since any two of those lines meet precisely at zero, we know, for free, that zero is also a subspace.

\bigskip
Now, I know what you're thinking: If intersection of subspaces is a subspace, then surely the union of subspaces is as well, right?

Well...

\begin{ex}
	Consider the lines $\mb X$ (that is, $\R e_1$) and $\mb Y$ (that is, $\R e_2$). Then $\mb X\cup\mb Y$ is just the two axis together.
	
	It clearly is closed under scalar multiplication, but, sadly, it is not closed under addition. This is very easy to see:
	
	For instance, $e_1\in\mb X$ and $e_2\in\mb Y$, so $e_1,e_2\in \mb X\cup \mb Y$, by definition of set union.
	
	If $\mb X\cup \mb Y$ was a subspace, then $e_1+e_2\in \mb X\cup \mb Y$, but $e_1+e_2=(1,1)$ and we know that $(1,1)\notin \mb X$  and $(1,1)\notin \mb Y$ and, therefore, $(1,1)\notin \mb X\cup \mb Y$.
	
	This shows that, in general, union of subspaces is \textbf{not} a subspace.
\end{ex}

With this we see that the set-theoretical notion of union is \textit{just not good enough} for vector spaces. We need a ``vectorial'' notion of union:

\begin{df}
	Let $X,Y\subseteq \R^2$ be two subspaces. We define the \textbf{sum of $X$ and $Y$} to be the subspace $X+Y$ given by:
	\begin{itemize}
		\item $X+Y$ contains both $X$ and $Y$;
		\item Any other subspace $Z$ that contains both $X$ and $Y$ also contains $X+Y$.
	\end{itemize}
\end{df}

\begin{prop}
	Given any two subspaces $X,Y\subseteq \R^2$, then their sum can be uniquely expressed as
	\[X+Y:=\{v\in \R^2\mid \exists x\in X,\exists y\in Y\mbox{ such that } v=x+y\},\]that is, $X+Y$ is the set of all sums of all elements in both $X$ and $Y$.
\end{prop}
\begin{proof}
	This proof consists of two parts: First, we need to show that this set, let's call it $S$, is indeed a subspace, and then, second, we need to prove that it satisfies the two defining properties of the sum of $X$ and $Y$.
	
	So, just like we said, define
	\[S:=\{v\in \R^2\mid \exists x\in X,\exists y\in Y: v=x+y\}.\] Let us show that $S\leq \R^2$.
	
	To do that, take any $v,v'\in S$ and $\lambda \in \R$. Then, by definition, there are some $x,x'\in X$ and some $y,y'\in Y$ such that $v=x+y$ and $v'=x'+y'$. With this, we can compute:
	\[v+v'=(x+y)+(x'+y')=(x+x')+(y+y')\]and since both $X$ and $Y$ are subspaces, $x+x'\in X$ and $y+y'\in Y$. This shows that $v+v'$ is the sum of $x+x'\in X$ and $y+y'\in Y$, and hence $v+v'\in S$.
	
	Similarly,
	\[\lambda v=\lambda(x+y)=\lambda x+\lambda y\]and, once more, since $X$ and $Y$ are subspaces, $\lambda x\in X$ and $\lambda y\in Y$, and, so, $\lambda v$ is the sum of $\lambda x\in X$ and $\lambda y\in Y$. This shows that $\lambda v\in S$.
	
	This shows that $S$ is indeed a subspace.
	
	\bigskip
	Finally, let us show that it satisfies the definition of sum:
	\begin{itemize}
		\item First, it is easy to see that it contains both $X$ and $Y$: Since $X$ and $Y$ are subspaces, they contain 0. So for every $x\in X$, $x+0\in S$, by definition of $S$. But $x+0=x\in X$. So $X\subseteq S$.
		
		Similarly, for every $y\in Y$, $0+y\in S$, by definition of $S$. But $0+y=y\in Y$, so $Y\subseteq S$.
		
		\item Let $Z$ be another subspace that contains both $X$ and $Y$. 
		
		Take $v\in S$. By definition of $S$, there are some $x\in X$ and 	some $y\in Y$ such that $v=x+y$. But since $Z$ is a subspace that contains $X$ and $Y$, this means that $x\in Z$ and $y\in Z$ - and so $v=x+y\in Z$.
		
		We have just shown that any element $v\in S$ is also in $Z$, which means that $S\subseteq Z$.
	\end{itemize}

This proves that $S$ satisfies the definition of sum and, therefore, $S=X+Y$, just as we wanted to show.
\end{proof}

\begin{ex}
	Let's use this to build new subspaces!
	
	Take the line $\mb X$ and the line $\mb Y$, once again. What is $\mb X+\mb Y$? Well, by what we just did, it's the set of all sums of points in $\mb X$ and $\mb Y$...
	
	But this is the whole $\R^2$!
	
	To see this, take any $v\in \R^2$. Then $v=(x,y)$, for some $x,y\in \R$, and we can rewrite this as $xe_1+ye_2$. But since $\mb X$ and $\mb Y$ are subspaces, $xe_1\in \mb X$ and $ye_2\in \mb Y$, so $v\in \mb X+\mb Y$.
	
	This shows that $\mb X+\mb Y\subseteq \R^2\subseteq \mb X+\mb Y $. That can only mean one thing: $\mb X+\mb Y=\R^2$.
\end{ex}

What this example shows us is another weird behavior of subspaces: They grow in huge steps at a time:

\begin{lemma}
	For any two different lines through zero $r,s\subseteq \R^2$, we have that $r+s=\R^2$.
\end{lemma}
\begin{proof}
	It's similar to the example's: Let $r=\R v$ and $s=\R u$ for any two non-parallel vectors $v,u\in \R^2$ (if they were parallel, by our previous discussions, $r=s$ and we don't want that).
	
	We'll show that $r+s$ contains both $e_1$ and $e_2$. This suffices, because we've already shown that $\mb X+\mb Y=\R^2$.
	
	To do that, write $v=(v_1,v_2)$ and $u=(u_1,u_2)$. Now, defining $\mu_1:=\dfrac{v_2}{u_2}\in\R$ we see that $$\mu_1 u=\dfrac{v_2}{u_2}(u_1,u_2)=\left(\frac{v_2}{u_2}u_1,\frac{v_2}{u_2}u_2\right)=\left(\frac{v_2}{u_2}u_1,v_2\right)$$and so
	\[v-\mu_1u=(v_1,v_2)-\left(\frac{v_2}{u_2}u_1,v_2\right)=\left(v_1-\frac{v_2}{u_2}u_1,0\right),\]that is, $v-\mu_1u\in \R e_1$. So we can just scale it up or down to obtain $e_1$. To do that, let us rewrite it a little bit:
	\[v-\mu_1u=\left(v_1-\frac{v_2}{u_2}u_1,0\right)=\left(\frac{v_1u_2-v_2u_1}{u_2},0\right)\]and see that if we multiply it by $\mu_2:=\dfrac{u_2}{v_1u_2-v_2u_1}$ we get
	\[\mu_2(v-\mu_1u)=\dfrac{u_2}{v_1u_2-v_2u_1}\left(\frac{v_1u_2-v_2u_1}{u_2},0\right)=(1,0)=e_1.\]
	
	Now we can just argue: Since $v\in r$ and $\mu_1u\in s$, we get that $v-\mu_1u\in r+s$. But since $r+s$ is a subspace, we see that $\mu_2(v-\mu_1u)\in r+s$, which shows that $e_1$ (and all its scalar multiples) is also in $r+s$. This proves that $\mb X\subseteq r+s$.
	
	\bigskip
	Let us now show that $e_2\in r+s$: It's essentially the same construction:
	
	Define $\lambda_1:=\dfrac{u_1}{v_1}\in \R$, which allows us to do $\lambda_1v=\left(u_1,\dfrac{u_1}{v_1}v_2\right)$ and so $\lambda_1v-u=\left(0,\dfrac{u_1}{v_1}v_2-u_2\right)$. 
	
	Now we rewrite that as $\lambda_1v-u=\left(0,\dfrac{u_1v_2-u_2v_1}{v_1}\right)$to see that by taking $\lambda_2:=\dfrac{v_1}{u_1v_2-u_2v_1}$ we can do $\lambda_2(\lambda_1v-u)=e_2$.
	
	Now, since $\lambda_1v\in r$ and $u\in s$, we see that $\lambda_1v-u\in r+s$, and so, since it is a subspace, $\lambda_2(\lambda_1v-u)\in r+s$. This shows that $e_2$ (and all its scalar multiples) is also in $r+s$, and so, $\mb Y\subseteq r+s$.
	
	\bigskip
	Finally, this shows that $r+s$ contains both $\mb X$ and $\mb Y$, and so, by, definition of sum, it contains $\mb X+\mb Y$. But we already know $\mb X+\mb Y=\R^2$. So we get $r+s\subseteq\R^2\subseteq r+s$, which finally shows that $r+s=\R^2$.
	
	This finishes the proof.
\end{proof}

Finally, we can prove:

\begin{theorem}
	Let $X\subseteq \R^2$ be a subspace. Then $X$ is either 0, a line through zero, or $\R^2$.
\end{theorem}
\begin{proof}
	We already know that every subspace contains 0.
	
	If $X\setminus\{0\}=\varnothing$, then $X=0$.
	
	\bigskip
	If not, then there's some $x\in X$ with $x\neq 0$. But since $X$ is a subspace, every multiple of $x$ is also in $X$. In particular, $\R x\subseteq X$. This shows that $X$ contains a line through zero.
	
	If $X\setminus\R x=\varnothing$, then $X=\R x$ and $X$ is a line through zero.
	
	\bigskip
	If not, then there's some $y\in X\setminus\R x$ with $y\neq 0$. But since $X$ is a subspace, every multiple of $y$ is also in $X$. In particular, $\R y\subseteq X$. Also, the fact that we chose $y$ outside of the line $\R x$ implies that $y\nparallel x$. This means that $\R x\neq\R y$.
	
	But this means that $X$ contains both $\R x$ and $\R y$, and so, by definition of sum, contains $\R x+\R y$. But we know that $\R x+\R y=\R^2$.
	
	This shows that $X\subseteq \R^2\subseteq X$, and thus $X=\R^2$.
	
	This ends the proof.
\end{proof}

So now we know that there aren't many choices of subspaces for $\R^2$: There's only the origin, lines through the origin and the whole set.

\newpage
\subsection{You span me right round}

This new section that we're starting right now doesn't really make sense for $\R^2$ and would be better-suited for studying general vector spaces. However, since it is usually taught in any introductory text on the subject, I guess I'll have to comply.

\begin{df}
	Let $v\in \R^2$ be any vector. We define the \textbf{subspace spanned by $v$} to be the subspace $\spen \{v\}$ (sometimes $\langle v\rangle$) defined by
	\[\spen\{v\}:=\{u\in \R^2\mid \exists\lambda\in \R \mbox{ such that } u=\lambda v\}.\]
	
	Analogously, given any finite collection of vectors $\{v_1,v_2,\cdots,v_n\}\subseteq \R^2$ we define the \textbf{subspace spanned by $v_1,v_2,\cdots,v_n$} to be the subspace $\spen\{v_1,v_2,\cdots,v_n\}$ (sometimes $\langle v_1,v_2,\cdots,v_n\rangle$) defined by
	\[\spen\{v_1,v_2,\cdots,v_n\}:=\{u\in \R^2\mid \exists\{\lambda_1,\lambda_2,\cdots,\lambda_n\}\subseteq\R \mbox{ such that }u=\lambda_1v_1+\lambda_2v_2+\cdots+\lambda_nv_n\}.\]
\end{df}

Why is this a stupid definition at this point? Well...

As we've already seen, $\R^2$ only has three kinds of subspaces: zero, lines through zero and $\R^2$ itself. Since the subspace spanned by a vector (or a collection of vectors) is a subspace (duh) it has to be one of those three.

This means that the whole study of spanning and subspaces comes down to classifying what kinds of collections of vectors span each kind of subspace.

Two of them are trivial:

\begin{prop}
	Take $v\in \R^2$. Then $\spen\{v\}=0$ if, and only if, $v=0$.
\end{prop}
\begin{proof}
	If $v=0$ then clearly, $\lambda v=0$ for all $\lambda \in \R$, so $\spen\{v\}=0$.
	
	Conversely, if $\spen\{v\}=0$, since $v\in\spen\{v\}$ (by taking $\lambda=1$) we can conclude that $v=0$.
\end{proof}

\begin{prop}
	Take $v\in \R^2$. Then $\spen\{v\}$ is a line if, and only if, $v\neq 0$.
\end{prop}
\begin{proof}
	If $\spen\{v\}$ is a line, then $v\neq 0$ by the preceding proposition.
	
	Conversely, if $v\neq 0$, then $\spen\{v\}=\R v$, by definition. So it is a line.
\end{proof}

There are still some cases left to classify, however:
\begin{itemize}
	\item Is there some collection $\{v_1,v_2,\cdots,v_n\}$ which spans 0?
	\item Is there some collection $\{v_1,v_2,\cdots,v_n\}$ which spans a line?
	\item Is there some collection $\{v_1,v_2,\cdots,v_n\}$ which spans $\R^2$?
\end{itemize}

And you might be tempted to say that any collection of two non-null vectors spans $\R^2$. But that's not the case:

\begin{ex}
	Let $v=(1,1)$, $u=(-3,-3)$. We claim that $\spen\{v,u\}=\spen\{v\}=\R v$.
	
	To see that, take any $w\in \spen\{v,u\}$ - that is, there are two real numbers $\lambda,\mu\in \R$ such that $w=\lambda v+\mu u$. In other words,
	\[w=\lambda(1,1)+\mu(-3,-3)=(\lambda,\lambda)+(-3\mu,-3\mu)=(\lambda-3\mu,\lambda-3\mu)\]and we see that $w=(\lambda-3\mu)v$, and so $w\in \spen\{v\}$. This shows $\spen\{v,u\}\subseteq\spen\{v\}$.
	
	Conversely, take $w\in \spen\{v\}$ - that is, there is some $\lambda\in \R$ such that $w=\lambda v$. But that, $w=\lambda v+0u$ shows that $w\in\spen\{v,w\}$. Therefore, $\spen\{v\}\subseteq\spen\{v,u\}$.
	
	We have shown that $\spen\{v\}=\spen\{v,u\}$, even though $v\neq u$.
\end{ex}

This kind of situation gives rise to an awful and useless definition, but very helpful when dealing with real-life applications of linear algebra:

\begin{df}
	Let $\{v_1,v_2,\cdots,v_n\}\subseteq\R^2$ be a collection of vectors in $\R^2$. We say that this collection is \textbf{linearly dependent} if one of the vectors in the collection is spanned by the other vectors.
	
	In symbols, there's some $i\leq n$ and some real numbers $\lambda_1,\lambda_2,\cdots,\lambda_{i-1},\lambda_{i+1},\cdots,\lambda_n$ such that
	\[v_i=\lambda_1v_1+\lambda_2v_2+\cdots+\lambda_{i-1}v_{i-1}+\lambda_{i+1}v_{i+1}+\cdots+\lambda_nv_n.\]
	
	\bigskip
	Analogously, we say that the collection is \textbf{linearly independent} if it's not linearly dependent.
\end{df}

See what I said about this area being useless? This mostly appears when doing linear algebra the \textbf{wrong way} - that is, by being focused on finding solutions to linear systems and doing matrix stuffs.

Why is it stupid? Well, because we already have a better language to describe the same things!

\begin{lemma}
	Take $v,u\in \R^2$ any two vectors. Then $\spen\{v,u\}=\R v+\R u$.
\end{lemma}
\begin{proof}
	There's literally nothing to be proven here. They are equal by definition.
\end{proof}
\begin{cor}
	Let $\{v_1,v_2,\cdots,v_n\}\subseteq\R^2$ be any collection of vectors. Then $\spen\{v_1,v_2,\cdots,v_n\}=\R v_1+\R v_2+\cdots+\R v_n$.
\end{cor}

\begin{lemma}
	Let $\{v_1,v_2,\cdots,v_n\}\subseteq\R^2$ be any collection of vectors. Then it is linearly dependent if, and only if, there is some $i\leq n$ such that 
	\[\R v_i\subseteq \R v_1+\R v_2+\cdots +\R v_{i-1}+\R v_{i+1}+\cdots+\R v_n.\]
\end{lemma}
\begin{proof}
	Once again, there's nothing to be done here, since they are equal by definition.
\end{proof}
\begin{cor}
	Two vectors $v,u\in \R^2$ are linearly independent if, and only if, $v\nparallel u$.
\end{cor}

But we've already proven that if we take any two non-parallel vectors then they span the whole $\R^2$. That means that if we take any three non-parallel vectors, then any one of them is already spanned by the other two. From this we can conclude two results:

\begin{prop}
	Let $\{v_1,v_2,\cdots,v_n\}\subseteq\R^2$ be any collection of vectors. If it spans $\R^2$, then $n\geq 2$.
\end{prop}
\begin{prop}
	Let $\{v_1,v_2,\cdots,v_n\}\subseteq\R^2$ be any collection of vectors. If it is linearly independent, then $n\leq 2$.
\end{prop}

Finally, there's one last thing to prove here:

\begin{theorem}
	Take $X\subseteq \R^2$ any subset. Then the following conditions are equivalent:
	\begin{enumerate}[(a)]
		\item $X$ is a base;
		\item $X$ spans $\R^2$ and has 2 elements;
		\item $X$ spans $\R^2$ and is linearly independent;
		\item $X$ has 2 elements and is linearly independent.
	\end{enumerate}
\end{theorem}
\begin{proof}
	The two preceding propositions give us, for free, that (c) implies both (b) and (d): If $X$ is linearly independent it has at most 2 elements, and if it spans it has at least 2 elements. Since it satisfies both, it must have precisely 2 elements.
	
	\begin{itemize}
		\item \underline{(b) implies (c)}:
		
		Let $X=\{x_1,x_2\}$ since it only has two elements. Assume they're not linearly independent. Then, $x_1=\lambda x_2$ for some real number $\lambda$, and so $\R x_1+\R x_2=\R x_1$, so the sum is a line.
		
		However, we're assuming that $X$ spans $\R^2$ - that is, $\R^2=\R X=\R x_1+\R x_2$. So we would have that $\R^2=\R x_1$ - in other words, that the whole plane is just a line, which is clearly absurd.
		
		Therefore, $X$ has to be linearly independent.
		
		\item \underline{(d) implies (c)}:
		
		Let $X=\{x_1,x_2\}$ since it has only two elements. Since they're linearly independent, we see that $x_1\nparallel x_2$ and so $\R x_1+\R x_2=\R^2$, $X$ spans $\R^2$.
	\end{itemize}

This shows that (b), (c) and (d) are equivalent.

Finally, let us prove that (a) is equivalent to the other three:
\begin{itemize}
	
	\item \underline{(a) implies (c)}:
	
	Let $X$ be a base, and take the linear function $\id_{\R^2}:\R^2\to \R^2$. Since $X$ is a base, we know that for all $v\in \R^2$, $\id_{\R^2}(v)=\lambda_1\id_{\R^2}(x_1)+\lambda_2\id_{\R^2}(x_2)+\cdots+\lambda_n\id_{\R^2}(x_n)$.
	
	But $\id_{\R^2}$ is the identity function, so this becomes
	\[v=\lambda_1x_1+\lambda_2x_2+\cdots+\lambda_n x_n\]which tells us that any $v\in \R^2$ is spanned by $X$ - in other words, $X$ spans $\R^2$.
	
	On the other hand, assume $x_i\parallel x_j$ for some $i\neq j$. Then, $x_j=\mu_{i,j} x_j$, by definition. But, $x_i=1x_i$, so there's two ways of writing $x_i$ as a linear combination of the base $X$. This cannot happen, by definition of base. So $x_i\nparallel x_j$, no matter which $i\neq j$ we start with.
	
	 This means that we can take any two (say, $x_1,x_2$) and the others will be spanned by those two. But if $n> 2$ that's also a problem: For instance, take $x_3$. Since it is spanned by $x_1,x_2$, there are $\lambda_1,\lambda_2\in \R$ such that $x_3=\lambda_1x_1+\lambda_2x_2$. But we already know that the only possible way to write $x_3$ in terms of the base is $x_3=1x_3$ (by the same reasoning as above). So if there were more than two elements in $X$ we'd have a contradiction.
	 
	 This allows us to finally conclude that $X=\{x_1,x_2\}$ and so it is linearly independent and spans.
	
	\item \underline{(c) implies (a)}:
	
	Let $X=\{x_1,x_2\}$ be a linearly independent spanning set for $\R^2$ (we already know it has 2 elements) and any linear function $f:\R^2\to\R^2$. Then, since $X$ spans $\R^2$ and is linearly independent, we see that any vector $v\in \R^2$ is of the form $v=\lambda_1 x_1+\lambda_2x_2$ for some $\lambda_1,\lambda_2\in \R$.
	
	 But this tells us that $f(v)=f(\lambda_1 x_1+\lambda_2x_2)$ and since $f$ is linear, this is simply $f(v)=\lambda_1 f(x_1)+\lambda_2 f(x_2)$.
	 
	 This tells us that $X$ is a base, since any linear function is completely determined by $X$.
\end{itemize}

This shows that (a) and (c) are equivalent, and since (c) is already equivalent to (b) and (d) this suffices and ends the proof.
\end{proof}

\newpage
\subsection{Back to linearity}

Now that we have the very needed tools of subspaces and spanning sets, let's move forward with our discussions on linear functions.

\begin{prop}
	For any linear function $f:\R^2\to\R^2$, $\im f$ is a subspace.
\end{prop}
\begin{proof}
	Take $v,u\in \im f$. That means that $v=f(v')$ and $u=f(u')$ for some $v',u'\in\R^2$. Then
	\[f(u'+v')=f(u')+f(v')=u+v\]and therefore $u+v$ is also in the image of $f$ (since it is the image of $u'+v'$ under $f$).
	
	Similarly, given any scalar $\lambda\in \R$ we have that
	\[f(\lambda v')=\lambda f(v')=\lambda v\]and so $\lambda v$ is also in the image of $f$ (since it is the image of $\lambda v'$ under $f$).
	
	Since $\im f$ is closed under sums and scalar multiplications, it must be a subspace.
\end{proof}
\begin{cor}
	For any linear function $f:\R^2\to\R^2$ its image is either zero, a line through zero or the whole set.
\end{cor}

\begin{ex}
	Let $f:\R^2\to\R^2$ be a function given by $f(x,y)=(2x-y,5y)$. What is, then, $\im f$?
	
	Well, it cannot be zero, since $f(0,1)=(2\cdot0-1,5\cdot 1)=(-1,5)\neq 0$.
	
	So it can only be the line $\R (-1,5)$ (since we already know $(-1,5)$ is in the image) or the whole set $\R^2$.
	
	But it's easy to see that $f(1,1)=(2\cdot 1-1,5\cdot 1)=(1,5)$ and $(1,5)\notin\R(-1,5)$ so there are two non-parallel vectors in the image. It follows that the image of $f$ has the be the whole set $\R^2$.
	
	\bigskip
	Consider now the linear function $g:\R^2\to\R^2$ given by $g(x,y)=(x-y,x-y)$. What is the image of $g$?
	
	Once again, it can't be zero: $g(1,0)=(1-0,1-0)=(1,1)\neq 0$.
	
	Therefore $\im g$ is either the line $\R (1,1)$ or $\R^2$.
	
	Now suppose $\im g =\R^2$. That means that, for instance, $(1,0)\in \im g$. But this would mean that there's some $(a,b)\in \R^2$ such that $g(a,b)=(1,0)$, by definition of $g$.
	
	But doing computations it's easy to convince ourselves that is impossible: If it were possible, we'd have $g(a,b)=(a-b,a-b)=(1,0)$ and so $a-b=1$ and $a-b=0$, which is a contradiction. A number can't equal both $1$ and $0$ at the same time!
	
	Therefore, $(1,0)\notin \R^2$. This already tells us that $\im g$ is a line (because it's either a line or the whole plane).
	
	Taking one step further, it's not hard to convince yourself that a point $(x,y)$ lies in the image of $g$ if, and only if, $x=y$.
	
	That is, the image is \textbf{not} the whole set $\R^2$, but only the line $\R(1,1)$.
\end{ex}

We'll now make a somewhat arbitrary definition, but one that's going to make a \textbf{lot} of sense the more of you think about it.

\begin{df}
	Let $f:\R^2\to\R^2$ be any linear function. We define its \textbf{kernel} to be the set $\Ker f$ given by
	\[\Ker f:=\{v\in \R^2\mid f(v)=0\}.\]
\end{df}

In other words, the kernel of a linear function is the set of points that is killed by it.

\begin{ex}
	Expanding on the example above, let's compute $\Ker f$ and $\Ker g$:
	
	Take $(a,b)\in \R^2$ and assume $f(a,b)=0$. What can we say about $a$ and $b$? Well, by definition of $f$, $f(a,b)=(2a-b,5b)$ so if that equals 0 we must have $2a-b=0$ and $5b=0$.
	
	The second equation alone tells us that $b=0$. Now armed with that, the first equation becomes $2a=0$ and so $a=0$ too.
	
	This means that for a point $(a,b)$ to go to 0 under $f$, we must have $a=b=0$. That means that $\Ker f=0$.
	
	\bigskip
	Analogously, take $(a,b)\in \R^2$ and assume that $g(a,b)=0$. But this is just $(a-b,a-b)=0$, which is a single equation: $a=b$.
	
	Therefore, the only way for a point $(a,b)\in\R^2$ to be taken to 0 by $g$ is for $a$ and $b$ to be equal - for instance, $(1,1)$ or $(-3,-3)$. This means that $\Ker g=\R(1,1)$.
\end{ex}

\begin{lemma}
	For any linear function $f:\R^2\to\R^2$, we have that $\Ker f$ is a subspace.
\end{lemma}
\begin{proof}
	Take $v,u\in \Ker f$. This means that $f(v)=f(u)=0$, by definition of kernel. But then, since $f$ is linear, we have:
	\[f(v+u)=f(v)+f(u)=0+0=0\]and so $v+u\in\Ker f$ since its image under $f$ is 0.
	
	Similarly, given any $\lambda\in \R$ we can easily see that
	\[f(\lambda v)=\lambda f(v)=\lambda\cdot 0=0\]and so $\lambda v\in\Ker f$ since its image under $f$ is 0.
	
	This shows that $\Ker f$ is closed under addition and scalar multiplication, and is, therefore, a subspace.
\end{proof}
\begin{cor}
	For any linear function $f:\R^2\to\R^2$, we have that $\Ker f$ is either zero, a line through zero or the whole plane $\R^2$.
\end{cor}

Now we're gonna present one of the main reasons why kernels are important: They tell us when functions are injective:

\begin{prop}
	Let $f:\R^2\to\R^2$ be a linear function. Then $f$ is injective if, and only if, $\Ker f=0$.
\end{prop}
\begin{proof}
	Assume $f$ is injective and take $v\in \Ker f$. This means that $f(v)=0$. But linear functions always take 0 to 0 - that is, $f(0)=0$.
	
	Since $f$ is injective, we can't have two different points in the domain with the same image under $f$. This means that $v=0$. With this, we can prove that any point in the kernel is just 0, so $\Ker f=0$.
	
	\bigskip
	Conversely, assume $\Ker f=0$ and take two points $v,u\in \R^2$ such that $f(v)=f(u)$. We want to show that $v=u$.
	
	But $f$ is linear, so $f(v)=f(u)$ implies $f(v)-f(u)=0$ - that is, $f(v-u)=0$. This shows that $v-u\in\Ker f$.
	
	But we're assuming that $\Ker f=0$. This means that $v-u=0$, and therefore $v=u$ and $f$ is injective.
\end{proof}

So from now on, instead of checking directly whether or not a function is injective, we can just use kernels.

\begin{ex}
	One final word about the preceding examples: We've already shown that $\Ker f=0$ and $\Ker g=\R(1,1)$. This shows, then, that $f$ is injective and $g$ is not injective.
\end{ex}

Finally, we're ready to work with isomorphisms.

\begin{df}
	Let $f:\R^2\to\R^2$ be a function. We'll say that \textbf{$f$ is a linear isomorphism} (or just an isomorphism) if it is a set isomorphism (that is, a bijection) which is also linear.
\end{df}
\begin{lemma}
	Let $f:\R^2\to\R^2$ be a linear isomorphism. Then $f^{-1}:\R^2\to\R^2$ is also a linear isomorphism.
\end{lemma}
\begin{proof}
	We already know that if $f$ is an isomorphism, then $f^{-1}$ is also an isomorphism, since they're mutually inverse to each other.
	
	It suffices to prove, then, that $f^{-1}$ is linear when $f$ is linear.
	
	Take $v,u\in \R^2$. Since $f$ is an isomorphism, it is also a surjection. This means that $v=f(v')$ and $u=f(u')$ for some $v',u'\in\R^2$. Then:
	\[f^{-1}(v+u)=f^{-1}(f(v')+f(u'))=f^{-1}(f(v'+u'))=(f^{-1}\circ f)(v'+u')=v'+u'=f^{-1}(v)+f^{-1}(u)\]so $f^{-1}$ preserves sums.
	
	Analogously, for any scalar $\lambda \in\R^2$ we have
	\[f^{-1}(\lambda v)=f^{-1}(\lambda f(v'))=f^{-1}(f(\lambda v'))=(f^{-1}\circ f)(\lambda v')=\lambda v'=\lambda f^{-1}(v)\]and so $f^{-1}$ preserves scalar multiplication.
	
	It follows that $f^{-1}$ is indeed linear, just as we stated, which finishes the proof.
\end{proof}

\begin{prop}
	Let $f,g:\R^2\to\R^2$ be linear functions. Then both of $f\circ g$ and $g\circ f$ are also linear functions.
\end{prop}
\begin{proof}
	We'll only show one of them, the other is identical.
	
	Take $v,u\in\R^2$. Then:
	
	\[(f\circ g)(v+u)=f(g(v+u))=f(g(v)+g(u))=f(g(v))+f(g(u))=(f\circ g)(v)+(f\circ g)(u)\]so $f\circ g$ preserves sums.
	
	Analogously, taking $\lambda\in \R$:
	\[(f\circ g)(\lambda v)=f(g(\lambda v))=f(\lambda g(v))=\lambda f(g(v))=\lambda(f\circ g)(v)\]so $f\circ g$ preserves scalar multiplication.
	
	It follows then that $f\circ g$ is linear, which ends the proof.
\end{proof}
\begin{cor}
	Let $f,g:\R^2\to\R^2$ be two linear isomorphisms. Then both $f\circ g$ and $g\circ f$ are also linear isomorphisms.
\end{cor}

And finally, a surprising result:

\begin{theorem}
	Let $f:\R^2\to\R^2$ be a linear function. Then the following are equivalent:
	\begin{enumerate}[(a)]
		\item $f$ is a linear isomorphism;
		\item $f$ is injective;
		\item $f$ is surjective.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Clearly (a) implies both (b) and (c), by definition.
	
	\begin{itemize}
		\item \underline{(b) implies (a)}:
		
		Let $f:\R^2\to\R^2$ be a linear injection and choose $X=\{x_1,x_2\}\subseteq \R^2$  base. Since $f$ is injective, $f(x_1)\neq f(x_2)$.
		
		We claim that $f(x_1)\nparallel f(x_2)$. This would suffice, because then we'd have to non-parallel vectors in the image and, therefore, by the previous section, this is sufficient to span any other vector - that is, $f$ would be surjective (and since it's already injective, it would be a bijection).
		
		To see then that $f(x_1)\nparallel f(x_2)$, assume they \textbf{are} parallel - that is, $f(x_1)=\lambda f(x_2)$ for some $\lambda\in\R$. But $f(\lambda x_2)=\lambda f(x_2)$ as well. Since $f$ is injective, $f(x_1)=f(\lambda x_2)$ must then imply $x_1=\lambda x_2$ - that is, $x_1\parallel x_2$. But we've already proven that no two vectors in a base can be parallel.
		
		This is a contradiction! Therefore, we \textbf{cannot} have that $f(x_1)\parallel f(x_2)$, and the result follows by our previous discussion.
		
		\item \underline{(c) implies (a)}:
		
		Let $f:\R^2\to \R^2$ be a linear surjection and choose $v,u\in \R^2$ such that $v\nparallel u$. Since $f$ is surjective, there's some $v',u'\in \R^2$ such that $f(v')=v$ and $f(u')=u$. Notice that this implies $v'\nparallel u'$ (cause otherwise $v'=\lambda u'$ would imply $v=\lambda u$, which cannot happen since we're taking $v\nparallel u$).
		
		So $\{v,u\}$ is a base of $\R^2$. This means that any other vector $w\in \R^2$ can be written in a unique way as $w=\lambda_1v+\lambda_2u$.
		
		In particular, $0$ can always be written as $0=0v+0u$, so that must be its unique way.
		
		Take then $z\in \Ker f$. Then $f(z)=0$, by definition of kernel. But since $\{v',u'\}$ is also a base, we know, by definition of base, that we can write $z=\mu_1v'+\mu_2u'$ in a unique way.
		
		Now we can apply $f$ to see that $f(z)=\mu_1f(v')+\mu_2f(u')$. But $f(z)=0$, $f(v')=v$ and $f(u')=u$, which tells us that $0=\mu_1v+\mu_2u$ is another way to write $0$ in terms of the base $\{v,u\}$. But by definition of base there's a unique way to do so - which, as we've seen above, is $0=0v+0u$.
		
		Therefore, $\mu_1=\mu_2=0$. But this tells us that $z=\mu_1v'+\mu_2u1$ now becomes $z=0v'+0u'=0$ - that is, $z=0$.
		
		So we took any point $z\in \Ker f$ and showed that $z=0$. This implies that $\Ker f=0$ and so $f$ is injective.
		
		Since $f$ is already surjective, we see that $f$ is a bijection, which finishes the argument.
	\end{itemize}

Finally, since (a) is equivalent to both (b) and (c), it follows that (b) and (c) are also equivalent amongst themselves.

This ends the proof.
\end{proof}
\begin{rmk}
	If you were unhappy with the way we defined base previously, try proving this theorem we've just proven using the classical definition of a base (i.e. a linearly independent spanning set).
	
	Good luck. You're gonna need it.
\end{rmk}

\newpage
\section{Closing the distance}
\subsection{A systematic approach}

In this section our aim is to study some more geometrical properties that vectors in $\R^2$ inherit simply because $\R^2$ is, as we've seen, an Euclidean plane.

\begin{df}
	Given any two natural numbers $n,m\in\N$, we define a \textbf{$n\times m$ real matrix} to be a table with $n$ rows and $m$ columns wherein each entry is a real number.
	
	We denote the set of all $n\times m$ real matrices by $M_{n\times m}(\R)$.
\end{df}

\begin{ex}
	\[\begin{pmatrix}
	1 & 2 & -1\\
	0 & 0 & 9
	\end{pmatrix}\] is a $2\times 3$ real matrix,\[\begin{pmatrix}
	1\\
	\pi\\
	\pi
	\end{pmatrix}\]is a $3\times 1$ real matrix,\[\begin{pmatrix}
	27
	\end{pmatrix}\]is a $1\times 1$ real matrix.
\end{ex}

\begin{prop}
	The set of $2\times 1$ real matrices is in bijection with $\R^2$ - that is, $M_{2\times 1}(\R)\iso\R^2$.
\end{prop}
\begin{proof}
	Let $\psi:M_{2\times 1}(\R)\to\R^2$ be given by $\psi\begin{pmatrix}
	x\\y
	\end{pmatrix}:=(x,y)$. Then $\psi$ is clearly a bijection.
\end{proof}

\begin{df}
	For any $X,Y\in M_{2\times 1}(\R)$ and any $\lambda \in \R$ we define $X+Y$ and $\lambda X$ by putting
	\[X+Y:=\psi^{-1}(\psi_X+\psi_Y)\]
	\[\lambda X:=\psi^{-1}(\lambda\psi_X)\]where $\psi_X=\psi(X)$ and $\psi_Y=\psi(Y)$.
\end{df}

\begin{lemma}
	Let $X,Y,Z\in M_{2\times1}(\R)$ and $\lambda,\mu\in R$. Then:
		\begin{itemize}
			\item $(X+Y)+Z=X+(Y+Z)$
			\item $X+Y=Y+X$;
			\item There exists a matrix $0\in M_{2\times 1}(\R)$ such that $X+0=0+X=X$;
			\item There exists a matrix $-X\in M_{2\times 1}(\R)$ such that $X+(-X)=(-X)+X=0$
			\item $\lambda(\mu X)=(\lambda \mu)X$;
			\item $\lambda X=X\lambda$;
			\item $1\in \R$ is such that $1X=X1=X$;
			\item $\lambda(X+Y)=\lambda X+\lambda Y$;
			\item $(\lambda+\mu)X=\lambda X+\mu X$
		\end{itemize}
\end{lemma}
\begin{proof}
	\begin{itemize}
		\item \begin{align*}
			(X+Y)+Z&=\psi^{-1}(\psi_X+\psi_Y)+Z\\
			&=\psi^{-1}(\psi(\psi^{-1}(\psi_X+\psi_Y))+\psi_Z)\\
			&=\psi^{-1}((\psi_X+\psi_Y)+\psi_Z)\\
			&=\psi^{-1}(\psi_X+(\psi_Y+\psi_Z))\\
			&=\psi^{-1}(\psi_X+\psi(\psi^{-1}(\psi_Y+\psi_Z)))\\
			&=X+\psi^{-1}(\psi_Y+\psi_Z)=X+(Y+Z)
		\end{align*} so addition is associative;
		
		\item \begin{align*}
			X+Y&=\psi^{-1}(\psi_X+\psi_Y)\\
			&=\psi^{-1}(\psi_Y+\psi_X)=Y+X
		\end{align*}so addition is commutative;
		
		\item Let $0:=\psi^{-1}(0,0)$. Then $\psi(0)=(0,0)$, so\[X+0=\psi^{-1}(\psi_X+\psi(0))=\psi^{-1}(\psi_X)=X\]so $0$ is the identity element of the addition;
		
		\item Let $-X:=\psi^{-1}(-\psi_X)$. Then \begin{align*}
			\psi_X+\psi(-X)&=\psi_X+\psi(\psi^{-1}(-\psi_X))\\
			&=\psi_X-\psi_X=(0,0)=\psi(0),
		\end{align*}so \begin{align*}
			X+(-X)&=\psi^{-1}(\psi_X+\psi(-X))\\&=\psi^{-1}(\psi(0))=0
		\end{align*}and $-X$ is the additive inverse of $X$;
		
		\item \begin{align*}
			\lambda(\mu X)&=\lambda(\psi^{-1}(\mu \psi_X))\\
			&=\psi^{-1}(\lambda \psi(\psi^{-1}(\mu \psi_X)))\\
			&=\psi^{-1}(\lambda (\mu \psi_X))\\
			&=\psi^{-1}((\lambda\mu)\psi_X)=(\lambda\mu) X
		\end{align*}so scalar multiplication is associative;
		
		\item $\lambda X=\psi^{-1}(\lambda \psi_X)=\psi^{-1}(\psi_X\lambda)=X\lambda$ so scalar multiplication is commutative;
		
		\item $1X=\psi^{-1}(1\psi_X)=\psi^{-1}(\psi_X)=X$ so $1$ is the identity of the scalar multiplication;
		
		\item \begin{align*}
			\lambda(X+Y)&=\lambda \psi^{-1}(\psi_X+\psi_Y)\\
			&=\psi^{-1}(\lambda \psi(\psi^{-1}(\psi_X+\psi_Y)))\\
			&=\psi^{-1}(\lambda (\psi_X+\psi_Y))\\
			&=\psi^{-1}(\lambda \psi_X+\lambda \psi_Y)\\
			&=\psi^{-1}(\psi(\psi^{-1}(\lambda \psi_X)+\psi(\psi^{-1}(\lambda \psi_Y))))\\
			&=\psi^{-1}(\lambda \psi_X)+\psi^{-1}(\lambda \psi_Y)\\
			&=\lambda X+\lambda Y
		\end{align*}so scalar multiplication distributes over addition;
		
		\item \begin{align*}
			(\lambda+\mu)(X)&=\psi^{-1}((\lambda+\mu)\psi_X)\\
			&=\psi^{-1}(\lambda \psi_X+\mu \psi_X)\\
			&=\psi^{-1}(\psi(\psi^{-1}(\lambda \psi_X))+\psi(\psi^{-1}(\mu \psi_X)))\\
			&=\psi^{-1}(\lambda \psi_X)+\psi^{-1}(\mu \psi_X)\\
			&=\lambda X+\mu X
		\end{align*}so scalar multiplication distributes over real number addition.
	\end{itemize}

This finishes the proof.
\end{proof}

Don't get too worried, what we did is more intuitive than it might seem at first:

Since we already know how to add vectors in $\R^2$ and we know that $M_{2\times 1}(\R)\iso \R^2$, we can define $\begin{pmatrix}
a\\b
\end{pmatrix}+\begin{pmatrix}
c\\d
\end{pmatrix}$ by first taking each one of them to the corresponding vectors $(a,b)$ and $(c,d)$, then adding those up
\[(a,b)+(c,d)=(a+c,b+d)\]and then bringing the result back to matrices, and putting that as the result of the matrix addition:
\[\begin{pmatrix}
a\\b
\end{pmatrix}+\begin{pmatrix}
c\\d
\end{pmatrix}:=\begin{pmatrix}
a+c\\b+d
\end{pmatrix}.\]

Similarly, in order to define $\lambda\begin{pmatrix}
a\\b
\end{pmatrix}$ we, once again, first take $\begin{pmatrix}
a\\b
\end{pmatrix}$ to $(a,b)$, then we multiply that by $\lambda$:
\[\lambda(a,b)=(\lambda a,\lambda b)\]and then bring that back to matrices and put that as the result of the scalar multiplication:
\[\lambda\begin{pmatrix}
a\\b
\end{pmatrix}:=\begin{pmatrix}
\lambda a\\\lambda b
\end{pmatrix}.\]

For this very reason, we can think of the elements of $\R^2$ not only as pairs of real numbers, not only as points in a plane, not only as vectors, but also as $2\times 1$ matrices.

\bigskip
Now that we can think of vectors as matrices, it's no surprise to also be able to think of linear maps as matrices.

\begin{prop}
	There is a bijection $M_{2\times 2}(\R)\iso\hom_\R(\R^2,\R^2)$.
\end{prop}
\begin{proof}
	Let $E=\{e_2e_2\}$ be the canonical base of $\R^2$. Define $\phi:M_{2\times 2}(\R)\to \hom_\R(\R^2,\R^2)$ by putting, for each $A=\begin{pmatrix}
	a_1&a_3\\a_2&a_4
	\end{pmatrix}$, $\phi(A)(e_1):=(a_1,a_2)$ and $\phi(A)(e_2):=(a_3,a_4)$.
	
	We'll denote $\phi(A)$ simply by $\phi_A$. Then the above equalities become $\phi_A(e_1)=(a_1,a_2)$ and $\phi_A(e_2)=(a_3,a_4)$ -  that is, if $c_1$ and $c_2$ are, respectively, the first and second columns of $A$, then $\phi_A(e_1)=\psi(c_1)$ and $\phi_A(e_2)=\psi(c_2)$.
	
	\begin{itemize}
		\item \underline{$\phi$ is injective:}
		
		Assume that $A,B\in M_{2\times 2}(\R)$ are two matrices such that $\phi_A=\phi_B$. This means, in particular, that $\phi_A(e_1)=\phi_B(e_1)$ and $\phi_A(e_2)=\phi_B(e_2)$. But this implies that the first and second columns of $A$ and $B$ are equal. Since they only have two columns, this implies $A=B$, and so $\phi$ is injective.
		
		\item \underline{$\phi$ is surjective:}
		
		Take any linear function $f:\R^2\to \R^2$. Then by computing $f(e_1)=(a,b)$ and $f(e_2)=(c,d)$ we can then define $A^f:=\begin{pmatrix}
		a&c\\b&d
		\end{pmatrix}$. Clearly, then, $\phi_{A^f}=f$, so $f\in\im(\phi)$ and so $\phi$ is surjective.
	\end{itemize}

This shows that $\phi$ is bijective, and so we have proven the result.
\end{proof}

So we can think of $2\times 2$ matrices as linear transformations, and vice-versa.

Since we used a result like this to introduce addition and scalar multiplication to $2\times 1$ matrices, you can guess what's coming next, right?

\begin{prop}
	Let $f,g:\R^2\to\R^2$ be two linear functions and $\lambda\in \R$  be any scalar. Then the functions $f+g,f\circ g,\lambda f:\R^2\to\R^2$ defined, respectively, as $(f+g)(v):=f(v)+g(v)$, $(f\circ g)(v):=f(g(v))$ and $(\lambda f)(v):=\lambda f(v)$ are linear functions.
\end{prop}
\begin{proof}
	Choose, once and for all, $v,u\in \R^2$ and $\mu\in \R$. Then:
	\begin{itemize}
		\item \underline{$f+g$ is linear:}		
		\begin{align*}
			(f+g)(v+u)&=f(v+u)+g(v+u)\\
			&=f(v)+f(u)+g(v)+g(u)\\
			&=f(v)+g(v)+f(u)+g(u)=(f+g)(v)+(f+g)(u)
		\end{align*}and
		\begin{align*}
			(f+g)(\mu v)&=f(\mu v)+g(\mu v)\\
			&=\mu f(v)+\mu g(v)\\
			&=\mu(f(v)+g(v))=\mu(f+g)(v)
		\end{align*}so $f+g$ is linear.
		
		\item \underline{$f\circ g$ is linear:}		
		\begin{align*}
			(f\circ g)(v+u)&=f(g(v+u))\\
			&=f(g(v)+g(u))\\
			&=f(g(v))+f(g(u))=(f\circ g)(v)+(f\circ g)(u)
		\end{align*}and
		\begin{align*}
			(f\circ g)(\mu v)&=f(g(\mu v))\\
			&=f(\mu g(v))\\
			&=\mu f(g(v))=\mu(f\circ g)(v)
		\end{align*}so $f\circ g$ is linear.
		
		\item \underline{$\lambda f$ is linear:}
		\begin{align*}
			(\lambda f)(v+u)&=f\lambda(v+u)\\
			&=\lambda (f(v)+f(u))\\
			&=\lambda f(v)+\lambda f(u)=(\lambda f)(v)+(\lambda f)(u)
		\end{align*}and
		\begin{align*}
			(\lambda f)(\mu v)&=\lambda f(\mu v)\\
			&=\lambda \mu f(v)\\
			&=\mu\lambda f(v)=\mu(\lambda f)(v)
		\end{align*}so $\lambda f$ is linear.
	\end{itemize}

This ends the proof.
\end{proof}

\begin{df}
	The bijection $\phi:\hom_\R(\R^2,\R^2)\to M_{2\times 2}(\R)$ induces, for all $X,Y\in M_{2\times 2}(\R)$ and $\lambda\in \R$, $X+Y$, $XY$ and $\lambda X$ by putting $$X+Y:=\phi(\phi_X+\phi_Y)$$ $$XY:=\phi(\phi_X\circ \phi_Y)$$ $$\lambda X:=\phi(\lambda \phi_X),$$ where $\phi_X=\phi(X)$ and $\phi_Y=\phi(Y)$.
\end{df}

Now, it would be great to be able to calculate each of those matrices. Well, it turns out to be easier than it seems:

\begin{prop}
	Let $X,Y\in M_{2\times 2}(\R)$ and $\lambda\in \R$. If we write $X=\begin{pmatrix}
	x_1&x_3\\x_2&x_4
	\end{pmatrix}$ and $Y=\begin{pmatrix}
	y_1&y_3\\y_2&y_4
	\end{pmatrix}$ then the following equalities hold:
	\[X+Y=\begin{pmatrix}
	x_1+y_1&x_3+y_3\\
	x_2+y_2&x_4+y_4
	\end{pmatrix}\]\[XY=\begin{pmatrix}
	x_1y_1+x_3y_2&x_1y_3+x_3y_4\\
	x_2y_1+x_4y_2&x_2y_3+x_4y_4
	\end{pmatrix}\]\[\lambda X=\begin{pmatrix}
	\lambda x_1&\lambda x_3\\
	\lambda x_2&\lambda x_4
	\end{pmatrix}.\]
\end{prop}
\begin{proof}
	Let $E=\{e_1,e_2\}$ be the canonical base for $\R^2$. Then $\phi_X(e_1)=(x_1,x_2)$, $\phi_X(e_2)=(x_3,x_4)$, $\phi_y(e_1)=(y_1,y_2)$ and $\phi_Y(e_2)=(y_3,y_4)$.
	
	It follows then that 
	\[(\phi_X+\phi_Y)(e_1)=\phi_X(e_1)+\phi_Y(e_1)=(x_1,x_2)+(y_1,y_2)=(x_1+y_1,x_2+y_2)\]
	\[(\phi_X+\phi_Y)(e_2)=\phi_X(e_2)+\phi_Y(e_2)=(x_3,x_4)+(y_3,y_4)=(x_3+y_3,x_4+y_4)\]
	so \[X+Y=\begin{pmatrix}
	x_1+y_1&x_3+y_3\\
	x_2+y_2&x_4+y_4
	\end{pmatrix}.\]
	
	Similarly, 
	\begin{align*}
		\phi_X(\phi_Y(e_1))&=\phi_X(y_1,y_2)\\
		&=\phi_X(y_1e_1+y_2e_2)\\
		&=y_1\phi_X(e_1)+y_2 \phi_X(e_2)\\
		&=y_1(x_1,x_2)+y_2(x_3,x_4)\\
		&=(x_1y_1,x_2y_1)+(x_3y_2,x_4y_2)=(x_1y_1+x_3y_2,x_2y_1+x_4y_2)		
	\end{align*}
	\begin{align*}
	\phi_X(\phi_Y(e_2))&=\phi_X(y_3,y_4)\\
	&=\phi_X(y_3e_1+y_4e_2)\\
	&=y_3\phi_X(e_1)+y_4 \phi_X(e_2)\\
	&=y_3(x_1,x_2)+y_4(x_3,x_4)\\
	&=(x_1y_3,x_2y_3)+(x_3y_4,x_4y_4)=(x_1y_3+x_3y_4,x_2y_3+x_4y_4)		
	\end{align*}so\[XY=\begin{pmatrix}
	x_1y_1+x_3y_2&x_1y_3+x_3y_4\\
	x_2y_1+x_4y_2&x_2y_3+x_4y_4
	\end{pmatrix}.\]
	
	Finally,
	\begin{align*}
		(\lambda \phi_X)(e_1)&=\lambda \phi_X(e_1)=\lambda (x_1,x_2)=(\lambda x_1,\lambda x_2)
	\end{align*}
	\begin{align*}
	(\lambda \phi_X)(e_2)&=\lambda \phi_X(e_2)=\lambda (x_3,x_4)=(\lambda x_3,\lambda x_4)
	\end{align*}so \[\lambda X=\begin{pmatrix}
	\lambda x_1&\lambda x_3\\
	\lambda x_2&\lambda x_4
	\end{pmatrix}.\]
	
	This ends the proof.
\end{proof}

This is the best explanation why matrix multiplication is so weird. Simply put, it's because it's the worst possible way to write function composition. But, all in all, it's just that - function composition.

Finally, we can prove one last result.

\begin{df}
	We define the \textbf{evaluation map} $ev:\hom_\R(\R^2,\R^2)\times \R^2\to \R^2$ to be the map given by $ev(f,v):=f(v)$.
\end{df}

\begin{df}
	The evaluation map induces another evaluation map $M_{2\times 2}(\R)\times M_{2\times 1}(\R)\to M_{2\times 1}(\R)$ given by 
	\[AX:=\psi^{-1}(\phi_A(\psi_X)).\]
\end{df}

\begin{lemma}
	For any $A\in M_{2\times 2}(\R)$ and any $X\in M_{2\times 1}(\R)$ we have that \[AX=\begin{pmatrix}
	a_1x_1+a_3x_2\\
	a_2x_1+a_4x_2
	\end{pmatrix}\]where $A=\begin{pmatrix}
	a_1&a_3\\
	a_2&a_4
	\end{pmatrix}$ and $X=\begin{pmatrix}
	x_1\\x_2
	\end{pmatrix}$.
\end{lemma}
\begin{proof}
	Let, once more, $E=\{e_1,e_2\}$ be the canonical base of $\R^2$. Then $\phi_A(e_1)=(a_1,a_2)$ and $\phi_A(e_2)=(a_3,a_4)$.
	
	Therefore,
	\begin{align*}
		\phi_A(\psi_X)&=\phi_A(x_1,x_2)\\
		&=\phi_A(x_1e_1+x_2e_2)\\
		&=x_1\phi_A(e_1)+x_2\phi_A(e_2)\\
		&=x_1(a_1,a_2)+x_2(a_3,a_4)\\
		&=(a_1x_1,a_2x_1)+(a_3x_2,a_4x_2)=(a_1x_1+a_3x_2,a_2x_1+a_4x_2)
	\end{align*}and we see that \[AX=\begin{pmatrix}
	a_1x_1+a_3x_2\\
	a_2x_1+a_4x_2
	\end{pmatrix}\]which proves the result.
\end{proof}

\newpage
\subsection{Rotato potato}

\begin{prop}
	There is a bijection between the set of $1\times 2$ real matrices and $\R^2$ - that is, $M_{1\times 2}(\R)\iso \R^2$.
\end{prop}
\begin{proof}
	Consider the function $\varphi:M_{1\times 2}(\R)\to\R^2$ given by $\varphi\begin{pmatrix}
	x & y
	\end{pmatrix}=(x,y)$.
	
	This is clearly a bijection.
\end{proof}

\begin{df}
	For any $X,Y\in M_{1\times 2}(\R)$ and any $\lambda \in \R$ we define $X+Y$ and $\lambda X$ by putting
	\[X+Y:=\varphi^{-1}(\varphi_X+\varphi_Y)\]
	\[\lambda X:=\varphi^{-1}(\lambda\varphi_X)\]where $\varphi_X=\varphi(X)$ and $\varphi_Y=\varphi(Y)$.
\end{df}

\begin{lemma}
	Let $X,Y,Z\in M_{1\times2}(\R)$ and $\lambda,\mu\in \R$. Then:
	\begin{itemize}
		\item $(X+Y)+Z=X+(Y+Z)$
		\item $X+Y=Y+X$;
		\item There exists a matrix $0\in M_{1\times 2}(\R)$ such that $X+0=0+X=X$;
		\item There exists a matrix $-X\in M_{1\times 2}(\R)$ such that $X+(-X)=(-X)+X=0$
		\item $\lambda(\mu X)=(\lambda \mu)X$;
		\item $\lambda X=X\lambda$;
		\item $1\in \R$ is such that $1X=X1=X$;
		\item $\lambda(X+Y)=\lambda X+\lambda Y$;
		\item $(\lambda+\mu)X=\lambda X+\mu X$
	\end{itemize}
\end{lemma}

We'll refrain from doing this proof since it is essentially the same proof as in the case for $2\times 1$ matrices.

\bigskip
Since we already see matrices as linear transformations we would like to teach matrices how to act on $1\times 2$ matrices then.

To do that, however, we need to create matrices in a new way:

\begin{prop}
	The function $\tau: M_{2\times 2}(\R)\to\hom_\R(\R^2,\R^2)$ given by $\tau(A)(e_1):=(a_1,a_2)$ and $\tau(A)(e_2):=(a_3,a_4)$, where $A=\begin{pmatrix}
	a_1&a_2\\a_3&a_4
	\end{pmatrix}$, is a bijection.
\end{prop}
\begin{proof}
	Let us put $\tau(A)=\tau_A$ just to simplify our notation. Then, if $r_1$ and $r_2$ are, respectively, the first and second rows of $A$, then $\tau_A(e_1)=\varphi(r_1)$ and $\tau_A(e_2)=\varphi(r_2)$.
	
	
	
	\begin{itemize}
		\item \underline{$\tau$ is injective:}
		
		Assume that $A,B\in M_{2\times 2}(\R)$ are two matrices such that $\tau_A=\tau_B$. This means, in particular, that $\tau_A(e_1)=\tau_B(e_1)$ and $\tau_A(e_2)=\tau_B(e_2)$. But this implies that the first and second rows of $A$ and $B$ are equal. Since they only have two rows, this implies $A=B$, and so $\tau$ is injective.
		
		\item \underline{$\tau$ is surjective:}
		
		Take any linear function $f:\R^2\to \R^2$. Then by computing $f(e_1)=(a,b)$ and $f(e_2)=(c,d)$ we can then define $A^f:=\begin{pmatrix}
		a&b\\c&d
		\end{pmatrix}$. Clearly, then, $\tau_{A^f}=f$, so $f\in\im(\tau)$ and so $\tau$ is surjective.
	\end{itemize}
	
	This shows that $\tau$ is bijective, and so we have proven the result.
\end{proof}

With this, we can finally define how to act on $1\times 2$ matrices:

\begin{df}
	The evaluation map $ev:\hom_\R(\R^2,\R^2)\times\R^2\to \R^2$ induces a unique evaluation map $M_{1\times 2}(\R)\times M_{2\times 2}(\R)\to M_{1\times 2}(\R)$ given by
	\[YA:=\varphi^{-1}(\tau_A(\varphi_Y)).\]
\end{df}

\begin{lemma}
	For any $A\in M_{2\times 2}(\R)$ and any $Y\in M_{1\times 2}(\R)$ we have that \[YA=\begin{pmatrix}
	a_1y_1+a_3y_2&
	a_2y_1+a_4y_2
	\end{pmatrix}\]where $A=\begin{pmatrix}
	a_1&a_2\\
	a_3&a_4
	\end{pmatrix}$ and $Y=\begin{pmatrix}
	y_1&y_2
	\end{pmatrix}$.
\end{lemma}
\begin{proof}
	Let, once more, $E=\{e_1,e_2\}$ be the canonical base of $\R^2$. Then $\tau_A(e_1)=(a_1,a_2)$ and $\tau_A(e_2)=(a_3,a_4)$.
	
	Therefore,
	\begin{align*}
	\tau_A(\varphi_X)&=\tau_A(y_1,y_2)\\
	&=\tau_A(y_1e_1+y_2e_2)\\
	&=y_1\tau_A(e_1)+y_2\tau_A(e_2)\\
	&=y_1(a_1,a_2)+y_2(a_3,a_4)\\
	&=(a_1y_1,a_2y_1)+(a_3y_2,a_4y_2)=(a_1y_1+a_3y_2,a_2y_1+a_4y_2)
	\end{align*}and we see that \[YA=\begin{pmatrix}
	a_1y_1+a_3y_2&
	a_2y_1+a_4y_2
	\end{pmatrix}\]which proves the result.
\end{proof}

\begin{df}
	We define the \textbf{transpose map} $(-)^t:M_{2\times 1}(\R)\to M_{1\times 2}(\R)$ to be the map given by $X^t:=\varphi^{-1}(\psi_X)$.
	
	In other words, if we write $X=\begin{pmatrix}
	x_1\\x_2
	\end{pmatrix}$ then its transpose is $X^t=\begin{pmatrix}
	x_1&x_2
	\end{pmatrix}$.
\end{df}
\begin{prop}
	The transpose map is a bijection between $M_{2\times 1}(\R)$ and $M_{1\times 2}(\R)$
\end{prop}
\begin{proof}
	This is immediate from the definition of the transpose map: Since $(-)^t=\varphi^{-1}\circ \psi$ and both $\varphi^{-1}$ and $\psi$ are bijections, the result follows.
\end{proof}
\begin{df}
	By an abuse of notation, we'll denote $((-)^t)^{-1}=(-)^t$. That is, the symbol $(-)^t$ means both the process of turning a $2\times 1$ matrix into a $1\times 2$ matrix as well as its inverse process of turning a $1\times 2$ matrix into a $2\times 1$ matrix.
\end{df}

Now, let's do the same for $2\times 2$ matrices:

\begin{df}
	We define the \textbf{transpose map} (once again, by abuse of notation) $(-)^t:M_{2\times 2}(\R)\to M_{2\times 2}(\R)$ to be the map given by $A^t=\tau^{-1}(\phi_A)$.
	
	In other words, if we write $A=\begin{pmatrix}
	a&c\\b&d
	\end{pmatrix}$ then its transpose is $A^t=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}$.
\end{df}
\begin{prop}
	The transpose map is a bijection between $M_{2\times2}(\R)$ and itself.
\end{prop}
\begin{proof}
	This is immediate from the definition of the transpose map: Since $(-)^t=\tau^{-1}\circ \phi$ and both $\tau^{-1}$ and $\phi$ are bijections, the result follows.
\end{proof}
\begin{cor}
	The inverse of the transpose map is itself.
\end{cor}
\begin{proof}
	We need to prove that $\tau^{-1}\circ \phi\circ\tau^{-1}\circ\phi=\id_{M_{2\times 2}(\R)}$.
	
	Take any $A=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}$. Then $\phi_A(e_1)=(a,c)$ and $\phi_A(e_2)=(b,d)$. Therefore, $$A^t=\tau^{-1}(\phi_A)=\begin{pmatrix}
	a&c\\b&d
	\end{pmatrix}.$$
	
	But now $\phi_{A^t}(e_1)=(a,b)$ and $\phi_{A^t}(e_2)=(c,d)$. Therefore, $$(A^t)^t=\tau^{-1}(\phi_{A^t})=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}=A.$$
	
	Combining all of this together, we see that $$(\tau^{-1}\circ \phi\circ\tau^{-1}\circ\phi)(A)=(A^t)^t=A=\id_{M_{2\times 2}(\R)}(A)$$ and so the result follows.
\end{proof}
\begin{cor}
	By rephrasing the previous corollary we see that $\tau^{-1}\circ\phi=\phi^{-1}\circ\tau$.
\end{cor}

Now, we can take all of these together and get a very important result:

\begin{theorem}
	Let $X\in M_{2\times 1}(\R)$, $A\in M_{2\times 2}(\R)$ and $Y\in M_{1\times 2}(\R)$. Then $(YA)^t=A^tY^t$ and $(AX)^t=X^tA^t$.
\end{theorem}
\begin{proof}
	Let us remember our definitions:
	\begin{multicols}{3}
		\begin{itemize}
			\item \(X^t=\varphi^{-1}(\psi_X)\)
			\item \(Y^t=\psi^{-1}(\varphi_Y)\)
			\item \(AX=\psi^{-1}(\phi_A(\psi_X))\)
			\item \(YA=\varphi^{-1}(\tau_A(\varphi_Y))\)
			\item \(A^t=\tau^{-1}(\phi_A)\)
			\item \(A^t=\phi^{-1}(\tau_A)\)
		\end{itemize}
	\end{multicols}

Therefore, 
\begin{align*}
	(AX)^t&=(\psi^{-1}(\phi_A(\psi_X)))^t\\
	&=\varphi^{-1}(\psi(\psi^{-1}(\phi_A(\psi_X))))\\
	&=\varphi^{-1}(\phi_A(\psi_X))\\
	&=\varphi^{-1}(\tau(\tau^{-1}(\phi_A))(\varphi(\varphi^{-1}(\psi_X))))\\
	&=\varphi^{-1}(\tau_{A^t}(\varphi_{X^t}))=X^tA^t
\end{align*}and
\begin{align*}
(YA)^t&=(\varphi^{-1}(\tau_A(\varphi_Y)))^t\\
&=\psi^{-1}(\varphi(\varphi^{-1}(\tau_A(\varphi_Y))))\\
&=\psi^{-1}(\tau_A(\varphi_Y))\\
&=\psi^{-1}(\phi(\phi^{-1}(\tau_A))(\psi(\psi^{-1}(\varphi_Y))))\\
&=\psi^{-1}(\phi_{A^t}(\psi_{Y^t}))=A^tY^t
\end{align*}so the result follows.
\end{proof}

Finally, to end this section, we'll introduce a new perspective which is better seen from the perspective of matrices:

\begin{df}
	Let $Y\in M_{1\times 2}(\R)$ and $X\in M_{2\times 1}(\R)$. Inspired by the evaluation maps, we define the \textbf{multiplication of $Y$ and $X$} to be the $1\times 1$ matrix $YX$ given by \[\begin{pmatrix}
	y_1&y_2
	\end{pmatrix}\begin{pmatrix}
	x_1\\x_2
	\end{pmatrix}:\begin{pmatrix}
	x_1y_1+x_2y_2
	\end{pmatrix}.\]
\end{df}

\begin{prop}
	There is a bijection $M_{1\times 1}(\R)\iso \R$.
\end{prop}
\begin{proof}
	Let $\sigma:M_{1\times 1}(\R)\to \R$ be given by $\sigma\begin{pmatrix}
	a
	\end{pmatrix}:=a$. Then it clearly is a bijection, which ends the proof.
\end{proof}

\begin{df}
	The bijection $\sigma: M_{1\times 1}(\R)\to \R$ induces operations $X+Y$ and $\lambda X$ for all $X,Y\in M_{1\times 1}(\R)$ and $\lambda \in \R$ given, respectively, by
	
	\[X+Y:=\sigma^{-1}(\sigma_X+\sigma_Y)\] 
	\[\lambda X:=\sigma^{-1}(\lambda \sigma_X)\]where $\sigma_X:=\sigma(X)$ and $\sigma_Y:=\sigma(Y)$.
\end{df}

\begin{prop}
	If $X=\begin{pmatrix}
	x
	\end{pmatrix}$, $Y=\begin{pmatrix}
	y
	\end{pmatrix}$ and $\lambda\in \R$, then $X+Y=\begin{pmatrix}
	x+y
	\end{pmatrix}$ and $\lambda X=\begin{pmatrix}
	\lambda x
	\end{pmatrix}$.
\end{prop}
\begin{proof}
	This follows trivially by definition of $\sigma$.
\end{proof}

To end this section, then, we'll provide a list of good properties of all matrices additions and multiplications we've done so far.

\begin{lemma}
	Let $Y,Y'\in M_{1\times 2}(\R)$, $X,X'\in M_{2\times 1}(\R)$, $A,A'\in M_{2\times 2}(\R)$ and $\lambda \in \R$. Then the following hold:
	\begin{multicols}{2}
		\begin{enumerate}[a)]
			\item $Y(AX)=(YA)X$;
			\item $(Y+Y')X=YX+Y'X$;
			\item $Y(X+X')=YX+YX'$;
			\item $(Y+Y')A=YA+Y'A$;
			\item $A(X+X')=AX+AX'$;
			\item $Y(A+A')=YA+YA'$;
			\item $(A+A')X=AX+A'X$;
			\item $Y(AA')=(YA)A'$;
			\item $(AA')X=A(A'X)$;
			\item $(\lambda Y)X=\lambda (YX)=Y(\lambda X)$;
			\item $(\lambda Y)A=\lambda (YA)=Y(\lambda A)$;
			\item $(\lambda A)X=\lambda(AX)=A(\lambda X)$.
		\end{enumerate}
	\end{multicols}
\end{lemma}
\begin{proof}
	Using
	\[Y=\begin{pmatrix}
	y_1&y_2
	\end{pmatrix},\quad Y'=\begin{pmatrix}
	y_1'&y_2'
	\end{pmatrix}\]
	\[X=\begin{pmatrix}
	x_1\\x_2
	\end{pmatrix},\quad X'=\begin{pmatrix}
	x_1'\\x_2'
	\end{pmatrix}\]
	\[A=\begin{pmatrix}
	a_1&a_2\\a_3&a_4
	\end{pmatrix},\quad A'=\begin{pmatrix}
	a'_1&a'_2\\a'_3&a'_4
	\end{pmatrix}.\]
	
	all the proofs follow by simple computation and will be left as an exercise to the reader.
\end{proof}

\newpage
\subsection{The distance between us}

\begin{df}
	Let $v,u\in \R^2$. We define the \textbf{inner product} of $v$ and $u$ to be the real number $\gen{v,u}$ given by
	\[\gen{v,u}:=\sigma(\varphi^{-1}(v)\psi^{-1}(u))\].
\end{df}

\begin{prop}
	For any two vectors $v=(v_1,v_2)$ and $u=(u_1,u_2)$ in $\R^2$, we have that $\gen{v,u}=v_1u_1+v_2u_2$.
\end{prop}
\begin{proof}
	If $v=(v_1,v_2)$ and $u=(u_1,u_2)$, then $\varphi^{-1}(v)=\begin{pmatrix}
	v_1&v_2
	\end{pmatrix}$ and $\psi^{-1}(u)=\begin{pmatrix}
	u_1\\u_2
	\end{pmatrix}$.
	
	From this, we can see that $\varphi^{-1}(v)\psi^{-1}(u)=\begin{pmatrix}
	v_1u_1+v_2u_2
	\end{pmatrix}$ and, therefore,
	\[\gen{v,u}=\sigma(\varphi^{-1}(v)\psi^{-1}(u))=\sigma\begin{pmatrix}
	v_1u_1+v_2u_2
	\end{pmatrix}=v_1u_2+v_2u_2\]which ends the proof.
\end{proof}

In other words, the inner product of two vectors $v,u\in \R^2$ is just the multiplication of $v$, written as a $1\times 2$ matrix, and $u$, written as a $2\times 1$ matrix.

\bigskip
The reason why we didn't simply define it using vectors, but instead went all the way around, passing through matrices, to define the inner product is twofold.

First, because it makes way more sense to multiply matrices than it does to multiply vectors.

Second, because further ahead we're gonna need to define what's called the \textbf{algebraic dual} of a vector space, and then we'll see that inner products arise naturally from algebraic dualization. However, in the case of $\R^2$, we'll see that the algebraic dual of $M_{2\times 1}(\R)$ is simply $M_{1\times 2}(\R)$ - so this mindset is a particular case of a more general approach we're gonna use later on and, as such, provides great insight on how to approach the general case.

\begin{ex}
	Let $v=(1,2)$, $u=(-1,-5)$ and $w=(\pi,0)$. Then we can compute:
	\begin{multicols}{2}
		\begin{itemize}
			\item[] \(\gen{v,u}=1\cdot(-1)+2\cdot (-5)=-1-10=-11\)
			\item[] \(\gen{v,w}=1\cdot\pi+2\cdot 0=\pi+0=\pi\)
			\item[] \(\gen{u,w}=(-1)\cdot\pi+(-5)\cdot 0=-\pi+0=-\pi\)
			\item[] \(\gen{u,v}=(-1)\cdot 1+(-5)\cdot 2=-1-10=-11\)
			\item[] \(\gen{v,w}=\pi\cdot 1+0\cdot2=\pi+0=\pi\)
			\item[] \(\gen{u,w}=\pi\cdot(-1)+0\cdot (-5)=-\pi+0=-\pi\)
		\end{itemize}
	\end{multicols}

This gives us intuition about our next result:
\end{ex}

\begin{prop}
	The inner product is commutative - that is, for any two vectors $v,u\in \R^2$ we have $\gen{v,u}=\gen{u,v}$.
\end{prop}
\begin{proof}
	Let $v=(v_1,v_2)$ and $u=(u_1,u_2)$. Then:
	\[\gen{v,u}=v_1u_1+v_2u_2=u_1v_1+u_2v_2=\gen{u,v}\]so the inner product is commutative and the result follows.
\end{proof}

\begin{ex}
	Let $v=(2,7)$, $u=(2,-3)$ and $w=(-6,9)$. Then we can compute:
	\[\gen{v,u}=2\cdot 2+7\cdot(-3)=4-21=-17\]
	\[\gen{v,w}=2\cdot(-6)+7\cdot 9=-12+63=51\]
	\[\gen{u,w}=2\cdot (-6)+(-3)\cdot 9=-12-27=-39\]and once again we have an intuition about a new result:
\end{ex}
\begin{prop}
	The inner product preserves addition and scalar multiplication - that is, for any three vectors $v,u,w\in \R^2$ and $\lambda\in \R$ we have $\gen{v,u+w}=\gen{v,u}+\gen{v,w}$ and $\gen{v,\lambda u}=\lambda\gen{v,u}$.
\end{prop}
\begin{proof}
	Let, once again, $v=(v_1,v_2)$, $u=(u_1,u_2)$ and $w=(w_1,w_2)$. Then $u+w=(u_1+w_1,u_2+w_2)$ and $\lambda u=(\lambda u_1,\lambda u_2)$, and so:
	\begin{align*}
		\gen{v,u+w}&=v_1(u_1+w_1)+v_2(u_2+w_2)\\
		&=v_1u_1+v_1w_1+v_2u_2+v_2w_2\\
		&=(v_1u_1+v_2u_2)+(v_1w_1+v_2w_2)=\gen{v,u}+\gen{v,w}
	\end{align*}and
	\[\gen{v,\lambda u}=v_1(\lambda u_1)+v_2(\lambda u_2)=\lambda(v_1u_1)+\lambda (v_2u_2)=\lambda(v_1u_1+v_2u_2)=\lambda\gen{v,u}\]so the inner product preserves addition scalar multiplication, which proves the result.
\end{proof}

Now let's quickly go back to thinking about vectors geometrically. If we take any vector $v=(v_1,v_2)\in \R^2$, we know that $v$ can be written as $(v_1,0)+(0,v_2)$ and that the vectors $(v_1,0)$ and $(0,v_2)$ are perpendicular (since they lie on different axis - $(v_1,0)$ lies on the $X$-axis and $(0,v_2)$ lies on the $Y$-axis).

Therefore, we know that $0v_1v$ is a right triangle - as is $0v_2v$. In particular, we know that the angle $0\hat{v_1}v$ is a right angle. So, by applying the Pythagorean Theorem, we see that $\overline{0v_1}^2+\overline{v_1v}^2=\overline{0v}^2$.

But $\overline{0v_1}$ is just $\abs{v_1}$, and $\overline{v_1v}$ is just $\abs{v_2}$ (since they are perpendicular). Not only that, but $\overline{0v}$ is just $\norm{v}$, by definition. So the above equation becomes
\[\norm{v}^2=\abs{v_1}^2+\abs{v_2}^2.\]

Finally, since $\abs{x}\geq0$ and $x^2\geq0$ for all $x\in \R$, we can finally get our final equation
\[\norm{v}^2=v_1^2+v_2^2.\]

You might be asking why we left all this for now, instead of doing it back when we were doing geometric computations with vectors. Well, there's one good reason...

\begin{lemma}
	For any vector $v\in\R^2$ we have that $\norm{v}=\gen{v,v}^{\frac{1}{2}}$.
\end{lemma}
\begin{proof}
	This is trivial by the preceding discussion:
	
	Let $v=(v_1,v_2)$. Then $\gen{v,v}=v_1v_1+v_2v_2=v_1^2+v_2^2=\norm{v}^2$.
\end{proof}
\begin{cor}
	Let $v,u\in \R^2$ be any two vectors and $\theta\in[0,2\pi)$ be the angle between them. Then $\cos\theta = \dfrac{\gen{v,u}}{\norm{v}\norm{u}}$.
\end{cor}
\begin{proof}
	Remember, from geometry, that if $ABC$ is a triangle with sides $a,b,c\in\R$ and angle $\theta\in[0,2\pi)$ between $a$ and $b$, then the Law of Cosines tells us that
	\[c^2=a^2+b^2-2\abs{a}\abs{b}\cos\theta.\]
	
	Take now any two vectors $v,u\in \R^2$. They determine a unique triangle $0vu$, whose sides measure $\overline{0v}=\norm{v}$, $\overline{0u}=\norm{u}$ and $\overline{vu}$. Let us first compute $\overline{vu}$.
	
	We claim that $\overline{vu}=\norm{v-u}$.
	
	\[\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=.8cm,y=.8cm]
	\begin{axis}[
	x=.8cm,y=.8cm,
	axis lines=middle,
	xmin=-0.9199999999999992,
	xmax=9.040000000000004,
	ymin=-1.0600000000000016,
	ymax=5.980000000000001,
	yticklabels={},	
	xticklabels={},]
	\clip(-0.92,-1.06) rectangle (9.04,5.98);
	\draw [->] (0.,0.) -- (2.,4.);
	\draw [->] (0.,0.) -- (6.,1.);
	\draw [->,dash pattern=on 3pt off 3pt] (6,1) -- (2,4) node [midway,above,sloped] {$v-u$};
	\begin{scriptsize}
	\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	\draw [fill=ududff] (2.,4.) circle (2.5pt);
	\draw[color=black] (0.86,2.37) node {$v$};
	\draw [fill=ududff] (6.,1.) circle (2.5pt);
	\draw[color=black] (3.5,0.4) node {$u$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]
	
	This is easy to see, because $(v-u)+u=v$ so if we overlay the vector $v-u$ at the end of the vector $u$ we get the vector $v$ (by definition of vector sum).
	
	Now that that's taken care of, we can use the previous lemma to see that 
	\[\norm{v-u}^2=\gen{v-u,v-u}\]and since we already know that the inner product preserves addition and scalar multiplication, that is simply
	\[\norm{v-u}^2=\gen{v-u,v-u}=\gen{v,v}-\gen{u,v}-\gen{v,u}+\gen{u,u}\]which, since we already know the inner product is commutative, is simply
	\[\norm{v-u}^2=\gen{v-u,v-u}=\gen{v,v}-2\gen{v,u}+\gen{u,u}.\]
	
	Now we use that $\gen{v,v}=\norm{v}^2$ and $\gen{u,u}=\norm{u}^2$ to see that
	\[\norm{v-u}^2=\norm{v}^2+\norm{u}^2-2\gen{v,u}.\]
	
	But, since $0vu$ is a triangle with sides $\norm{v}$, $\norm{u}$ and $\norm{v-u}$, we have that the Law of Cosines tells us that
	\[\norm{v-u}^2=\norm{v}^2+\norm{u}^2-2\norm{v}\norm{u}\cos\theta,\]where $\theta$ is the angle between $v$ and $u$.
	
	Finally, by comparing the two equalities we see that\[\gen{v,u}=\norm{v}\norm{u}\cos\theta\]so
	\[\cos\theta=\frac{\gen{v,u}}{\norm{v}\norm{u}}\]and the result follows.
\end{proof}
\begin{cor}
	Two non-null vectors $v,u\in\R^2$ are perpendicular if, and only if, $\gen{v,u}=0$.
\end{cor}
\begin{proof}
	Assume $v\perp u$. Then if $\theta\in[0,2\pi)$ is the angle between $v$ and $u$, we know that $\theta=\frac{\pi}{2}$. But then $\cos\theta=0$, so
	\[\gen{v,u}=\norm{v}\norm{u}\cos\theta=\norm{v}\norm{u}\cdot0=0\]and we see that $\gen{v,u}=0$.
	
	Conversely, if $v,u\in \R^2$ are non-null vectors such that $\gen{v,u}=0$, then
	\[\cos\theta=\frac{\gen{v,u}}{\norm{v}\norm{u}}=\frac{0}{\norm{v}\norm{u}}=0\]so $\cos\theta=0$.
	
	This implies $\theta\in\{\frac{\pi}{2},\frac{3\pi}{2}\}$, but since we're only measuring internal angles, $0\leq \theta\leq\frac{\pi}{2}$. And so, the only possible case is that $\theta=\frac{\pi}{2}$, so $v\perp u$.
	
	This finishes the proof.
\end{proof}

\begin{cor}[Cauchy-Schwarz]
	For any two vectors $v,u\in \R^2$, $\abs{\gen{v,u}}\leq \norm{v}\norm{u}$.
\end{cor}
\begin{proof}
	Since $\gen{v,u}=\norm{v}\norm{u}\cos\theta$ and $-1\leq \cos\theta\leq 1$, we see that 
	\[-\norm{v}\norm{u}\leq \gen{v,u}\leq \norm{v}\norm u\]so $\abs{\gen{v,u}}\leq \norm{v}\norm{u}$, as we had claimed.
\end{proof}

\begin{prop}
	Let $v,u\in \R^2$ be any two vectors, and $\lambda \in \R$. Then the following hold:
	\begin{enumerate}
		\item $\norm{\lambda v}=\abs{\lambda}\norm{v}$;
		\item $\norm{v}\geq0$, with the equality happening if, and only if $v=0$;
		\item $\norm{v+u}\leq \norm{v}+\norm{u}$.
	\end{enumerate}
\end{prop}
\begin{proof}
	\begin{enumerate}
		\item \begin{align*}
			\norm{\lambda v}&=\gen{\lambda v,\lambda v}^{\frac{1}{2}}\\
			&=\lambda^\frac{1}{2}\gen(v,\lambda v)^{\frac{1}{2}}\\
			&=\lambda^\frac{1}{2}\lambda^\frac{1}{2}\gen{v,v}^\frac{1}{2}=\abs{\lambda}\norm{v}
		\end{align*}
		
		\item $\norm{v}=\gen{v,v}^\frac{1}{2}=\sqrt{v_1^2+v_2^2}\geq 0$.
		
		If $\norm{v}=0$, then $v_1^2+v_2^2=0$, so $v_1^2=-v_2^2$, but they're both non-negative, so $v_1^2=v_2^2=0$. This implies $v=0$.
		
		Clearly, if $v=0$ then $\norm{v}=0$.
		
		\item \begin{align*}
			\norm{v+u}^2&=\gen{v+u,v+u}\\
			&=\gen{v,v}+\gen{v,u}+\gen{u,v}+\gen{u,u}\\
			&=\gen{v,v}+2\gen{v,u}+\gen{u,u}\\
			&\leq \norm{v}^2+2\abs{\gen{v,u}}+\norm{u}^2\\
			&\le \norm{v}^2+2\norm{v}\norm{u}+\norm{u}^2\\
			&=(\norm{v}+\norm{u})^2			
		\end{align*}so $\norm{v+u}\leq \norm{v}+\norm{u}$, as we had claimed.
	\end{enumerate}

This finishes the proof.
\end{proof}

\begin{cor}
	For any vector $v\in \R^2$ we have that $\gen{v,v}\geq 0$. In particular, $\gen{v,v}=0$ if, and only if, $v=0$.
\end{cor}
\begin{proof}
	This follows trivially from the previous proposition and the fact that $\gen{v,v}=\norm{v}^2$.
\end{proof}

It may not seem much, but this allows us to define:

\begin{df}
	Let $v,u\in R^2$ be any two vectors. We define the \textbf{distance between $v$ and $u$} to be $d(v,u)\in \R$, which is given by
	\[d(v,u):=\norm{v-u}.\]
\end{df}

\begin{prop}
	The distance we defined above satisfies the following properties for all $v,u,w\in \R^2$:
	\begin{enumerate}[a)]
		\item $d(v,u)=d(u,v)$;
		\item $d(v,u)\geq0$, with the equality happening precisely when $v=u$;
		\item $d(v,u)\leq d(v,w)+d(w,u)$.		
	\end{enumerate}
\end{prop}
\begin{proof}
	\begin{enumerate}[a)]
		\item \[d(v,u)=\norm{v-u}=\norm{-(u-v)}=\norm{u-v}=d(u,v).\]
		
		\item This is obvious from the definition.
		
		\item \begin{align*}
			d(v,u)&=\norm{v-u}\\
			&=\norm{v-w+w-u}\\
			&=\norm{(v-w)+(w-u)}\\
			&\leq \norm{v-w}+\norm{w-u}=d(v,w)+d(w,u).
		\end{align*}
	\end{enumerate}
\end{proof}

\begin{cor}
	The letter (c) above is an equality if, and only if, $v,u,w$ lie on the same line and $w$ is between $v$ and $u$.
\end{cor}
\begin{proof}
	We'll prove this for lines through zero. The proof for an arbitrary line follows directly and is left as an exercise to the reader.
	
	Let $r=\R v$ be a line through zero, such that $u\in r$. Then $u=\mu v$ for some $\mu\in \R$. Notice that we can assume that all three of $0,v,u$ are different, cause, otherwise, the result is trivially true. This implies $\mu\neq 1$ and both different from $0$.
	
	Then
	\begin{align*}
		d(v,u)&=\norm{v-u}\\
		&=\norm{v-\mu v}\\
		&=\norm{(1-\mu)v}=\abs{1-\mu}\norm{v}
	\end{align*}and similarly $d(v,0)=\norm{v}$ and $d(u,0)=\norm{u}=\abs{\mu}\norm{v}$.
	
	Therefore, if $v$ is between $0$ and $u$, it means that $0<1<\mu$, so
	\begin{align*}
		d(0,u)&=\abs{\mu}\norm{v}\\
		&=(\mu)\norm{v}\\
		&=(\mu-1+1)\norm{v}\\
		&=(\mu-1)\norm{v}+\norm v\\
		&=\abs{\mu-1}\norm{v}+\norm{v}=d(v,u)+d(0,v)
	\end{align*}and we see the equality holds.
	
	\bigskip
	Conversely, assume $v,u,0\in \R^2$ are such that the equality holds, that is, 
	\[d(0,u)=d(v,0)+d(u,v).\]
	
	Then if either of these is zero, the result follows - for instance, $d(v,0)=\norm{v}$ implies $v=0$ (so they lie in the same line), and since between any two points there's always a line, this implies that $u$ is also in a line with $v$ and $0$.
	
	So we can assume all of these distances are non-zero, which means that $u$ and $v$ are non-zero vectors. 
	
	But we've already shown that
	\[\norm{v-u}^2=\norm{v}^2-2\gen{v,u}+\norm{u}^2,\]but the equality\[d(0,u)=d(v,0)+d(u,v)\]can be rewritten as
	\[d(u,v)=d(0,u)-d(0,v)\]which gives us
	\begin{gather*}
		d(u,v)=d(0,u)-d(0,v)\\
		\norm{v-u}=\norm{u}-\norm{v}\\
		\norm{v-u}^2=\left(\norm{u}-\norm{v}\right)^2\\
		\norm{v-u}^2=\norm{u}^2-2\norm{u}\norm{v}+\norm{v}^2
	\end{gather*}and by comparing the two expressions for $\norm{v-u}^2$ we see that $\gen{v,u}=\norm{v}\norm{u}$.
	
	But we've also shown that, for any two vectors (in particular for $v,u$), $\gen{v,u}=\norm{v}\norm{u}\cos\theta$, where $\theta$ is the angle between $v,u$.
	
	Therefore, once again by comparing the two expressions for $\gen{v,u}$, we see that $\cos\theta=1$, so $\theta=0$.
	
	This tells us that $v\parallel u$ and finishes the proof.
\end{proof}

\begin{ex}
	Let $v=(2,3)$, $u=(4,-1)$. Then $d(v,u)=\norm{v-u}=\norm{(-2,4)}=\sqrt{(-2)^2+4^2}=\sqrt{4+16}=\sqrt{20}=2\sqrt 5$.
	
	Notice that $d(e_1,e_2)=\norm{(1,0)-(0,1)}=\sqrt{1^2+1^2}=\sqrt{2}$ which is precisely the diagonal of a square whose side length is 1. 
	
	These examples show that this notion of distance is precisely the notion of distance that's intrinsic to the geometry of the Euclidean plane. We just have a proper way of computing this distances now.
\end{ex}

\begin{rmk}
	The third property above is called the \textbf{Triangle Inequality}. It basically says that in any triangle you can't have a side that's bigger than the sum of the other two.
\end{rmk}

Now that we have an algebraic notion of distance, you know what we're gonna do next, right?

\subsection{Geomancy}

In this section we're gonna prove some results about the geometry of $\R^2$.

For instance, we've already proven that any two lines $r,s\subseteq \R^2$ can be described, uniquely, by two pairs a vectors $v_1,v_2,u_1,u_2$ such that $r=\R v_1+v_2$ and $s=\R u_1+u_2$. Let us now prove that this is indeed a good description of lines:

\begin{prop}
	Let $r,s\subseteq \R^2$ be two different lines through zero in $\R^2$. Then $r\cap s=\{0\}$.
\end{prop}
\begin{proof}
	Let $r=\R v$ and $s=\R u$ for some $v,u\in \R^2$. We already know that $0\in r\cap s$.
	
	Assume there is some $x\in r\cap s$ such that $x\neq 0$. This means that $r=\lambda v=\mu u$, for some $\lambda,\mu\in\R$, both different from $0$. But since $\lambda$ is real, $\lambda v=\mu u$ implies $v=\frac{\mu}{\lambda }u$ and therefore $r=\R v=\R u=s$ (since $v\parallel u$).
	
	But, by hypothesis, we're assuming that $ r\neq s$, so this cannot happen. This means that we cannot assume that there's some $x\neq 0$ in $r\cap s$. It follows that $r\cap s=\{0\}$, which proves the result.
\end{proof}
\begin{cor}
	Let $r,s\subseteq \R^2$ be two different lines in $\R^2$. Then $r\cap s$ is either $\varnothing$ or $\{0\}$.
\end{cor}
\begin{proof}
	Let $r=\R v_1+v_2$ and $s=\R u_1+u_2$ for some $v_1,v_2,u_1,u_2\in \R^2$.
	
	If $r\cap s=\varnothing$, there's nothing to do.
	
	Assume, then, that $r\cap s\neq\varnothing$. That means there is at least one $x\in r\cap s$. Assume, then, that there is also $y\in r\cap s$. We will show that $x=y$.
	
	If that's the case (that is, $x,y\in r\cap s$), then there are real numbers $\lambda_1,\lambda_2,\mu_1,\mu_2\in\R$ such that
	\[x=\lambda_1v_1+v_2=\mu_1u_1+u_2\]
	\[y=\lambda_2v_1+v_2=\mu_2u_1+u_2.\]
	
	But then, $x-y=(\lambda_1-\lambda_2)v_1=(\mu_1-\mu_2)u_1$ and, therefore, $x-y\in\R v_1\cap \R u_1$.
	
	If $\R v_1=\R u_1$, then, by definition, $r\parallel s$ and therefore $r\cap s=\varnothing$. But this contradicts our assumption that there are $x,y\in r\cap s$. Therefore, $\R v_1\neq \R u_1$.
	
	This means that we can use the proposition to conclude that $\R v_1\cap \R u_1=\{0\}$. But we've just shown that $x-y\in \R v_1\cap \R u_1$. It follows that $x-y=0$, and so $x=y$.
	
	This shows that if we have two points in $r\cap s$, then they are in fact the same point.
	
	So $r\cap s$ has, at most, one point, which ends the proof.
\end{proof}

This allows us to prove some very useful results:

\begin{df}
	A \textbf{rhombus} is a set of four points $A,B,C,D$ in the Euclidean plane such that $\overline{AB}=\overline{BC}=\overline{CD}=\overline{DA}$.
	
	In other words, a rhombus is a quadrilateral which has four congruent sides.
\end{df}

\begin{lemma}
	Let $\R v,\R u$ be two lines through zero. Then $\R v\perp \R u$ if, and only if, $v\perp u$.
\end{lemma}
\begin{proof}
	Assume $\R v\perp \R u$. This means that any two vectors $v'\in \R v$ and $u'\in \R u$ are perpendicular. In particular, $v\in \R v$ and $u\in \R u$ implies that $v\perp u$.
	
	\bigskip
	Conversely, assume that $v\perp u$. Take now $v'\in \R v$ and $u'\in \R u$. We want to show that $v'\perp u'$.
	
	Since $v'\in \R v$ and $u'\in \R u$, there are real numbers $\lambda,\mu\in \R$ such that $v'=\lambda v$ and $u'=\mu u$, by definition.
	
	But then:
	\[\gen{v',u'}=\gen{\lambda v,\mu u}=\lambda\mu\gen{v,u}=0\] since $v\perp u$. This shows that $v'\perp u'$.
	
	The result follows.
\end{proof}
\begin{cor}
	The diagonals of a rhombus are perpendicular to each other.
\end{cor}
\begin{proof}
	First, notice that any rhombus can be considered as having vertices $0,v,v+u,u$ where $\norm v=\norm u$.
	
	Take now the lines $\R(v+u)$ and $\R(v-u)+v$, which contain the diagonals of the rhombus. To show that the diagonals are perpendicular, it suffices to show that these two lines are perpendicular.
	
	But notice that since $\R(v-u)+v\parallel\R(v-u)$, it suffices to show that $\R(v+u)\perp\R(v-u)$.
	
	Finally, notice that, by the preceding lemma, showing $\R(v+u)\perp\R(v-u)$ is the same as showing that $(v+u)\perp (v-u)$.
	
	This can be done by a simple computation:
	\begin{align*}
		\gen{v+u,v-u}&=\gen{v,v}-\gen{v,u}+\gen{u,v}-\gen{u,u}\\
		&=\norm{v}^2-\gen{v,u}+\gen{v,u}-\norm{u}^2=0
	\end{align*}since $\norm{v}=\norm{u}$.
	
	This shows that $(v+u)\perp(v-u)$, which, by the preceding observations, ends the proof.
\end{proof}

\begin{df}
	A \textbf{rectangle} is a set of four points $A,B,C,D$ in the Euclidean plane such that $A\hat{B}C$, $B\hat{C}D$, $C\hat{D}A$ and $D\hat{A}B$ are right angles.
	
	In other words, a rectangle is a quadrilateral which has four congruent angles (which are, therefore, right angles).
\end{df}
\begin{lemma}
	Let $v,u\in \R^2$ be two non-null vectors. Then $(v+u)\parallel(v-u)$ if, and only if, $v\parallel u$.
\end{lemma}
\begin{proof}
	Assume $(v+u)\parallel(v-u)$ and consider $\R(v+u)$ and $\R(v-u)$. Since they are lines through zero, this means they are equal, $\R(v+u)=\R(v-u)$. Let's call $r=\R (v+u)=\R(v-u)$.
	
	We've already shown that lines through zero are closed under addition, so $(v+u)+(v-u)\in r$.
	
	But $(v+u)+(v-u)=2v\in \R v$, and $v\neq 0$ implies, since two lines through zero meet only at zero, that $\R v=r$
	
	Similarly, $(v+u)-(v-u)$ must lie in $r$, but $(v+u)-(v-u)=2u\in \R u$, and, once again, since $u\neq 0$ and two lines through zero meet only at zero, this implies that $\R u=r$.
	
	Combining these two, we see that $\R v=r=\R u$, and so $v\parallel u$.
	
	\bigskip
	Conversely, if $v\parallel u$, this means that $\R v=\R u$, and so, since lines through zero are closed under addition, this means that both $v+u$ and $v-u$ lie in $\R v=\R u$. Therefore $v+u\parallel v-u$, which ends the proof.
\end{proof}

\begin{cor}
	The diagonals of a rectangle meet at their midpoint.
\end{cor}
\begin{proof}
	First, notice that any rectangle can be considered as having vertices $0,v,v+u,u$ where $v\in\mb X$ and $u\in\mb{Y}$.
	
	Since $v\nparallel u$, by hypothesis, the preceding lemma tells us that $(v+u)\nparallel(v-u)$.
	
	In particular, since $\R(v-u)+v\parallel\R (v-u)$ this implies that the line $\R(v-u)+v$ crosses the line $\R(v+u)$, since they're not parallel.
	
	Now take the midpoint of the line segment $0v+u$ - that is, $\frac{1}{2}(v+u)$. We claim that $\frac{1}{2}(v+u)\in \R(v-u)+v$.
	
	Indeed, 
	\begin{align*}
		\frac{1}{2}(v+u)&=\frac{1}{2}v+\frac{1}{2}u\\
		&=\frac{1}{2}v+\frac{1}{2}u-v+v\\
		&=-\frac{1}{2}v+\frac{1}{2}u+v\\
		&=-\left(\frac{1}{2}v-\frac{1}{2}u\right)+v=-\frac{1}{2}(v-u)+v\in \R(v-u)+v
	\end{align*}so $\frac{1}{2}(v+u)$ lies in both $\R(v+u)$ and $\R(v-u)+v$. Since two lines meet at, at most, one point, it follows that $\R(v+u)\cap\R(v-u)+v=\{\frac{1}{2}(v+u)\}$.
	
	\bigskip
	Finally, it remains to show that $\frac{1}{2}(v+u)$ is the midpoint of the line segment $vu$.
	
	To do this, let's simply compute:
	
	\begin{align*}
		d\left(\frac{1}{2}(v+u),v\right)&=\Norm{\left(\frac{1}{2}(v+u)\right)-v}\\
		&=\Norm{\frac{1}{2}v+\frac{1}{2}u-v}\\
		&=\Norm{-\frac{1}{2}v+\frac{1}{2}u}\\
		&=\Norm{-\frac{1}{2}(v-u)}\\
		&=\abs{-\frac{1}{2}}\Norm{v-u}\\
		&=\frac{1}{2}d(v,u)=\frac{1}{2}\overline{vu}.
	\end{align*}
	
	It follows that $\frac{1}{2}(v+u)$ is indeed the midpoint of $vu$, which ends the proof.
\end{proof}

\begin{df}
	Given $x\in \R^2$ any vector and $r\in \R$ a non-negative real number, a \textbf{circle of radius $r$ and center $x$} is the set $\mc C(x,r)$ given by
	\[\mc C(x,r):=\{v\in \R^2\mid d(v,x)=r\}.\]
	
	In other words, a circle is the set of all points that are a fixed distance from a fixed point.
\end{df}

\begin{df}
	Given a line $r\subseteq \R^2$ and a point $v\in \R^2$ we define the \textbf{distance between $v$ and $r$} to be the real number $d(v,r)\in\R$ given by
	\[d(v,r):=\min_{u\in r}\{d(v,u)\}.\]
	
	That is, the distance between a point and a line is the length of the smallest line segment connecting the point to the line.
\end{df}

\begin{lemma}
	Let $r\subseteq \R^2$ be a line and $v\in \R^2$ be any point. Then the distance between $r$ and $v$ is well-defined.
\end{lemma}
\begin{proof}
	We need to prove that no matter which $r$ and which $v$ we choose, $d(v,r)$ always exists.
	
	To do this, consider the following construction:
	
	\begin{enumerate}[(i.)]
		\item Take $t$ the line perpendicular to $r$ through $v$;
		\item Mark $u\in r\cap t$.
	\end{enumerate}

	We claim that $d(v,u)=\min_{w\in r}\{d(v,w)\}$.
	
	Notice that $\R$ is totally ordered, so it suffices to show that there's no one smaller than $d(v,u)$.
	
	Take any $w\in r$ and consider the triangle $vuw$.
	
	Since, by construction $vu\perp r$, it follows that $vuw$ is a right-triangle with hypotenuse $vw$. But, by Pythagoras' Theorem, we know that $d(v,w)^2=d(v,u)^2+d(u,w)^2$.
	
	In particular, $d(v,w)^2\geq d(v,u)^2$, which implies (since distances are always non-negative) that $d(v,w)\geq d(v,u)$.
	
	This shows that $d(v,u)\leq d(v,w)$ for all $w\in r$, and so $d(v,u)=\min_{w\in r}\{d(v,w)\}$, which ends the proof.
\end{proof}
\begin{cor}
	Let $v\in \R^2$ and $r\subseteq \R^2$ be any line. Let also $u\in r$ be such that $d(v,u)=d(v,r)$. Then $v-u\perp r$.
\end{cor}
\begin{proof}
	By the same argument as before: Assume $w\in r$ is the point obtained by the construction of the preceding proof. So $v-w\perp r$, by construction.
	
	It follows that $vuw$ is a right-triangle at $w$. Once again, by using Pythagoras' Theorem, we see that $d(v,w)^2=d(v,u)^2+d(u,w)^2$.
	
	But $d(v,u)=d(v,w)$ implies $d(u,w)^2=0$, which is simply $d(u,w)=0$. Now, by the properties of distance, this happens if, and only if, $u=w$.
	
	Since $v-w\perp r$ by construction, this implies $v-u\perp r$, which ends the proof.
\end{proof}

\begin{lemma}
	Let $r=\R v+u\subseteq \R^2$ be any line, and $R\in \R$ any non-negative number. Then the set 
	\[\mc D(R,r):=\{w\in \R^2\mid d(w,r)=R\}\]is just the set $s\cup t$ where $s=r+\overrightarrow{R}$ and $t=r-\overrightarrow{R}$, and $\overrightarrow{R}\in \R^2$ is any vector such that $\overrightarrow{R}\perp r$ and $\norm{\overrightarrow{R}}=R$.
\end{lemma}
\begin{proof}
	Take $x\in \mc D(R,r)$. This means that there's some $y\in r$ such that $d(x,y)=R$.
	
	Define then $\overrightarrow{R}:=x-y$. By definition, $\norm{\overrightarrow{R}}=R$ and, by the preceding corollary, $\overrightarrow{R}\perp r$.
	
	Since $y\in r=\R v+u$ and $\overrightarrow{R}=x-y$, we see that $$x=y+\overrightarrow{R}\in r+\overrightarrow{R}=s.$$
	
	This shows that $\mc D(R,r)\subseteq s\cup t$.
	
	\bigskip
	Conversely, take $x\in s\cup t$ and any $\overrightarrow{R}\in\R^2$ such that $\norm{\overrightarrow{R}}=R$ and $\overrightarrow{R}\perp r$. This means that $x=\lambda v+u\pm\overrightarrow{R}$ for some $\lambda\in \R$.
	
	Take now $y=\lambda v+u$. Clearly, $y\in r$. Now,
	\begin{align*}
		d(x,y)&=\norm{x-y}\\
		&=\norm{(\lambda v+u\pm\overrightarrow{R})-(\lambda v+u)}\\
		&=\norm{\pm\overrightarrow{R}}\\
		&=\norm{\overrightarrow{R}}=R.
	\end{align*}
	
	Notice, however, that $x-y=\pm\overrightarrow{R}\perp r$, so, by the preceding lemma, we see that $x\in \mc D(R,r)$.
	
	This shows that $s\cup t\subseteq\mc D(R,r)$, and, therefore, $\mc D(R,r)=s\cup t$, which ends the proof.
\end{proof}

\begin{lemma}
	If a line passes through the center of a circle, then it crosses the circle.
\end{lemma}
\begin{proof}
	Let $\mc C(x,r)$ be a circle for some $x\in \R^2$ and some non-negative $r\in \R$, as well as $s:=\R v+u$ be a line for some $v,u\in \R^2$.
	
	If $x\in s$ this means that $x=\lambda v+u$ for some $\lambda \in \R$.
	
	We claim that $x+v'\in \mc C(x,r)$, for $v':=\frac{r}{\norm v}v$.
	
	Indeed, 
	\begin{align*}
		d(x+v',x)&=\norm{(x+v')-x}\\
		&=\norm{v'}\\
		&=\Norm{\frac{r}{\norm v}v}\\
		&=\abs{\frac{r}{\norm v}}\norm{v}\\
		&=\frac{r}{\norm v}\norm{v}=r
	\end{align*}
	
	Now,
	\begin{align*}
		x+v'&=(\lambda v+u)+\left(\frac{r}{\norm v}v\right)=\left(\lambda + \frac{r}{\norm v}\right)v+u\in \R v+u
	\end{align*}and so $x+v'$ is a point in $\R v+u$ that is at distance $r$ from $x$ - this means that $x+v'\in r\cap \mc C(x,r)$ and, so, the proof is done.
\end{proof}
\begin{lemma}
	If a line contains a point in the inside of a circle, then it crosses the circle.
\end{lemma}
\begin{proof}
	Consider the line $s':=\R v$, the point $x':=x-u$ and the circle $\mc C':=\mc C(x',r)$. Then for all $y'\in s'$ we have
	\begin{align*}
		d(y',x')&=\norm{y'-x'}\\
		&=\norm{\lambda v-(x-u)}\\
		&=\norm{\lambda v+u-x}\\
		&=\norm{y-x}=d(y,x)
	\end{align*}where $y=\lambda v+u\in s$. This shows that $d(\lambda v+u,x)=d(\lambda v,x-u)$ for all $\lambda\in \R$.
	
	This means that we can rephrase the problem to work only with lines through zero.
	
	So, assume $u=0$, that is, $s$ is a line through zero.
	
	Take now any $y\in s$ such that $d(y,x)<r$ (that is, $y$ is inside the circle $\mc C(x,r)$). Since $s$ is a line through zero, every scalar multiple of $y$ also belongs to $s$.
	
	Assume that $d(y',x)<r$ for all $y'\in s$. Then:
	\begin{align*}
		d(y,y')\leq d(y,x)+d(y',x)\leq r+r=2r
	\end{align*}so we're saying that no matter which point $y'\in s$ we pick, its distance from $y$ can never exceed $2r$.
	
	But that's absurd - take, for instance, $y'=y+\frac{3r}{\norm{y}}y=\left(1+\frac{3r}{\norm y}\right)y\in s$.
	
	Then
	\begin{align*}
		d(y,y')&=\norm{y-y'}\\
		&=\Norm{y-\left(1+\frac{3r}{\norm y}\right)y}\\
		&=\Norm{\left(-\frac{3r}{\norm y}\right)y}\\
		&\abs{-\frac{3r}{\norm y}}\norm{y}\\
		&=\frac{3r}{\norm y}\norm y=3r
	\end{align*}and so $d(y,y')=3r>2r$.
	
	This shows that there are some (infinitely many) points in $s$ which are outside of $\mc C(x,r)$.
	
	Since lines are connected and there are both points of $s$ which are inside $\mc C(x,r)$ and points of $s$ which are outside $\mc C(x,r)$, then $s$ must cross $\mc C(x,r)$ at some point.
	
	This ends the proof.
\end{proof}

Now, let's introduce a new tool to allow us to prove even more results:

\begin{prop}
	Let $r:=\R v$ be a non-vertical line. Then there is uniquely determined $a\in \R$ such that
	\[r=\{(x,y)\in \R^2\mid y=ax\}.\]
	
	If $r$ is a vertical line, then
	\[r=\{(x,y)\in \R^2\mid x=0\}.\]
\end{prop}
\begin{proof}
	Take $(x,y)\in r$. This means that $(x,y)=\lambda v$ for some $\lambda \in \R$. But writing $v=(v_1,v_2)$ we see that
	\begin{align*}
		(x,y)&=\lambda(v_1,v_2)=(\lambda v_1,\lambda v_2),
	\end{align*}and so $y=\lambda v_2$ and $x=\lambda v_1$. 
	
	Now, since $r$ is non-vertical if, and only if, $v_1\neq 0$, we see that if $r$ is non-vertical, then
	\begin{align*}
		y&=\lambda v_2\\
		&=\frac{\lambda v_1}{\lambda v_1}\lambda v_2\\
		&=\frac{x}{\lambda v_1}\lambda v_2\\
		&=\frac{\lambda v_2}{\lambda v_1}x=\frac{v_2}{v_1}x
	\end{align*}so if we define $a=\frac{v_2}{v_1}$ we can conclude that $(x,y)$ satisfies $y=ax$.
	
	Conversely, if $r$ is vertical, then $v_1=0$, so $x=0$.
	
	This shows that $r\subseteq\{(x,y)\in \R^2\mid y=ax\}$ if it's non-vertical, and $r\subseteq\{(x,y)\in \R^2\mid x=0\}$ if it is vertical.
	
	\bigskip
	On the other hand, take $u\in \{(x,y)\in \R^2\mid y=ax\}$. This means that $u=(u_1,au_1)$ and so $u=u_1(1,a)\in \R(1,a)$, which is a non-vertical line through zero.
	
	Similarly, if $u\in\{(x,y)\in \R^2\mid x=0\}$, then $u=(0,u_2)\in \mb Y$, which is a vertical line through zero.
	
	This ends the proof.
\end{proof}
\begin{cor}
	Let $r=\R v+u$ be a non-vertical line. Then there are some uniquely determined $a,b\in \R$ such that
	\[r=\{(x,y)\in \R^2\mid y=ax+b\}.\]
	
	If $r$ is a vertical line, then there's a uniquely determined $c\in \R$ such that
	\[r=\{(x,y)\in \R^2\mid x=c\}.\]
\end{cor}
\begin{proof}
	Let $r=\R v+u$ be a non-vertical line, $u=(u_1,u_2)$ and $r'=\R v$.
	
	Then $r'$ is also a non-vertical line, so, by the previous proposition, we know that there's a uniquely determined $a\in \R$ such that
	\[r'=\{(x,y)\in \R^2\mid y=ax\}.\]
	
	Take then $w\in r$. This means that $w=\lambda v+u$ for some $\lambda \in \R$. But what this tells us is that $w$ is just a vector in $r'$ added by $u$. Since all vectors in $r'$ are of the form $(x,ax)$, we know that $w=(x,ax)+u$, which is simply $(w_1,w_2)=(x+u_1,ax+u_2)$. 
	
	It follows that
	\begin{align*}
		w_2&=ax+u_2\\
		&=ax+(au_1-au_1)+u_2\\
		&=(ax+au_1)+(u_2-au_1)=aw_1+(u_2-au_1)\\
	\end{align*}so if we put $b=(u_2-au_1)$ we see that $b$ is uniquely determined by $v$ and $u$ (since $a=\frac{v_2}{v_1}$).
	
	Therefore, $w_2=aw_1+b$, so $w\in \{(x,y)\in \R^2\mid y=ax+b\}$.
	
	Analogously, if $r$ was vertical, this would mean that $v\parallel e_2$, so $v=(0,v_2)$. Since every point in $r$ is of the form $\lambda v+u$ this becomes $\lambda(0,v_2)+(u_1,u_2)=(u_1,\lambda v_2+u_2)$. We see then that the $X$-coordinate of such points is always $u_1$. So if we put $c=u_1$, we see that $(x,y)\in r$ implies $x=c$, so $r\subseteq\{(x,y)\in \R^2\mid x=c\}$.
	
	\bigskip
	Conversely, take $w\in \{(x,y)\in \R^2\mid y=ax+b\}$. This means that $w=(w_1,aw_1+b)$, so $w=w_1(1,a)+(0,b)\in \R (1,a)+(0,b)$, which is a non-vertical line since it's not parallel to $e_2$.
	
	Analogously, if $w\in \{(x,y)\in \R^2\mid x=c\}$ then $w=(c,w_2)=w_2e_2+(c,0)\in\R e_2+(c,0)$, so $w$ is in line which is parallel to $e_2$, and, so, it is vertical.
	
	This finishes the proof.
\end{proof}

\begin{lemma}
	Let $c\in \R^2$ and $r\in \R$ any non-negative real number. Then $\mc C(c,r)=\mc C(0,r)+c$, that is, any point in $\mc C(c,r)$ is just a point in $\mc C(0,r)$ added to $c$.
\end{lemma}
\begin{proof}
	Call $\mc C=\mc C(0,r)$ and $\mc C'= \mc C(c,r)$. We want to show that $\mc C+c=\mc C'$. Fix, then, $c=(c_1,c_2)$.
	
	Take then $v=(v_1,v_2)\in \mc C$, so $d(v,0)=r$. Therefore, $v+c=(v_1+c_1,v_2+c_2)\in \mc C+c$. Then:
	
	\begin{align*}
		d(v+c,c)&=\norm{(v+c)-c}=\norm{v}=\norm{v-0}=d(v,0)=r
	\end{align*}and so $v+c\in \mc C'$, by definition.
	
	This shows that $\mc C+c\subseteq \mc C'$.
	
	\bigskip
	Conversely, take $v\in \mc C'$, that is, $d(v,c)=r$. We claim that $v-c\in \mc C$. Indeed:
	
	\begin{align*}
		d(v-c,0)&=\norm{(v-c)-0}=\norm{v-c}=d(v,c)=r
	\end{align*}so $v-c\in \mc C$.
	
	Clearly, then, $v=(v-c)+c\in \mc C+c$ so $\mc C'\subseteq \mc C+c$.
	
	These two together show that $\mc C'=\mc C+c$ and the result follows.
\end{proof}

\begin{prop}
	Let $r\in \R$ be any non-negative number. Then
	\[\mc C(0,r)=\{(x,y)\in \R^2\mid x^2+y^2=r^2\}.\]
\end{prop}
\begin{proof}
	Take any $v\in \mc C(0,r)$. Then 
	\begin{align*}
		r&=d(v,0)=\norm{v-0}=\norm{v}=\sqrt{v_1^2+v_2^2}
	\end{align*}therefore $v_1^2+v_2^2=r^2$ and $v\in\{(x,y)\in \R^2\mid x^2+y^2=r^2\}$.
	
	This shows $\mc C(0,r)\subseteq\{(x,y)\in \R^2\mid x^2+y^2=r^2\}$.
	
	Conversely, take $v\in \{(x,y)\in \R^2\mid x^2+y^2=r^2\}$. So
	\[d(v,0)=\norm{v-0}=\norm{v}=\sqrt{v_1^2+v_2^2}=\sqrt{r^2}=r\]which implies $v\in \mc C(0,r)$ and, therefore, $\{(x,y)\in \R^2\mid x^2+y^2=r^2\}\subseteq \mc C(0,r)$.
	
	This shows \[\mc C(0,r)=\{(x,y)\in \R^2\mid x^2+y^2=r^2\},\]which ends the proof.
\end{proof}
\begin{cor}
	Let $r\in \R$ be any non-negative number and $c=(c_1,c_2)\in \R^2$ any point. Then $$\mc C(c,r)=\{(x,y)\in\R^2\mid (x-c_1)^2+(y-c_2)^2=r^2\}$$
\end{cor}
\begin{proof}
	Let $\mc C'=\mc C(c,r)$ and $\mc C=\mc C(0,r)$.
	 
	By the preceding proposition, we know that $\mc C=\{(x,y)\in\R^2\mid x^2+y^2=r^2\}$ and by the one before that we know that $\mc C'=\mc C+c$.
	
	Combining these two together we see that for every $v=(v_1,v_2)\in \mc C'$ we have that $v-c\in \mc C$ and so, if we put $c=(c_1,c_2)$, this tells us that $(v_1-c_1)^2+(v_2-c_2)^2=r^2$.
	
	This proves the result.
\end{proof}

\begin{lemma}
	Let $\mc C_1=\mc C(c_1,r_1),\mc C_2=\mc C(c_2,r_2)$ be any two circles, and let $I=\mc C_1\cap \mc C_2$. Then for all $v\in \R^2$ we have that 
	\[I\iso (\mc C_1+v)\cap (\mc C_2+v).\]
	
	In other words, the number of meeting points of two circles doesn't depend on where they are.
\end{lemma}
\begin{proof}
	First, we'll prove that $(\mc C_1+v)\cap (\mc C_2+v)=I+v$. Then we'll argue that $I+v\iso I$.
	
	Take $x\in (\mc C_1+v)\cap (\mc C_2+v)$. This means that both $d(x,c_1+v)=r_1$ and $d(x,c_2+v)=r_2$ hold, by definition.
	
	But then, since
	\begin{align*}
		r_1&=d(x,c_1+v)\\
		&=\norm{x-(c_1+v)}\\
		&=\norm{x-c_1-v}\\
		&=\norm{(x-v)-c_1}=d(x-v,c_1)
	\end{align*} and a similar equation holds for $c_2$ (that is, $r_2=d(x,c_2+v)=d(x-v,c_2)$) we see that $x-v\in I$, so $x\in I+v$.
	
	This shows $(\mc C_1+v)\cap (\mc C_2+v)\subseteq I+v$.
	
	\bigskip
	Conversely, given any $x\in I+v$, this means that $x-v\in I$, and so 
	\begin{align*}
		r_1&=d(x-v,c_1)\\
		&=\norm{(x-v)-c_1}\\
		&=\norm{x-(c_1+v)}=d(x,c_1+v)
	\end{align*}
	and a similar equation holds for $c_2$ (that is, $r_2=d(x-v,c_2)=d(x,c_2+v)$). But this implies that $x\in (\mc C_1+v)\cap (\mc C_2+v)$, and so $I+v\subseteq (\mc C_1+v)\cap (\mc C_2+v)$.
	
	This proves that $I+v=(\mc C_1+v)\cap (\mc C_2+v)$.
	
	\bigskip
	Finally, consider the function $f:I\to I+v$ given by $x\mapsto x+v$, as well as the function $g:I+v\to I$ given by $y\mapsto y-v$.
	
	Clearly, $f\circ g=\id_{I+v}$ and $g\circ f=\id_I$, so $g=f^{-1}$ and $f$ is an isomorphism.
	
	This finishes the proof.
\end{proof}

\begin{cor}
	Let $c_1,c_2\in \R^2$ be any two points and $r_1,r_2\in \R$ be any two non-negative real numbers. Then $\mc C(c_1,r_1)\cap\mc C(c_2,r_2)$ is either empty, a single point or two points.
\end{cor}
\begin{proof}
	Let $D:=d(c_1,c_2)$ and $R=r_1+r_2$. Let also $I=\mc C(c_1,r_1)\cap \mc C(c_2,r_2)$, $\mc C_1=\mc C(c_1,r_1)$ and $\mc C_2=\mc C(c_2,r_2)$.
	
	Finally, using the preceding lemma, it suffices to prove this result for $c_1=0$, and $c_2=c$, because every other case is just a translation of this case by some $v\in \R^2$. This tells us that $D=\norm{c}$.
	
	Then, we have three cases:
	\begin{itemize}
		\item \underline{$D>R$}:
		
		If there is a point $v\in I$, then $v0c$ is a triangle, so 
		\[d(0,c)\leq d(0,v)+d(c,v).\]
		
		But $v\in I$ implies $d(v,0)=r_1$ and $d(v,c)=r_2$, so $d(0,v)+d(c,v)=R$. Then the above inequality becomes $D\leq R$, which contradicts our assumption that $D>R$.
		
		Therefore, if $D>R$ then $I=\varnothing$.
		
		\item \underline{$D>R$}:
		
		Take, once more, $r=\R(c_1-c_2)+c_1$ the line through both centers. Then, once again, we know this line crosses both circles at points $v,u$, respectively.
		
		Now, since $D>R$, we can write $D=R+\delta$ for some positive $\delta\in \R$. Notice that we can rewrite this as
		\[d_1+d_2=r_1+r_2+\delta\]so
		\[d(c_1,c_2)=d(c_1,v)+d(c_2,u)+\delta,\]but we also know that
		\[d(c_1,c_2)=d(c_1,v)+d(c_2,u)+d(v,u),\]since $c_1,c_2,v,u$ are colinear. So $\delta=d(v,u)$.
	\end{itemize}
\end{proof}