\chapter{Real Linear Algebra}
\section{Introduction}
\subsection{Operations and other fun things to do with your friends and family}

To start working with vector spaces we first need to understand that the perspective is going to change a bit from the previous chapter. We're leaving the domain of \textbf{set theory} and jumping right in the domain of \textbf{algebra}.

\textit{Algebra }is the domain of mathematics that deals with operations and their properties.

\begin{df}
	Given any set $X$, a \textbf{(binary) operation} on $X$ is a function $f:X\times X \to X$.
\end{df}

\begin{ex}
	Let $\N$ be the set of natural numbers, as before. We have a few operations here:
	\[f,g,h:\N\times\N \to \N\]
	\[(n,m)\mapsto f(n,m):=n+m\]
	\[(n,m)\mapsto g(n,m):=nm\]
	\[(n,m)\mapsto h(n,m):=n^m\]and some of these operations have some properties that the others don't.
	
	For instance, all three functions satisfy the following property:
	
	\begin{itemize}
		\item Let $\phi$ be an operation on $X$. There is some $n_e\in X$ such that $\phi(n,n_e)=n$ for all $n\in X$.
	\end{itemize}

	In the case of $f$, if we choose $n_e:=0$, we see that $f(n,0)=n+0=n$, no matter which $n\in \N$ we chose, so $f$ satisfies the property above.
	
	In the case of $g$, if we choose $n_e:=1$, we see that $g(n,1)=n\cdot1=n$, no matter which $n\in\N$ we chose, so $g$ satisfies the property above.
	
	Finally, in the case of $h$, if we choose $n_e:=1$, we see that $h(n,1)=n^1=n$, no matter which $n\in\N$ we chose, so $h$ satisfies the property above.
	
	\bigskip
	Next up is the property:
	
	\begin{itemize}
		\item Let $\phi$ be an operation on $X$. There is some $n_e\in X$ such that $\phi(n_e,n)=n$ for all $n\in X$.
	\end{itemize}

	What can we say about $f,g,h$ in this case? Well, it's easy to see that for $f$ and $g$ it still holds true - and it does so for the same value of $n_e$ as before.
	
	However, for $h$ it fails. For instance, is there some number $x\in\N$ such that $h(x,2)=2$? Well, by definition of $h$ we would need to have $x^2=2$ and so $x=\sqrt{2}$ which is not in $\N$ - this tells us that there's no such $x\in\N$. It follows that this property fails for $h$.
	
	\bigskip
	Summing up all of these together, we get the following property:
	\begin{itemize}
		\item (Identity element) Let $\phi$ be an operation on $X$. There is some $n_e\in X$ such that $\phi(n,n_e)=n=\phi(n_e,n)$ for all $n\in X$.
	\end{itemize}
	And we see that $f$ and $g$ have what's called an \textit{identity element} - it's an element $n_e$ such that if you fix it in any input of your operation, then your operation is just the identity function.
	
	\bigskip
	Consider now the following property:
	\begin{itemize}
		\item (Associativity) Let $\phi$ be an operation on $X$. Then, for all $n,m,l\in X$ we have that $\phi(\phi(n,m),l)=\phi(n,\phi(m,l))$.
	\end{itemize}

	In the case of $f$ we can check
	\[f(f(n,m),l)=f(n+m,l)=(n+m)+l=n+(m+l)=f(n,m+l)=f(n,f(m,l))\]and see that $f$ is associative.
	
	In the case of $g$ we can check
	\[g(g(n,m),l)=g(nm,l)=(nm)l=n(ml)=g(n,ml)=g(n,g(m,l))\]and see that $g$ is associative.
	
	However, for $h$, once again, this property fails: For instance, let us compare $h(h(2,2),3)$ and $h(2,h(2,3))$:
	\[h(h(2,2),3)=h(2^2,3)=(2^2)^3=4^3=64\]
	\[h(2,h(2,3))=h(2,2^3)=2^{(2^3)}=2^8=256\]so they are clearly different, and $h$ is not associative.
	
	\bigskip
	One more:
	\begin{itemize}
		\item (Commutativity) Let $\phi$ be an operation on $X$. Then, for all $n,m\in X$ we have that $\phi(n,m)=\phi(m,n)$.
	\end{itemize}

	In the case of $f$ we can easily see that $f(n,m)=n+m=m+n=f(m,n)$.
	
	Similarly for $g$, we see that $g(n,m)=nm=mn=g(m,n)$.
	
	But, once again, $h(2,3)=8\neq 9=h(3,2)$, so $h$ is not commutative.
	
	\bigskip
	These are the most common operations in $\N$ and some of their properties. Now, let us show something that is \textbf{not} an operation:
	
	Consider the functions $$f',g':\N\times\N\to\N$$\[(n,m)\mapsto f'(n,m):=n-m\]
	\[(n,m)\mapsto g'(n,m):=n/m.\]
	
	Notice that I've just lied to you - these are \textbf{not} functions. To see that, take $f'$ and apply it on $(3,1)$. By definition of function, $f'(3,1)$ should lie on $\N$, the codomain of $f'$. But, by definition of $f'$, we see that $f'(3,1)=3-1=-2$, which is \textbf{not} in $\N$.
	
	Similarly, $g'$ isn't a function for the same reason: It should take, for instance, $(1,2)$ to a natural number - but it doesn't. It takes $(1,2)$ to $g'(1,2)=1/2$ which, once more, is not a natural number.
	
	However, for \textit{some} specific values of the input, $f'$ and $g'$ really have outputs in $\N$. For that reason, they are called \textbf{partial operations} and, sadly, won't be studied in this text, since we're mostly concerned with proper operations.
	
	If, however, you'd like to learn more about partial operations, you should click \href{https://ncatlab.org/nlab/show/groupoid}{here} or Google for ``groupoid'' - which is precisely the mathematical notion of a set with an associative partial operation.
\end{ex}

\begin{df}
	Given an operation $\phi:X\times X\to X$ we will say that
	\begin{itemize}
		\item (Identity element) $\phi$ admits an \textbf{identity element} if there is some $e\in X$ such that $\phi(x,e)=x=\phi(e,x)$ for all $x\in X$. In this case, $e$ is called an \textbf{identity element};
		\item (Associativity) $\phi$ is \textbf{associative} if $\phi(x,\phi(y,z))=\phi(\phi(x,y),z)$ for all $x,y,z\in X$;
		\item (Commutative) $\phi$ is \textbf{commutative} if $\phi(x,y)=\phi(y,x)$ for all $x,y\in X$;
		\item (Inverse element) $\phi$ admits \textbf{inverse elements} if for all $x\in X$ there is some $y\in X$ such that $\phi(x,y)=e=\phi(y,x)$ for some identity element $e\in X$.
	\end{itemize}
\end{df}

\begin{df}
	Let $X$ be a set with two operations, $f,g:X\times X\to X$. We say that \textbf{$f$ distributes over $g$ on the left} (resp. \textbf{on the right}) if $$f(x,g(y,z))=g(f(x,y),f(x,z))$$ (resp. $$f(g(x,y),z)=g(f(x,z),f(y,z)))$$ for all $x,y,z\in X$.
	
	If $f$ distributes over $g$ on both sides, we simply say that \textbf{$f$ distributes over $g$}.
\end{df}
\begin{ex}
	Following up on the previous example, we see that $g$ (the multiplication) distributes over $f$ (the addition):
	\[g(n,f(m,l))=g(n,m+l)=n(m+l)=nm+nl=f(nm,nl)=f(g(n,m),g(n,l))\]
	\[g(f(n,m),l)=g(n+m,l)=(n+m)l=nl+ml=f(nl,ml)=f(g(n,l),g(m,l))\]but $f$ doesn't distribute (on either side!) over $g$:
	\[1+(1\cdot 1)=1+1=2\neq 4=2\cdot 2=(1+1)\cdot(1+1)\]
	\[(1\cdot 1)+1=1+1=2\neq 4=2\cdot 2=(1+1)\cdot(1+1)\]
\end{ex}

All this talk now brings us to a very specific definition:

\begin{df}
	Let $X$ be a set with two operatios $A,M:X\times X\to X$. We will say that $(X,A,M)$ is a \textbf{field} if
	\begin{multicols}{3}
		\begin{enumerate}[(1)]
			\item $A$ is associative;
			\item $A$ is commutative;
			\item $A$ has an identity element;
			\item $A$ has inverses;
			\item $M$ is associative;
			\item $M$ is commutative;
			\item $M$ has an identity element;
			\item $M$ has inverses (excluding the additive identities);
			\item $M$ distributes over $A$.
		\end{enumerate}
	\end{multicols}
	

In this case, we call $A$ and $M$, respectively, the field's \textbf{addition} and \textbf{multiplication} operations, and denote them simply by $x+y:=A(x,y)$ and $xy:=M(x,y)$ for all $(x,y)\in X\times X$.
\end{df}

\begin{prop}
	The set $\R$ of real numbers with the usual addition and multiplication is a field.
\end{prop}
\begin{proof}
	This is immediate, since for every $x,y,z\in\R$ we have:
	\begin{multicols}{3}
		\begin{enumerate}[(1)]
			\item $x+(y+z)=(x+y)+z$;
			\item $x+y=y+x$;
			\item $x+0=0+x=x$;
			\item $x+(-x)=(-x)+x=0$;
			\item $x(yz)=(xy)z$;
			\item $xy=yx$;
			\item $x\cdot 1=1\cdot x=x$;
			\item $xx^{-1}=x^{-1}x=1$ if $x\neq 0$;
			\item $x(y+z)=xy+xz$ and $(x+y)z=xz+yz$.
		\end{enumerate}
	\end{multicols}	
\end{proof}

\begin{ex}
	Notice, however, that the sets $\N$ and $\Z$, of the naturals and integers, respectively, are \textbf{not} fields: $\N$ doesn't have either additive or multiplicative inverses (so it fails properties (4) and (8)), and $\N$ doesn't have multiplicative inverses (so it fails property (8)).
	
	On the other hand, it's easy to see that $\Q$, the set of rational numbers, is indeed a field. It is actually constructed to be, in some sense, ``the smallest field which extends $\Z$/$\N$''.
	
	Finally, the set $\C$ of complex numbers is also a field if you define the inverse of $z=x+iy$ to be $z^{-1}:=\dfrac{x-iy}{x^2+y^2}$. Indeed:
	
	\[zz^{-1}=(x+iy)\left(\frac{x-iy}{x^2+y^2}\right)=\frac{x^2+y^2}{x^2+y^2}=1\]so it is indeed an inverse for $z$.
\end{ex}

Let us show some properties of fields:

\begin{lemma}
	Let $(k,+,\cdot)$ be a field. Then the following hold:
	\begin{enumerate}[(a)]
		\item There's a unique additive identity;
		\item For each $x\in k$, there's a unique additive inverse;
		\item There's a unique multiplicative identity;
		\item For each $x\in k$, there's a unique multiplicative inverse;
		\item Let $0$ be an additive identity of $k$. Then $0x=0$ for all $x\in k$.
		\item Let $1$ be a multiplicative identity of $k$. Then $-x=(-1)x$, where $(-1)+1=0$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{enumerate}[(a)]
		\item Let $0$ and $0'$ be two additive identities of $k$. Then
		\[0=0+0'=0'\]where the leftmost equality holds since $0'$ is additive identity, and the rightmost equality holds since $0$ is additive identity, and so $0=0'$.
		
		\item Given $x\in k$, let $x'$ and $x''$ be two additive inverses to $x$. Then
		\[x'=x'+0=x'+(x+x'')=(x'+x)+x''=0+x''=x''\]and so $x'=x''$.
		
		\item Let $1$ and $1'$ be two multiplicative identities of $k$. Then
		\[1=1\cdot1'=1'\]where the leftmost equality holds since $1'$ is a multiplicative identity, and the rightmost equality holds since $1$ is a multiplicative identity, so $1=1'$.
		
		\item Given $x\in k$, let $x''$ and $x''$ be two multiplicative inverses to $x$. Then
		\[x'=x'\cdot1=x'(xx'')=(x'x)x''=1\cdot x''=x''\]and so $x'=x''$.
		
		\item Given $x\in k$, we have that
		\[0x=(0+0)x=0x+0x\]since $k$ is a field and $0$ is the additive identity. Let $y\in k$ be the additive inverse of $0x$.
		
		Then, since the above is true, we can see that $(0x)+y=(0x+0x)+y$ is also true. But the LHS is just 0, since $y$ is the additive inverse of $0x$, and the RHS is just $(0x+0x)+y=0x+(0x+y)=0x+0=0x$, so the above equation evaluates to $0=0x$.
		
		\item Given $x\in k$, we have that $0x=(1+(-1))x$ since $1+(-1)=0$. But now, by the distributive property of fields we see that $0x=(1+(-1))x=(1)x+(-1)x$.
		
		But $0x=0$ and $1x=x$, so this is just $0=x+(-1)x$. Since additive inverses are unique, we see that $(-1)x=-x$.
	\end{enumerate}
\end{proof}

This result basically tells us that every field is ``similar'' to $\R$, in some sense.

\begin{rmk}
	The reason why we require that the multiplication has inverses for all elements \textit{except for 0} is precisely because of item (e) above. Since $0x=0$ for all $x$, if we could have some $0^{-1}$, then $0=00^{-1}=1$ so we would have $0=1$.
	
	But since $1x=x$ for all $x$, this would imply that $x=1x=0x=0$, so \textbf{every element of the field would have to be 0 for it to be consistent}.
	
	In other words, the only set that satisfies all the properties of a field and also has a multiplicative inverse to 0 is the set $\{0\}$.
	
	In fact:
\end{rmk}

\begin{prop}
	The set $1=\{0\}$ with addition and multiplication being equal and given by $0+0=0\cdot 0=0$ is a field. Its multiplicative and additive identities are $0$, who is also the inverse of $0$.
\end{prop}
\begin{proof}
	There's literally nothing to prove.
\end{proof}

Finally, to end this section, let us give some examples of fields that aren't $1$, $\Q$, $\R$ or $\C$.

\begin{ex}
	Let $p\in \N$ be a prime number (that is, there are only two ways to write $p=nm$: $n=p, m=1$ and $n=1, m=p$). Consider the set $p\in\N$ - that is, $p=\{0,1,2,\cdots,p-1\}$. We will give a field structure to $p$ as follows:
	
	For any $x,y\in p$, define:
	\begin{itemize}
		\item $x+y$ is the remainder of the division of $x+y$ in $\N$ by $p$;
		\item $xy$ is the remainder of the division of $xy$ in $\N$ by $p$.
	\end{itemize}

	We claim that $p$ with those two operations is a field, which will be denoted by either $\Z_p$, $\Z/p\Z$ or $\mathds F_p$.
	
	For instance, let us do some computations with $p=3$.
	
	In this case, $p=\{0,1,2\}$, and so we have the following tables of operations:
	
	\begin{center}
		\begin{tabu}{|c|[2pt]c|c|c|}
			\hline+&0&1&2\\
			\tabucline[2pt]{-} 0&0&1&2\\
			\hline 1&1&2&0\\
			\hline 2&2&0&1\\
			\hline 
		\end{tabu} and \begin{tabu}{|c|[2pt]c|c|c|}
			\hline $\times$&0&1&2\\
			\tabucline[2pt]{-} 0&0&0&0\\
			\hline 1&0&1&2\\
			\hline 2&0&2&1\\
			\hline 
		\end{tabu}
	\end{center}
	
	It is, then, readily seen that $0$ and $1$ are, respectively, the additive and multiplicative identities of $\mathds{F}_3$.
	
	We can also see that $1+2=0$ so 1 and 2 are additive inverses to each other. Similarly, we see that $1\cdot 1=1=2\cdot 2$ so both 1 and 2 are multiplicative inverses to themselves.
	
	This shows that $\mathds{F}_3$ is a field.
	
	Building similar tables of operations we can prove that any $\mathds{F}_p$ is a field.
	
	Let us now show the necessity of $p$ being a prime.
	
	\bigskip
	Let $4=\{0,1,2,3\}$. Let's try building the same operations:
	
	\begin{center}
		\begin{tabu}{|c|[2pt]c|c|c|c|}
			\hline
			+&0&1&2&3\\\tabucline[2pt]{-}
			0&0&1&2&3\\\hline
			1&1&2&3&0\\\hline
			2&2&3&0&1\\\hline
			3&3&0&1&2\\\hline 
		\end{tabu} and \begin{tabu}{|c|[2pt]c|c|c|c|}
			\hline
			$\times$&0&1&2&3\\\tabucline[2pt]{-}
			0&0&0&0&0\\\hline
			1&0&1&2&3\\\hline
			2&0&2&0&2\\\hline
			3&0&3&2&1\\\hline
		\end{tabu}
	\end{center}but this shows that $2$ doesn't have any multiplicative inverses: $2\cdot 0=0$, $2\cdot 1=2$, $2\cdot 2=0$ and $2\cdot 3=2$.
	
	But by definition of a field, the only element that has no multiplicative inverse is 0. But clearly $2\neq 0$ (since $1+2\neq 1$), so $\Z/4\Z$ cannot be a field.
	
	This happens precisely because $4$ can be written as $4=nm$ in \textit{three} different ways: $4=4\cdot1=1\cdot 4=2\cdot 2$.
	
	Since this isn't supposed to be a course on field theory, we won't go into much detail on how to prove that $\Z/n\Z$ is a field if, and only if, $n$ is prime.
\end{ex}

\newpage
\subsection{$\R^2$D2}

Let us start this section with the set that will be the focus of most, if not all, of this chapter: $\R^2$.

By definition, $\R^2=\R\times \R$ is the set of ordered pairs of real numbers.

\begin{df}
	We're going to define the \textbf{addition} $A:\R^2\times \R^2\to \R^2$ to be given by $A((x,y),(z,w)):=(x+z,y+w)$ for any $(x,y),(z,w)\in\R^2$.
\end{df}

\begin{prop}
	The addition $A$ we've just defined satisfies the following properties:
	\begin{enumerate}[(i.)]
		\item $A$ is associative;
		\item $A$ is commutative;
		\item $A$ admits an identity element;
		\item $A$ admits inverses.
	\end{enumerate}
\end{prop}
\begin{proof}
	Choose any three elements $(a,b),(c,d),(e,f)\in\R^2$. Then:
	\begin{enumerate}[(i.)]
		\item \begin{align*}
			A\left(A\left((a,b),(c,d)\right),(e,f)\right)&=A((a+c,b+d),(e,f))\\
			&=((a+c)+e,(b+d)+f)\\
			&=(a+(c+e),b+(d+f))\\
			&=A((a,b),(c+e,d+f))=A((a,b),A((c,d),(e,f))),
		\end{align*}so $A$ is associative;
		
		\item \begin{align*}
		A((a,b),(c,d))&=(a+c,b+d)=(c+a,d+b)=A((c,d),(a,b))
		\end{align*}so $A$ is commutative;
		
		\item $A((a,b),(0,0))=(a+0,b+0)=(a,b)=(0+a,0+b)=A((0,0),(a,b))$ so $A$ has identity $(0,0)$;
		
		\item $A((a,b),(-a,-b))=(a-a,b-b)=(0,0)=(-a+a,-b+b)=A((-a,-b),(a,b))$ so $A$ has inverses.
	\end{enumerate}
\end{proof}

\begin{rmk}
	From now on, we're gonna denote $A((a,b),(c,d))$ by $(a,b)+(c,d)$ for any $(a,b),(c,d)\in \R^2$ since, by the preceding proposition, it behaves well-enough like number addition.
\end{rmk}

And it seems there's not much else we can do with $\R^2$ for now.

To proceed with our studies, then, we're gonna need a new approach.

\begin{df}
	We'll denote $E_*$ the \textbf{pointed Euclidean plane}. That is, $E$ is the set of all points in the Euclidean plane, and $*$ is a distinguished point.
\end{df}

\begin{ex}
	For instance,
	
	\[\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\clip(-0.5,-2.) rectangle (7.,6.);
	\begin{scriptsize}
	\draw [fill=ududff] (0.94,0.96) circle (2.5pt);
	\draw[color=ududff] (1.08,1.33) node {$A$};
	\draw [fill=ududff] (5.06,2.92) circle (2.5pt);
	\draw[color=ududff] (5.2,3.29) node {$B$};
	\end{scriptsize}
	\end{tikzpicture}\] we can think of the set $E_A$ (whose elements are all points in the plane, including $A$ and $B$, but distinguishing $A$) and the set $E_B$ (whose elements are all points in the plane, including $A$ and $B$, but distinguishing $B$).
\end{ex}

\begin{df}
	Given a pointed Euclidean plane $E_*$, we define a \textbf{vector in $E_*$} to be any directed segment starting in $*$.
\end{df}

\begin{ex}
	Continuing the above example,
	\[
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\clip(-3.18,-2.52) rectangle (6.,4.75);
	\draw [->] (0.94,0.96) -- (2.2,3.56);
	\draw [->] (0.94,0.96) -- (-2.32,1.66);
	\draw [->] (0.94,0.96) -- (-0.06,-1.7);
	\draw [->] (0.94,0.96) -- (5.5,0.1);
	\draw [->] (0.94,0.96) -- (4.34,2.82);
	\draw [->] (5.06,2.92) -- (4.34,2.82);
	\draw [->] (5.06,2.92) -- (5.5,0.1);
	\draw [->] (5.06,2.92) -- (6,0);
	\begin{scriptsize}
	\draw [fill=ududff] (0.94,0.96) circle (2.5pt);
	\draw[color=ududff] (0.72,1.43) node {$A$};
%	\draw [fill=ududff] (4.34,2.82) circle (2.5pt);
%	\draw [fill=ududff] (5.5,0.1) circle (2.5pt);
%	\draw [fill=ududff] (-0.06,-1.7) circle (2.5pt);
%	\draw [fill=ududff] (-2.32,1.66) circle (2.5pt);
%	\draw [fill=ududff] (2.2,3.56) circle (2.5pt);	
	\draw [fill=ududff] (5.06,2.92) circle (2.5pt);
	\draw[color=black] (1.46,2.47) node {$s$};
	\draw[color=black] (-0.66,1.17) node {$v$};
	\draw[color=black] (0.62,-0.23) node {$w$};
	\draw[color=black] (3.28,0.79) node {$u$};
	\draw[color=black] (2.64,2.15) node {$t$};
	\draw[color=ududff] (5.2,3.29) node {$B$};
	\draw[color=black] (4.8,2.7) node {$x$};
	\draw[color=black] (5,1.5) node {$y$};	
	\draw[color=black] (5.8,1.5) node {$z$};
	\end{scriptsize}
	\end{tikzpicture}
	\]we see that $s,t,u,v,w$ are vectors in $E_A$, but not in $E_B$, whereas $x,y,z$ are vectors in $E_B$, but not in $E_A$.
\end{ex}

\begin{lemma}
	Let $V_A$ denote the set of all vectors in $E_A$. Then $E_A\iso V_A$.
\end{lemma}
\begin{proof}
	Consider the function $t:V_A\to E_A$ that takes any vector $v\in V_A$ to its endpoint $t(v)$, and takes the null vector to $A$.
	
	\begin{itemize}
		\item $t$ is injective:
		
		If $t(v)=t(u)$ for some $v,u\in V_A$, then $v$ and $u$ have the same endpoints. Since they also have the same starting points (by definition), they are equal - hence, $v=u$ and $t$ is injective.
		
		\item $t$ is surjective:
		
		Let $P\in E_A$ be a point. Consider the directed segment $\overrightarrow{AP}$. It is, by definition, a vector in $V_A$ whose endpoint is $P$, and, hence, $t(\overrightarrow{AP})=P$, and so $t$ is surjective.
	\end{itemize}

\begin{df}
	Given any vector $v\in V_A$, we define its \textbf{magnitude} or \textbf{size} or \textbf{norm} to be $\lVert v\rVert:=\lvert AP\rvert$, where $v=\overrightarrow{AP}$.
\end{df}

Since $t$ is both injective and surjective, it is a bijection. This proves the result.
\end{proof}

Let now $V^{r,s}_A$ be the following set: Take any two perpendicular lines $r$ and $s$ through $A$. Then, $V^{r,s}_A$ will be the set $(V_A\cap r)\times (V_A\cap s)$ - that is, the set of all pairs vectors of $E_A$ such that the first vector lies entirely in $r$ and the second vector lies entirely in $s$.

\begin{lemma}
	For any $E_A$, we have that $V_A\iso V^{r,s}_A$.
\end{lemma}
\begin{proof}
	Let $f:V_A\to V^{r,s}_A$ be the following function: For any vector $\overrightarrow{AP}$ in $V_A$, we can consider the parallel to $r$ through $P$, $r_P$ and the parallel to $s$ through $P$, $s_P$.
	
	Since $r\parallel r_P$, we have that $r\cap r_P=\varnothing$, and similarly we have that $s\cap s_P=\varnothing$.
	
	But since $r\nparallel s_P$ and they are both lines, we have that $r\cap s_P$ is a single point - let's call it $P_r$. Similarly, we see that $s\cap r_P$ is a single point - let's call it $P_s$.
	
	But now, $P_r\in r$, by definition. So $\overrightarrow{AP_r}$ is a vector which lies entirely in $r$. Similarly, $P_s\in s$ so $\overrightarrow{AP_s}$ is a vector which lies entirely in $s$.
	
	We then define $f(\overrightarrow{AP}):=(\overrightarrow{AP_r},\overrightarrow{AP_s})$.
	\[\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\clip(-0.86,0.62) rectangle (8.56,7.02);
	\draw [->] (0.5,3.04) -- (7.5,4.08);
	\draw [->] (0.5,3.04) -- (1.0909987984666145,5.574475616501058);
	\draw [->] (0.5,3.04) -- (6.909001201533385,1.5455243834989414);
	\draw [domain=-0.86:8.56] plot(\x,{(--28.1568-2.08*\x)/8.92});
	\draw [domain=-0.86:8.56] plot(\x,{(--1.8632--8.92*\x)/2.08});
	\draw [dotted,domain=-0.86:8.56] plot(\x,{(-58.4136--8.92*\x)/2.08});
	\draw [dotted,domain=-0.86:8.56] plot(\x,{(-51.9936--2.08*\x)/-8.92});
	\begin{scriptsize}
	\draw [fill=ududff] (0.5,3.04) circle (2.5pt);
	\draw[color=ududff] (0.76,2.71) node {$A$};
	\draw[color=black] (-5.62,4.33) node {$r$};
	\draw[color=black] (2.16,9.29) node {$s$};
	\draw [fill=ududff] (7.5,4.08) circle (2.5pt);
	\draw[color=ududff] (7.76,4.37) node {$P$};
	\draw [fill=uuuuuu] (1.0909987984666145,5.574475616501058) circle (2.0pt);
	\draw[color=uuuuuu] (1.39,5.92) node {$P_s$};
	\draw [fill=uuuuuu] (6.909001201533385,1.5455243834989414) circle (2.0pt);
	\draw[color=uuuuuu] (7.29,1.94) node {$P_r$};
	\draw[color=uuuuuu] (3.29,2) node {$r$};
	\draw[color=uuuuuu] (0.5,4.5) node {$s$};
	\end{scriptsize}
	\end{tikzpicture}\]
	
	\begin{itemize}
		\item $f$ is injective:
		
		Let $v,u$ be vectors in $V_A$ such that $f(v)=f(u)$. This means that $v=\overrightarrow{AP}$ and $u=\overrightarrow{AQ}$ for some uniquely determined points $P,Q$ in $E_A$. Now, by definition, $f(v)=f(\overrightarrow{AP})=(\overrightarrow{AP_r},\overrightarrow{AP_s})$ and $f(u)=f(\overrightarrow{AQ})=(\overrightarrow{AQ_r},\overrightarrow{AQ_s})$.
		
		But $f(v)=f(u)$ implies $\overrightarrow{AP_r}=\overrightarrow{AQ_r}$ and $\overrightarrow{AP_s}=\overrightarrow{AQ_s}$, by definition of set product.
		
		But then this means that $P_r=Q_r$ and $P_s=Q_s$.
		
		This means that both $P$ and $Q$ belong to both of $r_P$ and $s_P$. But $r_P$ and $s_P$ are lines, so their intersection has, at most, one point. This means that $P=Q$, and so $f$ is injective.
		
		\item $f$ is surjective:
		
		Take any $(v,u)\in V^{r,s}_A$. Then, there are uniquely determined $P,Q\in E_A$ such that $v=\overrightarrow{AP}$ and $u=\overrightarrow{AQ}$.
		
		Now, consider the lines $r_Q$ and $s_P$ defined, respectively, to be the lines parallel to $r$ through $Q$, and parallel to $s$ through $P$.
		
		Since $s_P\parallel s\nparallel r\parallel r_Q$, we see that $s_P\nparallel r_Q$ and so $s_P\cap r_Q$ is a single point - T.
		
		It is, then, easy tos see that $f(\overrightarrow{AT})=(v,u)$, so $f$ is surjective.
	\end{itemize}

It follows then that $f$ is indeed a bijection, which ends the proof.
\end{proof}

Finally, we can prove the result we've all been waiting for:

\begin{theorem}
	Let $E_A$ be a pointed Euclidean plane. Then $V^{r,s}_A\iso \R^2$.
\end{theorem}
\begin{proof}
	Take any two points $R\in r$ and $P\in p$, respectively, both different from $A$.
	
	Now, given any $v\in r$ a vector lying entirely in $r$, we say that $v$ is \textbf{positive} if the endpoint of $v$ lies in the same side of the semiplane defined by $s$ as $R$, \textbf{zero} if $v=\overrightarrow{AA}$ the null vector, and \textbf{negative} otherwise.
	
	Similarly, given $u\in s$ a vector lying entirely in $s$, we say that $u$ is \textbf{positive} if the endpoint of $u$ lies in the same side of the samiplane defined by $r$ as $S$, \textbf{zero} if $u=\overrightarrow{AA}$ the null vector, and \textbf{negative} otherwise.
	
	This can be seen as a function $\mathrm{sgn}:V^{r,s}_A\to\{0,1,-1\}$.
	
	That said, we're gonna define a function $f:V^{r,s}_A\to \R^2$ by putting $$f(v,u):=(\mathrm{sgn}(v)\lVert v\rVert,\mathrm{sgn}(u)\lVert u\rVert).$$ We claim that $f$ is the bijection we're looking for.
	
	\begin{itemize}
		\item $f$ is injective:
		
		Let $(v,u),(v',u')\in V^{r,s}_A$ be two elements such that $f(v,u)=f(v',u')$. This means that $\mathrm{sgn}(v)\norm{v}=\mathrm{sgn}(v')\norm{v'}$ and $\mathrm{sgn}(u)\norm{u}=\mathrm{sgn}(u')\norm{u'}$. Since $\norm-$ is always a non-negative number, we see that $\mathrm{sgn}(v)\norm{v}=\mathrm{sgn}(v')\norm{v'}$ if, and only if, $\mathrm{sgn}(v)=\mathrm{sgn}(v')$, and similarly for $\mathrm{sgn}(u)=\mathrm{sgn}(u')$.
		
		Now this implies, together with the equations $\mathrm{sgn}(v)\norm{v}=\mathrm{sgn}(v')\norm{v'}$ and $\mathrm{sgn}(u)\norm{u}=\mathrm{sgn}(u')\norm{u'}$, that $\norm{v}=\norm{v'}$ and $\norm{u}=\norm{u'}$. But since the norm is entirely determined by the endpoint (since the starting point is fixed, being $A$), we see that $\norm{v}=\norm{v'}$ if, and only if, $v$ and $v'$ have the same endpoints. Similarly, we see that $u$ and $u'$ have the same endpoints.
		
		Finally, since $v$ and $v'$ have the same endpoints they must be equal, and the same goes for $u$ and $u'$.
		
		It follows that $(v,u)=(v',u')$ and so $f$ is indeed injective.
		
		\item $f$ is surjective:
		
		Take any $(x,y)\in \R^2$. Now draw two circles centered in $A$: one with radius $\abs{x}$ and one with radius $\abs{y}$. Call these circles $C_x$ and $C_y$, resp.
		
		Since $C_x$ is a circle and $r$ is a line which contains a point inside the circle, we know that $C_x\cap r$ is precisely two points - $R_1,R_2$. Similarly, we know that $C_y\cap s$ is precisely two points - $S_1,S_2$.
		
		Now, we take $\overrightarrow{x}$ to be the vector ending at either $R_1$ or $R_2$ such that $\mathrm{sgn}(\overrightarrow{x})=\mathrm{sgn}(x)$. Similarly, we define $\overrightarrow{y}$ to be the vector ending at either $S_1$ or $S_2$ such that $\mathrm{sgn}(\overrightarrow{y})=\mathrm{sgn}(y)$.
		
		Now, by construction, $(\overrightarrow{x},\overrightarrow{y})\in V^{r,s}_A$ is such that $\mathrm{sgn}(\overrightarrow{x})\norm{\overrightarrow{x}}=x$ and $\mathrm{sgn}(\overrightarrow{y})\norm{\overrightarrow{y}}=y$, so $f(\overrightarrow{x},\overrightarrow{y})=(x,y)$ and we see that $f$ is surjective.
	\end{itemize}

Finally, we can conclude that $f$ is the bijection we were looking for. This ends the proof of the theorem.
\end{proof}
\begin{cor}
	By composition of isomorphisms we have:
	
	\[E_A\iso V_A\iso V^{r,s}_A\iso\R^2,\]so the elements of $\R^2$ can be thought of as vectors in the pointed Euclidean plane $E_A$.
\end{cor}

\begin{lemma}
	In the above bijection $f:V^{r,s}_A\to\R^2$, the image of $r$ is the set
	\[\R\times\{0\}:=\{(x,y)\in \R^2\mid y=0\}\]and the image of $s$ is the set
	\[\{0\}\times \R:=\{(x,y)\in\R^2\mid x=0\}.\]
\end{lemma}
\begin{proof}
	Take $(v,\overrightarrow{AA})\in V^{r,s}_A$ any vector lying entirely on $r$. Then, by definition,
	\[f(v,\overrightarrow{AA})=(\mathrm{sgn}(v)\norm{v},\mathrm{sgn}(\overrightarrow{AA})\norm{\overrightarrow{AA}}),\]but both $\mathrm{sgn}(\overrightarrow{AA})$ and $\norm{\overrightarrow{AA}}$ equal 0. So $f(v,\overrightarrow{AA})=(\mathrm{sgn}(v)\norm{v},0)$.
	
	This shows that $f(r)\subseteq \R\times\{0\}$.
	
	Conversely, take any $(x,0)\in \R\times\{0\}$. It is clearly the image of some vector pair $(\overrightarrow{x},\overrightarrow{AA})$, and so $\R\times\{0\}\subseteq f(r)$.
	
	We can argue analogously and show that $f(s)=\{0\}\times\R$.
	
	This ends the proof.
\end{proof}

\begin{df}
	We will denote the sets $\R\times\{0\}$ and $\{0\}\times \R$ the \textbf{$X$-axis} and the \textbf{$Y$-axis}, respectively.
\end{df}

\begin{rmk}
	Notice that, as sets, both the $X$- and the $Y$-axis are in bijection with $\R$, and with a line (the $X$-axis is in bijection with $r$ and the $Y$-axis is in bijection with $s$).
	
	Therefore, $\R$ is in bijection with a line. So it makes sense to think of the set of real numbers as a \textit{line}, and call it the \textbf{real line}.
\end{rmk}

Finally, to end this section, one last result:

\begin{lemma}
	Take $t\in E_A$ any line through $A$. Then there is some point $(x_t,y_t)\in \R^2$ such that $f(t)=\{(x,y)\in\R^2\mid x=\lambda x_t \mbox{ and }y=\lambda y_t,\mbox{ for some }\lambda\in\R\}$.
\end{lemma}
\begin{proof}
	Let $\R(x_t,y_t)$ denote the set $\{(x,y)\in\R^2\mid x=\lambda x_t \mbox{ and }y=\lambda y_t,\mbox{ for some }\lambda\in\R\}$.
	
	Draw $\mc C(A,1)$ the circle of radius $1$ centered at $A$. Since $t$ contains a point in the inside of the circle, $t\cap \mc C(A,1)=2$. Choose any of those two points and call it $P$.
	
	Now, let us define $(x_t,y_t):=f(\overrightarrow{AP_r},\overrightarrow{AP_s})$.
	
	Now take any point $T\in t$, and look at the vectors $\overrightarrow{AT_r}$ and $\overrightarrow{AT_s}$. Now, by using similar triangles we see that 
	\[\dfrac{\mathrm{sgn}(\overrightarrow{AT_r})\norm{\overrightarrow{AT_r}}}{x_t}=\dfrac{\mathrm{sgn}(\overrightarrow{AT_s})\norm{\overrightarrow{AT_s}}}{y_t}=\frac{\mathrm{sgn(\overrightarrow{AT})\norm{\overrightarrow{AT}}}}{\mathrm{sgn}(\overrightarrow{AP})\norm{\overrightarrow{AP}}}\]
	\[\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-1.2799999999999998,
	xmax=6.560000000000002,
	ymin=-1.3500000000000016,
	ymax=4.3500000000000005,
	yticklabels={},	
	xticklabels={},	
	xlabel=$r$,
	ylabel=$s$,]
	\clip(-1.28,-1.35) rectangle (6.56,4.35);
	\draw [domain=-1.28:6.56] plot(\x,{(-0.--1.67*\x)/3.04});
	\draw [dotted] (5.261710485848469,-1.35) -- (5.261710485848469,4.35);
	\draw [dotted] (3.04,-1.35) -- (3.04,4.35);
	\draw [dotted,domain=-1.28:6.56] plot(\x,{(-1.67-0.*\x)/-1.});
	\draw [dotted,domain=-1.28:6.56] plot(\x,{(-2.890479115581231-0.*\x)/-1.});
	\draw [->] (0.,0.) -- (3.04,0.);
	\draw [->] (0.,0.) -- (5.261710485848469,0.);
	\draw [->] (0.,0.) -- (0.,1.67);
	\draw [->] (0.,0.) -- (0.,2.890479115581231);
	\draw [->] (0.,0.) -- (3.04,1.67);
	\draw [->] (0.,0.) -- (5.261710485848469,2.890479115581231);
	\begin{scriptsize}
	\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	\draw[color=uuuuuu] (0.22,-0.28) node {$A$};
	\draw [fill=ududff] (3.04,1.67) circle (2.5pt);
	\draw[color=ududff] (3.24,1.48) node {$P$};
	\draw [fill=xdxdff] (5.261710485848469,2.890479115581231) circle (2.5pt);
	\draw[color=xdxdff] (5.54,2.66) node {$T$};
	\draw [fill=uuuuuu] (3.04,0.) circle (2.0pt);
	\draw[color=uuuuuu] (3.27,-0.4) node {$P_r$};
	\draw [fill=uuuuuu] (0.,1.67) circle (2.0pt);
	\draw[color=uuuuuu] (-0.35,1.9) node {$P_s$};
	\draw [fill=uuuuuu] (0.,2.890479115581231) circle (2.0pt);
	\draw[color=uuuuuu] (-0.33,3.3) node {$T_s$};
	\draw [fill=uuuuuu] (5.261710485848469,0.) circle (2.0pt);
	\draw[color=uuuuuu] (5.51,-0.4) node {$T_r$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]
	
	So we pick $\lambda_T$ to be any of those (since they're all equal).
	
	Clearly, then, by definition of $\lambda_T$, we have that $$f(\overrightarrow{AT_r},\overrightarrow{AT_s})=(\mathrm{sgn}(\overrightarrow{AT_r})\norm{AT_r},\mathrm{sgn}(\overrightarrow{AT_s})\norm{AT_s})=(\lambda_Tx_t,\lambda_Ty_t).$$
	
	Since this holds true for any $T\in t$, we have that $f(t)\subseteq \R(x_t,y_t)$.
	
	\bigskip
	Conversely, take some $(x_t,y_t)\in\R^2$ fixed.
	
	Now, let $P$ be the inverse image of $(x_t,y_t)$ under $f$ (it can be done uniquely since $f$ is bijective), and let $t$ be the line through $A$ that also passes through $P$ (it is also unique, since a line is uniquely determined by two distinct points).
	
	By the same reasoning as before, if we now take the inverse image of $(\lambda x_t,\lambda y_t)$ for any real $\lambda\in \R$ to be some $P^\lambda$, we see that the triangles $AP_rP_s$ and $AP^\lambda_rP^\lambda_s$ are similar, since 
	\[\frac{\overline{AP_r}}{\overline{AP^\lambda_r}}=\frac{\overline{AP_s}}{\overline{AP^\lambda_s}}=\frac{\overline{P_sP_r}}{\overline{P^\lambda_sP^\lambda_r}}=\lambda,\]so $P^\lambda$ lies in $t$.
	
	This shows that $\R(x_t,y_t)\subseteq f(t)$.
	
	Finally, we can conclude that $f(t)=\R(x_t,y_t)$, which ends the proof.
\end{proof}

This shows that the subsets of $\R^2$ where each element is obtained simply by multiplying both coordinates of a fixed vector by a fixed number are \textbf{lines}.

With this we can finally define:

\begin{df}
	We're gonna define the \textbf{multiplication of a point in $\R^2$ by a real number} to be as such:
	\[\lambda(x,y):=(\lambda x,\lambda y)\]for any $\lambda\in \R$ and $(x,y)\in\R^2$.
\end{df}

\begin{rmk}
	By the preceding theorem, this multiplication is simply scaling the vector $(x,y)\in\R^2$ in a straight line to the center to become the same size as $\lambda$ (notice that if $\lambda <0$, this means that the vector will also change sign).
	
	For this reason, this operation is more often than not called the \textbf{scalar multiplication} or \textbf{multiplication by a scalar}.
\end{rmk}

\begin{cor}
	Every line through the point $(0,0)$ in $\R^2$ is just the subset of $\R^2$ formed by all scalar multiples of a fixed vector.
\end{cor}

And to wrap things up, some properties of scalar multiplication:

\begin{prop}
	Let $v,u\in \R^2$ and $\lambda,\mu\in \R$. Then, the following hold:
	\begin{enumerate}[(1)]
		\item (Associative) $(\mu\lambda)v=\mu(\lambda v)$;
		\item (Commutative) $\lambda v=v\lambda$;
		\item (Identity element) There is some $\epsilon\in \R$ such that $\epsilon v=v\epsilon=v$;
		\item (Scalar product distributes over vector sum) $\lambda(v+u)=\lambda v+\lambda u$ and $(v+u)\mu=v\mu+u\mu$;
		\item (Scalar product distributes over scalar sum) $(\mu+\lambda)v=\mu v+\lambda v$.
	\end{enumerate}
\end{prop}
\begin{proof}
	Write, once and for all, $v=(v_1,v_2)$ and $u=(u_1,u_2)$.
	\begin{enumerate}[(1)]
		\item This follows from the fact that real number multiplication is associative:
		\begin{align*}
			(\mu\lambda)v&=(\mu\lambda)(v_1,v_2)\\
			&=((\mu\lambda)v_1,(\mu\lambda)v_2)\\
			&=(\mu(\lambda v_1),\mu(\lambda v_2))\\
			&=\mu(\lambda v_1,\lambda v_2)\\
			&=\mu(\lambda (v_1,v_2))=\mu(\lambda v)
		\end{align*}which holds true since $\mu,\lambda,v_1,v_2$ are real numbers.
		
		\item This follows directly from the fact that real number multiplication is commutative:
		\begin{align*}
			\lambda v&=\lambda(v_1,v_2)\\
			&=(\lambda v_1,\lambda v_2)\\
			&=(v_1\lambda,v_2\lambda)\\
			&=(v_1,v_2)\lambda=v\lambda
		\end{align*}which holds true since $\lambda,v_1,v_2$ are real numbers.
		
		\item This follows from the fact that real number multiplication has an identity:
		\[1v=1(v_1,v_2)=(1v_1,1v_2)=(v_1,v_2)=v.\]
		
		\item This follows from the fact that real number multiplication distributes over real number sums:
		
		\begin{align*}
			\lambda(v+u)&=\lambda((v_1,v_2)+(u_1,u_2))\\
			&=\lambda(v_1+u_1,v_2+u_2)\\
			&=(\lambda(v_1+u_1),\lambda(v_2+u_2))\\
			&=(\lambda v_1+\lambda u_1,\lambda v_2+\lambda u_2)\\
			&=(\lambda v_1,\lambda v_2)+(\lambda u_1+\lambda u2)\\
			&=\lambda (v_1,v_2)+\lambda(u_1,u_2)=\lambda v+\lambda u
		\end{align*}which holds true since $\lambda, v_1,v_2,u_1,u_2$ are real numbers.
		
		Distribution on the right-side holds since we've already proven (2), so $\lambda(v+u)=(v+u)\lambda$ and $\lambda v+\lambda u=v\lambda+u\lambda$.
		
		\item Similar to (4):
		
		\begin{align*}
			(\mu+\lambda)v&=(\mu+\lambda)(v_1,v_2)\\
			&=((\mu+\lambda)v_1,(\mu+\lambda) v_2)\\
			&=(\mu v_1+\lambda v_1,\mu v_2+\lambda v_2)\\
			&=(\mu v_1,\mu v_2)+(\lambda v_1,\lambda v_2)\\
			&=\mu(v_1,v_2)+\lambda(v_1,v_2)=\mu v+\lambda v
		\end{align*}which holds true since $\mu,\lambda,v_1,v_2$ are real numbers.
		
		Distribution on the right follows, again, from (2).
	\end{enumerate}
\end{proof}

So from here onwards, vectors, points and elements of $\R^2$ will be taken as being the same thing, since everything we did in this section was to show that $\R^2$ is a great model of the pointed Euclidean plane.

To end this section, then, let us define:

\begin{df}
	We will call the point $(0,0)\in \R^2$ of the \textbf{origin} or the \textbf{zero} point of $\R^2$, and denote it simply by $0$ when there's no ambiguity.
\end{df}

\begin{prop}
	For any vector $v\in \R^2$ we have that $0v=0$.
\end{prop}
\begin{proof}
	Take any $v\in \R^2$. Now, by what we've already shown, we have:
	\[0v=(0+0)v=0v+0v\]and so, by subtracting $0v$ on both sides, we get $0v=0$, just as we wanted.
\end{proof}

\newpage
\section{The realest plane of vectors}
\subsection{Getting into shape}

So, now that we know what $\R^2$ is, we're gonna study how functions interact with it.

\begin{ex}
	Consider the two following functions $f,g:\R^2\to\R^2$ given by $f(x,y):=(x,x^2)$ and $g(x,y):=(2x-y,x+y)$ respectively.
	
	It's not hard to see that the first function takes the line $y=0$ (that is, the $X$-axis) into the parabola $y=x^2$. In particular, notice that this function doesn't preserve \textit{any} lines through zero.
	
	Conversely, we claim that $g$ does preserve \textit{every} line through zero. To see this, we will show that $g$ preserves scalar multiplication. That suffices, by definition of lines through zero:
	
	Indeed, if $r$ is the line of all scalar multiples of a vector $v$, then any vector $w\in r$ is of the form $\lambda v$ for some $\lambda \in \R$. Let $g(v)=v'\in\R^2$ be the image of $v$ under $g$. If we can prove that $g$ preserves scalar multiplication, then $g(w)=g(\lambda v)=\lambda g(v)=\lambda v'$, and so every point in the line of the multiples of $v$ goes to the line of the multiples of $v'$.
	
	So it remains to show that $g$ preserves scalar multiplication. But that's easy: Take any $(x,y)\in \R^2$ and any $\lambda\in\R$. Then:
	\begin{align*}
		g(\lambda(x,y))&=g(\lambda x,\lambda y)\\
		&=(2(\lambda x)-(\lambda y),(\lambda x)+(\lambda y))\\
		&=(2\lambda x-\lambda y,\lambda x+\lambda y)\\
		&=(\lambda(2x-y),\lambda(x+y))\\
		&=\lambda(2x-y,x+y)=\lambda g(x,y)
	\end{align*}and so we see that $g$ preserves scalar multiplication - and, therefore, preserves any line through the origin.
	
	\bigskip
	There's only one slight issue: Indeed, for a function to preserve lines it suffices for it to preserve scalar multiplication, but there's no telling what happens to the origin during this process. Think about it: Just because every line becomes another line after applying a certain function, that doesn't mean that every point must stand still.
	
	For instance, consider the function $h:\R^2\to\R^2$ defined by $h(x,y):=(x,x)$ if $y=0$ and $h(x,y):=(y,y)$ otherwise. Then:
	\[h(\lambda(x,0))=h(\lambda x,\lambda\cdot0)=(\lambda x,\lambda x)=\lambda(x,x)=\lambda h(x,0)\]
	\[h(\lambda(x,y))=h(\lambda x,\lambda y)=(\lambda y,\lambda y)=\lambda(y,y)=\lambda h(x,y)\]and so $h$ preserves scalar multiplication - and hence preserves lines. However, as we'll see further ahead, this function is still not good enough - for one simple reason: It might preserve lines, but it doesn't preserve parallelograms.
\end{ex}

\begin{prop}
	Given any two vectors $v,u\in \R^2$, consider the following construction:
	\begin{enumerate}
		\item Draw the two vectors $v,u\in \R^2$;
		\item Take $r_v$ the line parallel to $v$ and containing the endpoint of $u$ (there's a unique such line);
		\item Take $r_u$ the line parallel to $u$ and containing the endpoint of $v$ (there's a unique such line).
	\end{enumerate}
\[\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\begin{axis}[
x=1.0cm,y=1.0cm,
axis lines=middle,
xmin=-0.9199999999999992,
xmax=9.040000000000004,
ymin=-1.0600000000000016,
ymax=5.980000000000001,
yticklabels={},	
xticklabels={},]
\clip(-0.92,-1.06) rectangle (9.04,5.98);
\draw [->] (0.,0.) -- (2.,4.);
\draw [->] (0.,0.) -- (6.,1.);
\draw [dotted,domain=-0.92:9.04] plot(\x,{(-22.--4.*\x)/2.});
\draw [dotted,domain=-0.92:9.04] plot(\x,{(--22.--1.*\x)/6.});
\draw [->,dash pattern=on 3pt off 3pt] (0.,0.) -- (8.,5.);
\begin{scriptsize}
\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
\draw [fill=ududff] (2.,4.) circle (2.5pt);
\draw[color=black] (0.86,2.37) node {$v$};
\draw [fill=ududff] (6.,1.) circle (2.5pt);
\draw[color=black] (3.5,0.4) node {$u$};
\draw [fill=uuuuuu] (8.,5.) circle (2.0pt);
\draw[color=black] (3.86,4.8) node {$r_u$};

\draw[color=black] (3.86,2.81) node {$v+u$};

\draw[color=black] (7.5,2.81) node {$r_v$};
\end{scriptsize}
\end{axis}
\end{tikzpicture}\]

	Then, $v+u=r_u\cap r_v$.
\end{prop}

\begin{proof}
	First, how do we know that $r_v\cap r_u$ is non-empty? Well, by construction, $r_v\parallel v$ and $r_u\parallel u$. But $v$ and $u$ meet at 0, so $r_v$ and $r_u$ must also meet.
	
	Second, since $r_v$ and $r_u$ are lines, they have, at most, a single meet point. Since we already know they have a non-empty intersection, then we know that they meet at a single point.
	
	Call this point $P\in \R^2$.
	
Consider now the following triangles:

\[\definecolor{zzttqq}{rgb}{0.6,0.2,0.}
\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\begin{axis}[
x=1.0cm,y=1.0cm,
axis lines=middle,
xmin=-0.9399999999999992,
xmax=9.020000000000005,
ymin=-0.9400000000000015,
ymax=5.980000000000001,
yticklabels={},	
xticklabels={},]
\clip(-0.94,-0.94) rectangle (9.02,5.98);
\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (0.,0.) -- (2.,0.) -- (2.,4.) -- cycle;
\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (6.,1.) -- (8.,1.) -- (8.,5.) -- cycle;
\draw [->] (0.,0.) -- (2.,4.);
\draw [->] (0.,0.) -- (6.,1.);
\draw [dotted,domain=-0.94:9.02] plot(\x,{(-22.--4.*\x)/2.});
\draw [dotted,domain=-0.94:9.02] plot(\x,{(--22.--1.*\x)/6.});
\draw [->,dash pattern=on 3pt off 3pt] (0.,0.) -- (8.,5.);
\draw [dotted] (6.,-0.94) -- (6.,5.98);
\draw [dotted] (8.,-0.94) -- (8.,5.98);
\draw [dotted] (2.,-0.94) -- (2.,5.98);
\draw [color=zzttqq] (0.,0.)-- (2.,0.);
\draw [color=zzttqq] (2.,0.)-- (2.,4.);
\draw [color=zzttqq] (2.,4.)-- (0.,0.);
\draw [dotted] (-1,1) -- (9,1);
\draw [color=zzttqq] (6.,1.)-- (8.,1.);
\draw [color=zzttqq] (8.,1.)-- (8.,5.);
\draw [color=zzttqq] (8.,5.)-- (6.,1.);
\begin{scriptsize}
\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
\draw [fill=ududff] (2.,4.) circle (2.5pt);
\draw[color=black] (0.86,2.37) node {$v$};
\draw [fill=ududff] (6.,1.) circle (2.5pt);
\draw[color=black] (3.5,0.2) node {$u$};
\draw[color=black] (9.39,8.32) node {$r_v$};
\draw[color=black] (-3.51,2.98) node {$r_u$};
\draw [fill=uuuuuu] (8.,5.) circle (2.0pt);
\draw[color=uuuuuu] (8.26,5.29) node {$P$};
\draw [fill=uuuuuu] (2.,0.) circle (2.0pt);
\draw[color=uuuuuu] (2.25,-0.25) node {$v_1$};
\draw [fill=uuuuuu] (8.,1.) circle (2.0pt);
\draw[color=uuuuuu] (8.2,1.33) node {$u'$};
\draw [fill=uuuuuu] (6.,0.) circle (2.0pt);
\draw[color=uuuuuu] (6.23,-0.30) node {$u_1$};
\draw [fill=uuuuuu] (8.,0.) circle (2.0pt);
\draw[color=uuuuuu] (8.27,-0.28) node {$P_1$};
\end{scriptsize}
\end{axis}
\end{tikzpicture}\]

First, notice that segments $u_1P_1$ and $uu'$ are congruent (since $uu'P_1u_1$ is a rectangle, by construction).

Now we prove that the segments $0v_1$ and $uu'$ are congruent. To do that, we use the triangles $0v_1v$ and $uu'P$. We claim they are congruent: Indeed, both are right triangles (on $v_1$ and $u'$, respectively), the angles $v0v_1$ and $Puu'$ are the same (since the segment $Pu$ is parallel to $v0$, by construction) and the segments $0v$ and $Pu$ are congruent (once again, by construction).

This implies that the triangles are congruent and, therefore, $vv_1$ is congruent to $Pu'$, and $0v_1$ is congruent to $uu'$.

Now, it's easy to see that the segment $0P_1$ is just the concatenation of the segments $0u_1$ and $u_1P_1$, so the length of $0P_1$ is the sum of the lengths of $0u_1$ and $u_1P_1$.

But the length of $0u_1$ is, by definition, $u_1$, and the length of $u_1P_1$ is the length of $uu'$, which, as we've shown, is the length of $0v_1$ - which, once again by definition, is just $v_1$.

So the length of $0P_1$ - which is just $P_1$ - is the sum of $u_1$ and $v_1$ - that is, $P_1=u_1+v_1$.

\bigskip
We can proceed analogously on the $Y$-axis and show that $P_2=u_2+v_2$, which finally shows that $P=v+u$, just as stated.
\end{proof}

\begin{cor}
	Any line $r\subseteq \R^2$ is of the form $r=v+r'$ where $v$ is a fixed vector and $r'$ is a line through zero which is parallel to $r$.
\end{cor}
\begin{proof}
	There are two possible cases:
	\begin{enumerate}
		\item \underline{$r\cap Y\neq\varnothing$}.
		
		In this case, take $v:=r\cap Y$ and draw $r'\parallel r$ through $0$.
		
		Now given any $w\in r$, we can take a line parallel to $v$ through $w$, and since $v$ cuts $r'$, this new line will also cut $r'$. Call this new point $u$.
		
		\[\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
		\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=.4cm,y=.4cm]
		\begin{axis}[
		x=.7cm,y=.7cm,
		axis lines=middle,
		xmin=-0.8999999999999992,
		xmax=7.0000000000000036,
		ymin=-1.0000000000000016,
		ymax=7.000000000000002,
		yticklabels={},	
		xticklabels={},]
		\clip(-0.9,-1.) rectangle (7.,7.);
		\draw [domain=-0.9:7.] plot(\x,{(--34.2344--3.*\x)/8.84});
		\draw [domain=-0.9:7.] plot(\x,{(-0.--3.*\x)/8.84});
		\draw [->] (0.,0.) -- (0.,3.872669683257919);
		\draw [->] (0.,0.) -- (6.272729409172696,6.001424007637792);
		\draw [dash pattern=on 2pt off 2pt] (6.272729409172696,-1.) -- (6.272729409172696,7.);
		\draw [->] (0.,0.) -- (6.272729409172696,2.1287543243798743);
		\begin{scriptsize}
		\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
		\draw [fill=uuuuuu] (0.,3.872669683257919) circle (2.0pt);
		\draw[color=black] (-0.32,1.81) node {$v$};
		\draw [fill=xdxdff] (6.272729409172696,6.001424007637792) circle (2.5pt);
		\draw[color=black] (3.02,3.25) node {$w$};
		\draw [fill=uuuuuu] (6.272729409172696,2.1287543243798743) circle (2.0pt);		
		\draw[color=black] (3.02,1.5) node {$u$};
		\draw[color=black] (6.7,5.8) node {$r$};
		\draw[color=black] (6.7,1.8) node {$r'$};
		\end{scriptsize}
		\end{axis}
		\end{tikzpicture}\]Clearly, then, by construction, we see that $w=v+u$. 
		
		\item \underline{$r\cap Y=\varnothing$}.
		
		In this case, just take $v:=r\cap X$. We know this point exists because $X\perp Y$ and $r\parallel Y$ together imply $r\perp X$, and so $r\cap X\neq\varnothing$.
		
		Now we can just proceed as in the previous case. We'll skip the details of the proof.
	\end{enumerate}

This shows that every point in $r$ can be written as $v+r'$, which ends the proof.
\end{proof}
\begin{cor}
	Every line in $\R^2$ is of the form $v+\R u$ for some $v,u\in \R^2$.
\end{cor}

\begin{ex}
	Consider the vectors $v=(2,1)$, $u=(1,3)$.  Now, we can construct various lines: $\R v$, $\R u$, $v+\R u$, $u+\R v$ and $\R(v+u)$, for instance.
	
	By the previous discussion, we know that $v+\R u$ and $u+\R v$ don't pass through zero, and all the other lines do.
	
	Here's a sketch of these lines:
	\[\definecolor{yqqqyq}{rgb}{0.5019607843137255,0.,0.5019607843137255}
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\definecolor{qqqqff}{rgb}{0.,0.,1.}
	\definecolor{ffqqqq}{rgb}{1.,0.,0.}
	\definecolor{ffdxqq}{rgb}{1.,0.8431372549019608,0.}
	\definecolor{qqwuqq}{rgb}{0.,0.39215686274509803,0.}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-0.9177598207939823,
	xmax=4.01551855273497,
	ymin=-0.9650502335857208,
	ymax=5.001090829717888,
	yticklabels={},	
	xticklabels={},]
	\clip(-0.9177598207939823,-0.9650502335857208) rectangle (4.01551855273497,5.001090829717888);
	\draw [->] (0.,0.) -- (1.,3.) node [midway, above, sloped] {$u$};
	\draw [->] (0.,0.) -- (2.,1.) node [midway, above, sloped] {$v$};
	\draw [color=green,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(-0.--1.*\x)/2.});
	\draw [color=yellow,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(-0.--3.*\x)/1.});
	\draw [color=red,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(--5.--1.*\x)/2.});
	\draw [color=blue,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(-5.--3.*\x)/1.});
	\draw [->] (0.,0.) -- (3.,4.) node [midway, above, sloped] {$v+u$};
	\draw [color=purple,domain=-0.9177598207939823:4.01551855273497] plot(\x,{(-0.--4.*\x)/3.});
	\begin{scriptsize}
	\draw [fill=xdxdff] (0.,0.) circle (2.5pt);
	\draw [fill=ududff] (2.,1.) circle (2.5pt);
	\draw [fill=ududff] (1.,3.) circle (2.5pt);
	\draw [fill=uuuuuu] (3.,4.) circle (2.0pt);	
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]with the following coloring: {\color{green}{$\R v$}}, {\color{yellow}{$\R u$}}, {\color{red}{$\R v+u$}}, {\color{blue}{$v+\R u$}} and {\color{purple}{$\R(v+u)$}}.
\end{ex}

\begin{ex}
	Now let's resume the discussion from the previous example: Just to refresh, we're considering the function $h:\R^2\to \R^2$ defined by
	\[h(x,y):=\begin{cases}
	(x,x),&\mbox{ if }y=0\\
	(y,y),&\mbox{ otherwise}
	\end{cases}\]and we've seen that this function does preserve lines through 0. But what does it do with parallelograms?
	
	For instance, take the line points $v=(2,4)$ and $u=(2,-4)$. Then, by what we've already shown, the point $v+u=(4,0)$ is precisely the remaining vertex of the parallelogram determined by $v$ and $u$. But look what happens if we try applying the same rule to $h$:
	\[h(v+u)=h(4,0)=(4,4)\]but
	\[h(v)+h(u)=h(2,4)+h(2,-4)=(4,4)+(-4,-4)=(0,0)\]so $h$ didn't preserve our parallelogram!
	
	\[\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-4.948163022910453,
	xmax=4.998779844455295,
	ymin=-4.9803933515895045,
	ymax=4.949458548650205,
	yticklabels={},	
	xticklabels={},]
	\clip(-4.948163022910453,-4.9803933515895045) rectangle (4.998779844455295,4.949458548650205);
	\draw [->] (0.,0.) -- (2.,-4.) node[midway, above, sloped] {$u$};
	\draw [->] (0.,0.) -- (2.,4.) node[midway, above, sloped] {$v$};
	\draw [->] (0.,0.) -- (4.,0.) node[midway, above, sloped] {$v+u$};
	\draw [line width=0.8pt,dotted,domain=-4.948163022910453:4.998779844455295] plot(\x,{(-16.--4.*\x)/2.});
	\draw [line width=0.8pt,dotted,domain=-4.948163022910453:4.998779844455295] plot(\x,{(--16.-4.*\x)/2.});
	\draw [->] (0.,0.) -- (4.,4.) node[midway, below, sloped] {$h(v)=h(v+u)$};
	\draw [->] (0.,0.) -- (-4.,-4.) node[midway, above, sloped] {$h(u)$};
	\begin{scriptsize}
	\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	\draw [fill=ududff] (2.,-4.) circle (2.5pt);
	\draw [fill=ududff] (2.,4.) circle (2.5pt);
	\draw [fill=xdxdff] (4.,0.) circle (2.5pt);
	\draw [fill=ududff] (4.,4.) circle (2.5pt);
	\draw [fill=ududff] (-4.,-4.) circle (2.5pt);
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]
	
	This is why $h$ is bad - it is distorting our drawings. We want our functions not only to preserve lines, but also to preserve \textit{parallelograms}. This is the motivation for the following definition, which, I might add, is the single most important definition in this whole chapter.
\end{ex}

\begin{df}
	A function $f:\R^2\to\R^2$ is said to be \textbf{linear} if for every $v,u\in \R^2$ and $\lambda\in \R$ we have:
	\begin{itemize}
		\item $f(v+u)=f(v)+f(u)$;
		\item $f(\lambda v)=\lambda f(v)$.
	\end{itemize}

	We'll denote the set of all linear functions in $\R^2$ by $\hom_\R(\R^2,\R^2)$.
\end{df}

\begin{rmk}
	This notation is, and should be, somewhat enigmatic at this point. Just trust us that it's going to make sense further on.
	
	Much further on.
\end{rmk}

This is precisely the concept we've just built in the example: The first item tells us that a linear function preserves parallelograms, and the second item tells us that a linear function preserves lines.

\begin{ex}
	Finishing up the motivating example, we claim that the function $g:\R^2\to R^2$ we defined before as being $g(x,y)=(2x-y,x+y)$ is a linear function.
	
	We've already proven, when we first introduced it, that $g$ preserves scalar multiplication. Let us then prove that it preserves summation:
	
	\begin{align*}
		g((a,b)+(c,d))&=g(a+c,b+d)\\
		&=(2(a+c)-(b+d),(a+c)+(b+d))\\
		&=(2a+2c-b-d,a+c+b+d)\\
		&=((2a-b)+(2c-d),(a+b)+(c+d))\\
		&=(2a-b,a+b)+(2c-d,c+d)=g(a,b)+g(c,d)
	\end{align*}for any $(a,b),(c,d)\in \R^2$, and so $g$ preserves summation and scalar multiplication - and is, therefore, a linear transformation.
\end{ex}

\begin{ex}
	Let us give some more examples of functions. Let $f,g,h:\R^2\to\R^2$ be given by $f(x,y)=(x,y)$, $g(x,y)=(y,x)$ and $h(x,y)=(x+y-4,y)$. Which ones of these are linear? Well, let's test: For this, fix, once and for all, $(a,b),(c,d)\in \R^2$ and $\lambda\in \R$. Then:
	\begin{align*}
		f((a,b)+(c,d))&=f((a+c,b+d))=(a+c,b+d)=(a,b)+(c,d)=f(a,b)+f(b,d)
	\end{align*}and
	\[f(\lambda (a,b))=f(\lambda a,\lambda b)=(\lambda a, \lambda b)=\lambda (a,b)=\lambda f(a,b)\]so $f$ is indeed linear.
	
	\begin{align*}
		g((a,b)+(c,d))&=g(a+c,b+d)=(b+d,a+c)=(b,a)+(d,c)=g(a,b)+g(c,d)
	\end{align*}and
	\[g(\lambda (a,b))=g(\lambda a,\lambda b)=(\lambda b,\lambda a)=\lambda (b,a)=\lambda g(a,b)\]so $g$ is indeed linear.
	
	$h$, however, isn't linear. It actually fails both checks: To see that, assume $h$ \textit{was} linear. Then $h(\lambda (a,b))=\lambda h(a,b)$ for all $\lambda \in \R$ and $(a,b)\in \R^2$.
	
	In particular, $h(3(1,1))=h(3,3)=(3+3-4,3)=(2,3)$, while $3h(1,1)=3(1+1-4,1)=3(-2,1)=(-6,3)$ and clearly $2\neq -6$ so they can't be equal.
	
	Similarly, $h((1,1)+(1,1))=h(2,2)=(2+2-4,2)=(0,2)$, while $h(1,1)+h(1,1)=(1+1-4,1)+(1+1-4,1)=(-4,2)$ and clearly $0\neq -4$ so they can't be equal.
	
	\bigskip
	It would be great then if we could tell at a glance whether a function is linear or not.
\end{ex}

\begin{lemma}
	Let $f:\R^2\to\R^2$ be a function. Then $f$ is linear if, and only if, there are real numbers $a,b,c,d\in \R$ such that $f(x,y)=(ax+cy,bx+dy)$ for all $(x,y)\in \R^2$.
\end{lemma}
\begin{proof}
	Let $f$ be linear. We want to show that there exists some real numbers $a,b,c,d\in\R$ such that $f(x,y)=(ax+cy,bx+dy)$.
	
	We can start off by seeing that $(x,y)=x(1,0)+y(0,1)$ and so, since $f$ is linear, we get $$f(x,y)=f(x(1,0)+y(0,1))=f(x(1,0))+f(y(0,1))=xf(1,0)+yf(0,1).$$
	
	But now, $f(1,0),f(0,1)\in\R^2$ and so there exist some real numbers $a,b,c,d\in \R$ such that $f(1,0)=(a,b)$ and $f(0,1)=(c,d)$. This allows us to go back and proceed with calculations:
	\[xf(1,0)+yf(0,1)=x(a,b)+y(c,d)=(xa,xb)+(yc,yd)=(xa+yc,xb+yd)\]and finally we see that, since real number multiplication is commutative, we can conclude that $f(x,y)=(ax+cy,bx+dy)$, as previously stated.
	
	\bigskip
	Conversely, let $f:\R^2\to\R^2$ be a function defined by $f(x,y)=(ax+cy,bx+dy)$ for all $(x,y)\in \R^2$, for some $a,b,c,d\in\R$ fixed. We claim that such an $f$ is linear.
	
	\begin{itemize}
		\item \begin{align*}
			f((x,y)+(x',y'))&=f(x+x',y+y')\\
			&=(a(x+x')+c(y+y'),b(x+x')+d(y+y'))\\
			&=(ax+ax'+cy+cy',bx+bx'+dy+dy')\\
			&=((ax+cy)+(ax'+cy'),(bx+dy)+(bx'+dy'))\\
			&=(ax+cy,bx+dy)+(ax'+cy',bx'+dy')=f(x,y)+f(x',y')
		\end{align*}so $f$ preserves sums.
		
		\item \begin{align*}
			f(\lambda (x,y))&=f(\lambda x,\lambda y)\\
			&=(a(\lambda x)+c(\lambda y),b(\lambda x)+d(\lambda y))\\
			&=(a\lambda x+c\lambda y,b\lambda x+d\lambda y)\\
			&=(\lambda (ax+by),\lambda(bx+dy))\\
			&=\lambda(ax+cy,bx+dy)=\lambda f(x,y)
		\end{align*}so $f$ preserves scalar multiplication.
	\end{itemize}

It follows that $f$ is linear, by definition, which ends the proof.
\end{proof}

This proof, however, far from only telling us that all linear functions are of such-and-such form, give us a very powerful tool for dealing with linear functions and vector spaces as a whole.

Re-analyze the proof above. Where did the $a,b,c,d$ come from? They are precisely the images of $(1,0)$ and $(0,1)$ under $f$. What we've shown, then, to some extent, is that the image of any point under a linear function is entirely determined by the images of $(1,0)$ and $(0,1)$.

\begin{theorem}
	Let $f:\R^2\to\R^2$ be a linear function. Then $f$ is uniquely determined by $f(1,0)$ and $f(0,1)$.
\end{theorem}
\begin{proof}
	Basically the same as above: Let's compute the image of $(x,y)$:
	\[f(x,y)=f(x(1,0)+y(0,1))=xf(1,0)+yf(0,1)\]and so if we call $f(1,0)=v$ and $f(0,1)=u$, we see that for any $(x,y)\in\R^2$, we have that $f(x,y)=xv+yu$.
	
	The result now follows.
\end{proof}

This has the following interesting consequence:
\begin{cor}
	Let $f:\{(1,0),(0,1)\}\to\R^2$ be any function. Then there is a unique linear function $\phi:\R^2\to\R^2$ such that $\phi(x,y):=xf(1,0)+yf(0,1)$.
\end{cor}
\begin{proof}
	All that remains is to show that $\phi$ is indeed linear, but even that is pointless: Since $f$ takes both $(1,0)$ and $(0,1)$ into $\R^2$, let $(a,b)=f(1,0)$ and $(c,d)=f(0,1)$. Then we readily see that $\phi(x,y)$ simply becomes
	\[\phi(x,y)=(ax+cy,bx+dy)\]which we've just shown is a linear function.
\end{proof}

This shows that the vectors $(1,0)$ and $(0,1)$ are very special. If you know what to do with them, you know what to do with literally everyone else.

\begin{df}
	We'll denote the vectors $(1,0)$ and $(0,1)$ by $e_1$ and $e_2$, respectively.
\end{df}

\begin{df}
	A set $X\subseteq \R^2$ is called a \textbf{base of $\R^2$} or a \textbf{basic set} if $$\hom(X,\R^2)\iso\hom_\R(\R^2,\R^2)$$ that is, every function $f:X\to \R^2$ extends uniquely to a linear function $f':\R^2\to\R^2$, and vice-versa.
\end{df}

\begin{rmk}
	Note that $\hom_\R(\R^2,\R^2)\subseteq\hom(X,\R^2)$ for any set $X\subseteq \R^2$, even if it's not a base: Indeed, any linear function $f:\R^2\to \R^2$ induces a function $f':X\to \R^2$ simply by only applying $f$ to the elements of $X$: $f'(x):=f(x)$ for all $x\in X$.
\end{rmk}

The corollary above is telling us that the set $\{e_1,e_2\}$ is a base of $\R^2$.

\begin{df}
	The set $E:=\{e_1,e_2\}$ will be called the \textbf{canonical base} of $\R^2$.
\end{df}

\begin{ex}
	Let $f:E\to\R^2$ be defined by $f(e_1):=(3,-\pi)$ and $f(e_2):=(4,0)$. Then there's a unique linear function $\phi:\R^2\to\R^2$ defined by $\phi(x,y):=xf(e_1)+yf(e_2)$. Let us compute some images of $\phi$:
	
	$$\phi(1,1)=1f(e_1)+1f(e_2)=1(3,-\pi)+1(4,0)=(3,-\pi)+(4,0)=(7,-\pi)$$
	$$\phi(1,0)=1f(e_1)+0f(e_2)=1(3,-\pi)+0(4,0)=(3,-\pi)=f(e_1)$$
	$$\phi(4,5)=4f(e_1)+5f(e_2)=4(3,-\pi)+5(4,0)=(12,-4\pi)+(20,0)=(32,-4\pi)$$
	
	Note that this is the same as defining $\phi(x,y)=(3x+4y,-\pi x)$.
\end{ex}

So for now, what you need to keep in mind regarding bases is that they are important precisely because of this property.

This will allow us to prove many results about vector spaces by simply stating a parallel result in terms of the base, and then using some previously proven set-theory result.

In order to get to those results, however, we'll need to keep on advancing.

\newpage
\subsection{You span my head right round, right round}

\begin{ex}
	Let $r\subseteq \R^2$ be the line $\R(2,3)$. That is, $v\in r$ if, and only if, $v=\lambda (2,3)$ for some $\lambda\in \R$.
	
	Take then $v,u\in r$. Then, by what we've just said, there are some $\lambda,\mu\in \R$ such that $v=\lambda(2,3)$ and $u=\mu(2,3)$.
	
	We can then ask the question: Is $v+u$ also in $r$? Well, $v+u=\lambda(2,3)+\mu(2,3)=(\lambda+\mu)(2,3)$, and since both $\lambda,\mu$ are real numbers, so is $\lambda+\mu$. This shows that if $v,u\in r$ then so is $v+u\in r$..
	
	Similarly we can ask: Is $\alpha v\in r$ for any $\alpha\in \R$? Once again, $\alpha v=\alpha(\lambda (2,3))=(\alpha\lambda)(2,3)$ and since both $\alpha$ and $\lambda$ are real numbers, so is $\alpha\lambda$. This shows that if $v\in r$, then so is $\alpha v\in r$ for any $\alpha\in \R$.
	
	What this tells us is that lines through zero are \textbf{closed} under addition and scaling - that is, if we add two points on the same line, then they remain on that line, and if we scale any point on a line, it also remains there.
\end{ex}

\begin{prop}
	Let $v\in \R^2$ be any vector, and consider the line $r:=\R v$. Take now any $u\in r$. Then $r=\R u$.
\end{prop}
\begin{proof}
	Let $u\in \R v$ - that is, $u=\mu v$ for some $\mu\in \R$.
	
	Take now $w\in \R v$ - that is, $w=\omega v$ for some $\omega \in \R$. We want to show that $w=\omega' u$, for some $\omega'\in \R$, and therefore $w\in \R u$.
	
	\bigskip
	This is simple: Take $\omega':=\frac{\omega}{\mu}\in \R$, which we know is a real number, since both $\omega$ and $\mu$ are real numbers.
	
	Now \[\omega'u=\omega'\left(\mu v\right)=(\omega'\mu)v=\left(\frac{\omega}{\mu}\mu\right)v=\omega v=w\]shows that $w\in \R u$.
	
	We've just shown that every point in $\R v$ is also in $\R u$. We can proceed analogously and prove that every point in $\R u$ is also in $\R v$.
	
	By definition of set equality, it follows that $\R v=\R u$, which ends the proof.
\end{proof}

\begin{df}
	If $v,u\in \R^2$ are such that $\R v=\R u$ then we say that \textbf{$v$ and $u$ are parallel vectors}, which shall be denoted as $v\parallel u$.
\end{df}
\begin{cor}
	Two vectors $v,u\in \R^2$ are parallel if, and only if, there's some real number $\lambda\in \R$ such that $v=\lambda u$.
\end{cor}

\begin{ex}
	Let $v=(1,1)$ and $u=(-3,-3)$. We claim that $v\parallel u$.
	
	That's easy to see, since $u=-3v$.
	
	Now this shows us immediately that the lines $\R v$ and $\R u$ are the same line: Take any $w\in \R v$. That means that $w=\omega(1,1)=(\omega,\omega)$ for some $\omega\in \R$.
	
	 But now, taking $\omega':=\dfrac{\omega}{-3}$, we see that 
	 \[\omega' u=\left(\frac{\omega}{-3}\right)u=\omega\left(\frac{u}{-3}\right)=\omega v=w\]and we see that $w\in \R u$, just as stated.
\end{ex}

We can now ask the following question: We know that lines through zero are closed under addition and scaling. Are there any other subsets of $\R^2$ that are like that? If so, what are they? If not, then why?

Let us give a name to that property, because it's a mouthful:

\begin{df}
	Let $X\subseteq \R^2$ be a set satisfying
	\begin{itemize}
		\item For all $x,y\in X$, $x+y\in X$;
		\item For all $x\in X$ and $\lambda\in \R$, $\lambda x\in X$.
	\end{itemize}

Then $X$ will be called a \textbf{subspace of $\R^2$} which will be denoted as $X\leq \R^2$.
\end{df}

\begin{ex}
	Clearly, since lines through zero are the motivating example for this definition, we know that for every vector $v\in \R^2$, the line $\R v$ is a subspace of $\R^2$ - that is, $\R v\leq \R^2$.
	
	But, if you think about it, $\R^2\subseteq \R^2$ is a subset of $\R^2$ which is also closed under additions and scalar multiplications (duh). So $\R^2\leq \R^2$.
	
	\bigskip
	Conversely, \textbf{any} line which doesn't contain zero cannot be a subspace: To see this, take any two vectors $v,u\in \R^2$ and consider the line $\R v+u$, which does not contain zero.
	
	Take now $w\in \R v+u$ and any other real number - for instance, 0.
	
	If $\R v+u$ was a subspace, it would be closed under scalar multiplication, but $0w=0\notin\R v+u$, so it cannot be a subspace.
	
	\bigskip
	What else then can be a subspace?
\end{ex}

\begin{lemma}
	If $X,Y\subseteq \R^2$ are two subspaces, then $X\cap Y$ is also a subspace.
\end{lemma}
\begin{proof}
	This is easy to see:
	\begin{itemize}
		\item Take $v,u\in X\cap Y$. This means that $v,u\in X$ and $v,u\in Y$, by definition of intersection. But since $v,u\in X$ and $X$ is a subspace, then $v+u\in X$. Similarly, since $v,u\in Y$ and $Y$ is a subspace, then $v+u\in Y$.
		
		Finally, using once more the definition of intersection, we see that $v+u\in X$ and $v+u\in Y$ implies $v+u\in X\cap Y$, and so $X\cap Y$ is closed under addition.
		
		\item Take $v\in X\cap Y$ and $\lambda\in \R$. Once again, $v\in X\cap Y$ implies $v\in X$ and $v\in Y$, and since both are subspaces, this implies $\lambda v\in X$ and $\lambda v\in Y$.
		
		Finally, using once more the definition of intersection, we see that $\lambda v\in X$ and $\lambda v\in Y$ implies $\lambda v\in X\cap Y$, and so $X\cap Y$ is closed under scalar multiplication.
	\end{itemize}
This shows that $X\cap Y$ is a subspace, which ends the proof.
\end{proof}

With this we can already infer an important result:

\begin{cor}
	The set containing only the origin (called the \textbf{zero} set $0:=\{(0,0)\}$) is a subspace.
\end{cor}

We could check this directly, but that would imply that we even considered it a possible candidate for subspace before. Following this result, however, since lines through zero are subspaces, and since any two of those lines meet precisely at zero, we know, for free, that zero is also a subspace.