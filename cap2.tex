\chapter{Vetores em $\R^2$}

\section{Definições e Propriedades Básicas}

Em matemática, dados dois conjuntos $A$ e $B$, nós denotamos por $A\times B$ o conjunto de todos os pares ordenados de elementos de $A$ e $B$.

\begin{ex}
	Se $A=\{a,b,c,d\}$ e $B=\{1,2,3\}$, então \[A\times B=\{(a,1),(a,2),(a,3),(b,1),(b,2),(b,3),(c,1),(c,2),(c,3),(d,1),(d,2),(d,3)\}.\]
	
	Se $C=\{\ltimes,\rtimes\}$, então
	\[C\times B=\{(\ltimes,1),(\ltimes,2),(\ltimes 3),(\rtimes,1),(\rtimes,2),(\rtimes 3)\}.\]
	
	Se $D=\{$calça, camiseta$\}$ e $E=\{$azul, verde, amarela$\}$, então
	\[D\times E=\{\mbox{calça azul, calça verde, calça amarela, camiseta azul, camiseta verde, camiseta amarela}\}.\]
\end{ex}

Assim, o conjunto de todos os pares ordenados de números reais é o conjunto $\R\times \R$. Como o símbolo $\times$ remete a um produto, nós chamamos esse conjunto de $\R^2$.

Vamos representar os elementos de $\R^2$ como pontos no plano da seguinte maneira: Correspondemos o elemento $(a,b)\in \R^2$ ao ponto $P$ dado pelas coordenadas $a$ e $b$, como abaixo:

	\[\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=0.75cm,y=0.75cm]
	\begin{axis}[
	x=0.75cm,y=0.75cm,
	axis lines=middle,
	xlabel=$\R$,
	ylabel=$\R$,
	xmin=-3.22,
	xmax=8.520000000000005,
	ymin=-2.1400000000000032,
	ymax=5.76,
	yticklabels={},	
	xticklabels={}]
	\clip(-3.22,-2.14) rectangle (8.52,5.76);
	\draw [dash pattern=on 3pt off 3pt] (0.,2.)-- (4.,2.);
	\draw [dash pattern=on 3pt off 3pt] (4.,0.)-- (4.,2.);
	\begin{scriptsize}
	\draw [fill=black] (4.,2.) circle (2.5pt);
	\draw[color=black] (4.14,2.37) node {$P$};
	\draw [color=uuuuuu] (0.,2.)-- ++(-2.0pt,0 pt) -- ++(4.0pt,0 pt) ++(-2.0pt,-2.0pt) -- ++(0 pt,4.0pt);
	\draw[color=uuuuuu] (-0.32,2) node {$b$};
	\draw [color=uuuuuu] (4.,0.)-- ++(-2.0pt,0 pt) -- ++(4.0pt,0 pt) ++(-2.0pt,-2.0pt) -- ++(0 pt,4.0pt);
	\draw[color=uuuuuu] (4.,-0.4) node {$a$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]
	
Claramente essa correspondência é biunívoca - pois dado um ponto no plano, ele é unicamente determinado por duas coordenadas.

Assim estabelecemos um paralelo entre \textbf{pontos no plano} e \textbf{elementos de $\R^2$}. Por esse motivo, elementos de $\R^2$ são muitas vezes chamados de \textit{pontos} de $\R^2$.

Além disso, podemos fazer outra correspondência:
\[\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
\begin{axis}[
x=1.0cm,y=1.0cm,
axis lines=middle,
xmin=-1,
xmax=6,
ymin=-1,
ymax=4,
xticklabels={},yticklabels={},xlabel=$\R$,ylabel=$\R$,]
\clip(-1.,-1.) rectangle (6.,4.02);
\draw [->] (0.,0.) -- (4.,2.);
\begin{scriptsize}
\draw [fill=black] (4.,2.) circle (2.5pt);
\draw[color=black] (4.14,2.37) node {$P$};
\draw[color=black] (1.88,1.29) node {$v$};
\end{scriptsize}
\end{axis}
\end{tikzpicture}\]

Vamos chamar de \textbf{vetor em $\R^2$} uma seta partindo da origem e terminando em qualquer ponto do plano. A figura acima estabelece uma correspondência biunívoca entre vetores no plano e pontos no plano: Para cada vetor $v$ existe um único ponto final $P$. Similarmente, para cada ponto $P$ existe um único vetor $v$ que termina em $P$.

Juntando tudo isso, nós vemos que \textbf{o conjunto de pontos, pares ordenados e vetores em $\R^2$ são ``a mesma coisa''}. Com isso em mente, daqui para frente vamos usar todos esses significados, usando sempre o significado mais conveniente naquele momento.

\begin{ex}
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
	\[\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-1,
	xmax=3,
	ymin=-1,
	ymax=5,
	xtick={1,2},
	ytick={1,2,3,4},
	xticklabels={},
	yticklabels={},
	xlabel=$\R$,
	ylabel=$\R$]
	\clip(-2.02,-2.02) rectangle (3.98,6.04);
	\draw [->] (0.,0.) -- (2.,4.);
	\draw [dash pattern=on 3pt off 3pt] (2.,4.)-- (2.,0.);
	\draw [dash pattern=on 3pt off 3pt] (2.,4.)-- (0.,4.);
	\begin{scriptsize}
	\draw [fill=black] (2.,4.) circle (2.5pt);
	\draw[color=black] (2.14,4.37) node {$P$};
	\draw[color=black] (0.84,2.25) node {$v$};
	\draw  (2.,0.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw (2,-0.3) node {$2$};
	\draw  (0.,4.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw (-0.3,4) node {$4$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]
	
	A figura acima em $\R^2$ representa simultaneamente os três conceitos: O vetor $v$ tem como ponto final o ponto $P$ cujas coordenadas são $(2,4)$.
\end{ex}

\subsection{Somas e produtos por números}

Dados dois pares ordenados $(a,b)$ e $(c,d)$ em $\R^2$, nós podemos notar que como $a,b,c,d$ são números reais, nós podemos somar $a+c$ e $b+d$, e ambos serão números reais. Assim, faria sentido definir uma operação de soma em $\R^2$ dada por
\[(a,b)+(c,d):=(a+c,b+d).\] Contudo, pelo que já vimos acima, pares, pontos e vetores são ``a mesma coisa''. Será que essa soma tem alguma interpretação via pontos e vetores?

\begin{ex}
	\[\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-1,
	xmax=9,
	ymin=-1,
	ymax=6,
	xticklabels={},
	yticklabels={},
	xtick={0,...,8.0},
	ytick={0,...,5.0},]
	\clip(-2.98,-2.02) rectangle (9.04,6.98);
	\draw [->,] (0.,0.) -- (2.,4.) node[sloped,pos=0.5,above] {$v$};
	\draw [,dash pattern=on 4pt off 4pt] (2.,4.)-- (2.,0.);
	\draw [,dash pattern=on 4pt off 4pt] (2.,4.)-- (0.,4.);
	\draw [->,] (0.,0.) -- (6.,1.) node[sloped,pos=0.5,below] {$u$};
	\draw [,dash pattern=on 4pt off 4pt] (6.,1.)-- (6.,0.);
	\draw [,dash pattern=on 4pt off 4pt] (6.,1.)-- (0.,1.);
	\begin{scriptsize}
	\draw [fill=black] (2.,4.) circle (2.5pt);
	\draw[color=black] (2.14,4.37) node {$P$};
	\draw [color=black] (2.,0.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (2,-0.3) node {$2$};
	\draw [color=black] (0.,4.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (-0.3,4) node {$4$};
	\draw [fill=black] (6.,1.) circle (2.5pt);
	\draw[color=black] (6.14,1.37) node {$Q$};
	\draw [color=black] (6.,0.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (6,-0.37) node {$6$};
	\draw [color=black] (0.,1.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (-0.3,1) node {$1$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]Na figura acima temos os pares $(2,4)$ e $(6,1)$ representados pelos pontos $P$ e $Q$ e pelos vetores $v$ e $u$, respectivamente.
	
	\[\definecolor{qqqqff}{rgb}{0.,0.,1.}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-1,
	xmax=9,
	ymin=-1,
	ymax=6,
	xticklabels={},
	yticklabels={},
	xtick={0,...,8},
	ytick={0,...,5},]
	\clip(-1.,-1.08) rectangle (9.02,6.);
	\draw [->] (0.,0.) -- (2.,4.) node[sloped,pos=0.5,above] {$v$};
	\draw [,dash pattern=on 3pt off 3pt] (2.,4.)-- (2.,0.);
	\draw [,dash pattern=on 3pt off 3pt] (2.,4.)-- (0.,4.);
	\draw [->] (0.,0.) -- (6.,1.) node[sloped,pos=0.5,below] {$u$};
	\draw [,dash pattern=on 3pt off 3pt] (6.,1.)-- (6.,0.);
	\draw [,dash pattern=on 3pt off 3pt] (6.,1.)-- (0.,1.);
	\draw [->,dotted] (6.,1.) -- (8.,5.) node[sloped,pos=0.5,below] {$v$};
	\draw [->,dotted] (2.,4.) -- (8.,5.) node[sloped,pos=0.5,above] {$u$};
	\draw [->,dash pattern=on 1pt off 1pt on 3pt off 4pt] (0.,0.) -- (8.,5.) node[sloped,pos = 0.5,above] {$v+u$};
	\draw [,dash pattern=on 3pt off 3pt] (8.,5.)-- (8.,0.);
	\draw [,dash pattern=on 3pt off 3pt] (8.,5.)-- (0.,5.);
	\begin{scriptsize}
	\draw [fill=black] (2.,4.) circle (2.5pt);
	\draw[color=black] (2.14,4.37) node {$P$};
	\draw [color=black] (2.,0.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (2,-0.3) node {$2$};
	\draw [color=black] (0.,4.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (-0.3,4) node {$4$};
	\draw [fill=black] (6.,1.) circle (2.5pt);
	\draw[color=black] (6.3,1) node {$Q$};
	\draw [color=black] (6.,0.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (6,-0.37) node {$6$};
	\draw [color=black] (0.,1.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (-0.3,1) node {$1$};
	\draw [fill=qqqqff] (8.,5.) circle (2.5pt);
	\draw[color=qqqqff] (8.14,5.37) node {$P+Q$};
	\draw [color=black] (8.,0.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (8,-0.37) node {$8$};
	\draw [color=black] (0.,5.)-- ++(-2.5pt,0 pt) -- ++(5.0pt,0 pt) ++(-2.5pt,-2.5pt) -- ++(0 pt,5.0pt);
	\draw[color=black] (-0.3,5) node {$5$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]Então, somando os pares $(2,4)$ e $(6,1)$ obtemos o par $(2+6,4+1)=(8,5)$. Geometricamente, temos a figura acima: Dados os vetores $v$ e $u$, com coordenadas $(2,4)$ e $(6,1)$, respectivamente, o vetor $v+u$ que é obtido simplesmente pela concatenação do vetor $u$ ao vetor $v$.
\end{ex}

\begin{df}
	Dados dois vetores $u,v\in \R^2$ com coordenadas $u=(a,b)$ e $v=(c,d)$, definimos a \textbf{soma de $u$ com $v$} como sendo o vetor $u+v$ dado por
	\[u+v:=(a+c,b+d).\]
\end{df}

\begin{exerc}
	Mostre que essa soma satisfaz as seguintes propriedades:
	\begin{itemize}
		\item (Comutatividade) Para quaisquer vetores $u,v\in R^2$, $u+v=v+u$;
		\item (Associatividade) Para quaisquer três vetores $u,v,w\in \R^2$, $u+(v+w)=(u+v)+w$;
		\item (Existência e unicidade de elemento neutro) Existe um (único) vetor $\overrightarrow{0}\in \R^2$ tal que $v+\overrightarrow{0}=v$ para qualquer $v\in \R^2$;
		\item (Existência e unicidade de inversos) Para qualquer vetor $v\in\R^2$ existe um (único) vetor $-v\in\R^2$ tal que $v+(-v)=\overrightarrow{0}$.
	\end{itemize}
\end{exerc}

Por outro lado, é intuitivamente óbvio que se $(a,b)\in\R^2$ e $\lambda\in \R$, certamente o par $(\lambda a,\lambda b)\in \R^2$. Então parece natural definir
\[\lambda(a,b):=(\lambda a,\lambda b).\] Contudo, será que temos uma boa interpretação geométrica pra isso, como para a soma?

\begin{ex}
	Fixe o número $\lambda=\dfrac{1}{2}\in\R$ e o vetor $v=(4,2)\in\R^2$.
	\definecolor{ududff}{rgb}{0.30196078431372547,0.30196078431372547,1.}
	\definecolor{xdxdff}{rgb}{0.49019607843137253,0.49019607843137253,1.}
	\[\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-1,
	xmax=5,
	ymin=-1,
	ymax=3,
	xtick={1,...,4},
	ytick={1,2},]
	\clip(-1,-1) rectangle (6,4);
	\draw [->] (0.,0.) -- (4,2) node[sloped,pos=0.5,above] {$v$};
	\draw [dash pattern=on 3pt off 3pt] (4,2)-- (0,2);
	\draw [dash pattern=on 3pt off 3pt] (4,2)-- (4,0);	
	\begin{scriptsize}
	\draw [fill=black] (4,2) circle (2.5pt);
	\draw[color=black] (4,2.37) node {$P$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]O que seria o vetor $u=(2,1)=\left(\dfrac{1}{2}\cdot 4,\dfrac{1}{2}\cdot2\right)=\dfrac{1}{2}\cdot v$?
	
	\[\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-1,
	xmax=5,
	ymin=-1,
	ymax=3,
	xtick={1,...,4},
	ytick={1,2},]
	\clip(-1,-1) rectangle (6,4);
	\draw [->] (0.,0.) -- (4,2) node[sloped,pos=0.7,above] {$v$};
	\draw [->,blue] (0.,0.) -- (2,1) node[sloped,pos=0.7,below] {$\frac{1}{2}v$};
	\draw [dash pattern=on 3pt off 3pt] (4,2)-- (0,2);
	\draw [dash pattern=on 3pt off 3pt] (4,2)-- (4,0);
	\draw [dash pattern=on 3pt off 3pt] (2,1)-- (0,1);
	\draw [dash pattern=on 3pt off 3pt] (2,1)-- (2,0);	
	\begin{scriptsize}
	\draw [fill=black] (4,2) circle (2.5pt);
	\draw[color=black] (4,2.37) node {$P$};
	\draw [fill=blue] (2,1) circle (2.5pt);
	\draw[color=black] (2,1.3) node[blue] {$\frac{1}{2}P$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]
	
	Então o vetor $\dfrac{1}{2}\cdot v$ é o vetor \textbf{na mesma direção de $v$}, mas \textbf{com tamanho igual a $\dfrac{1}{2}$ vezes o tamanho de $v$}.
\end{ex}

\begin{exerc}	
	O que seria, então multiplicar por um número negativo, por exemplo $\lambda=-1\in\R$? Faça um desenho do que seria uma interpretação geométrica para $(-1)\cdot(4,2)$.
\end{exerc}

\begin{exerc}
	Mostre que a multiplicação de vetores por números satisfaz:
	\begin{itemize}
		\item (Comutatividade) $(\lambda\mu)v=(\mu\lambda)v$ para quaisquer números $\lambda,\mu\in \R$ e vetor $v\in \R^2$;
		\item (Associatividade) $\lambda(\mu v)=(\lambda\mu)v$ e para quaisquer números $\lambda,\mu\in \R$ e vetor $v\in \R^2$;
		\item (Existência e unicidade de elemento neutro) Existe um (único) numero real $u\in \R$ tal que $u v=v$ para qualquer vetor $v\in\R^2$;
		\item (Distributividades) Para quaisquer $\lambda,\mu \in \R$ e $v,u\in\R^2$, temos que $\lambda(v+u)=\lambda v=\lambda u$ e $(\lambda+\mu)v=\lambda v+\mu v$.
	\end{itemize}
\end{exerc}

\section{Vetores e Matrizes}

Antes de estudar mais a fundo os vetores, contudo, precisamos estabelecer um paralelo entre o estudo de matrizes e o estudo de vetores:

Considere a função $[-]:\R^2\to M_{2\times 1}(\R)$ dada por
\[[(x,y)]:=\begin{pmatrix}
x\\y
\end{pmatrix}\in M_{2\times 1}(\R).\] Essa função ``não faz nada'': Ela só nos permite enxergar vetores como matrizes coluna. Similarmente, podemos considerar a função $V:M_{2\times 1}(\R)\to \R^2$ dada por 
\[V\begin{pmatrix}
x\\y
\end{pmatrix}:=(x,y)\in \R^2.\] Essa função também ``não faz nada'': Ela só nos permite enxergar matrizes coluna como vetores. Contudo, note que:

\[V([(x,y)])=V\begin{pmatrix}
x\\y
\end{pmatrix}=(x,y)\]
\[\left[V\begin{pmatrix}
x'\\y'
\end{pmatrix}\right]=[(x',y')]=\begin{pmatrix}
x'\\y'
\end{pmatrix}\]ou seja, \textit{essas funções são inversas}! Isso nos diz que os conjuntos $\R^2$ e $M_{2\times 1}(\R)$ são ``a mesma coisa'', apenas vistos com outros olhos. Por isso, vamos adicionar mais um significado à palavra vetor: Não só pontos no plano, ou pares ordenados em $\R^2$ ou setas partindo da origem, mas também matrizes coluna com duas entradas.

\subsection{Funções lineares}

Nesta seção, então, vamos interpretar vários dos resultados do capítulo anterior, mas agora com o auxílio dos diversos significados de vetor.

Por exemplo, dada uma matriz quadrada $A=\begin{pmatrix}
a&b\\c&d
\end{pmatrix}$, a multiplicação $AX$, em que $X$ é alguma matriz coluna com duas entradas, é da seguinte forma:
\[\begin{pmatrix}
a&b\\c&d
\end{pmatrix}\begin{pmatrix}
x\\y
\end{pmatrix}=\begin{pmatrix}
ax+by\\cx+dy
\end{pmatrix}.\] Então para cada valor de $x$ e $y$, o resultado do produto $AX$ é diferente -  ora, isso nos permite definir:

\begin{df}
	Para cada matriz $A\in M_2(\R)$, definimos $T_A:\R^2\to \R^2$ a função dada por
	\[T_A(x,y):=V(A[(x,y)]).\]
\end{df}

Ou seja, para calcular $T_A(x,y)$, a gente escreve $(x,y)$ como uma matriz coluna $\begin{pmatrix}
x\\y
\end{pmatrix}$, multiplica ele por $A$ e escreve o resultado como um vetor.

\begin{ex}
	Dada a matriz $A=\begin{pmatrix}
	2 & 5\\
	1 & 3
	\end{pmatrix}$, a função $T_A$ é tal que
	\[T_A(x,y)=V(A[(x,y)])=V\left(\begin{pmatrix}
	2 & 5\\
	1 & 3
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}\right)=V\left(\begin{pmatrix}
	2x+5y\\x+3y
	\end{pmatrix}\right)=(2x+5y,x+3y)\]em outras palavras, a função $T_A$ é tal que $T_A(x,y)$ é simplesmente
	\[\begin{pmatrix}
	2 & 5\\
	1 & 3
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	2x+5y\\x+3y
	\end{pmatrix}\]escrito em forma de vetor, ou seja $(2x+5y,x+3y)$.
	
	Similarmente,
	\[T_{I_2}(x,y)=V(I_2[(x,y)])=V\left(\begin{pmatrix}
	1&0\\0&1
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}\right)=V\left(\begin{pmatrix}
	x\\y
	\end{pmatrix}\right)=(x,y),\]e, analogamente a como fizemos acima, ou seja, $T_{I_2}(x,y)$ é simplesmente
	\[\begin{pmatrix}
	1&0\\0&1
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	x\\y
	\end{pmatrix}\] escrito em forma de vetor, ou seja, $(x,y)$ - em outras palavras, a função $T_{I_2}$ é a função identidade que leva todo vetor nele mesmo.
\end{ex}

Como essas funções são dadas por matrizes, será que elas têm as propriedades de matrizes?

\begin{prop}
	Dados uma matriz $A\in M_2(\R)$, qualquer número $\lambda \in \R$ e dois vetores $v,u\in \R^2$, temos:
	\[T_A(\lambda v)=\lambda T_A(v)\]
	\[T_A(v+u)=T_A(v)+T_A(u).\]
\end{prop}

\begin{exerc}
	Prova a proposição acima (dica: calcule explicitamente $T_A(\lambda v)$ e $\lambda T_A(v)$ e veja que são a mesma coisa. Idem para as somas.).
\end{exerc}

Essas funções são bastante interessantes. Elas nos dão um ``dicionário'' que nos permite ver o mundo dos vetores como se fosse o mundo das matrizes, e vice-e-versa. Seria, então, interessante se a gente conseguisse um método de determinar quando uma função $f:\R^2\to \R^2$ é dessa forma. 

\begin{ex}
	A função $f:\R^2\to \R^2$ dada por $f(x,y)=(x,x^2)$ \textbf{não} é dada por uma matriz. Podemos ver isso usando a proposição acima: Se $f$ fosse igual a $T_A$ para alguma matriz $A\in M_2(\R)$, $f(v+u)$ seria igual a $f(v)+f(u)$. Mas
	\[f(0,0)=(0,0^2)=(0,0),\]enquanto
	\[f(1,0)+f(-1,0)=(1,1^2)+(-1,1^2)=(0,2).\] Agora, note que $(0,0)=(1,0)+(-1,0)$. Então nós acabamos de mostrar que $f((1,0)+(-1,0))\neq f(1,0)+f(-1,0)$, ou seja, $f$ \textbf{não pode ser dada por $T_A$}.
\end{ex}

\begin{prop}
	Uma função $f:\R^2 \to \R^2$ é dada por uma matriz se, e somente se, existem números reais $a,b,c,d\in \R$ tais que $f(x,y)=(ax+by,cx+dy)$.
\end{prop}
\begin{proof}
	Suponha que $f$ é dada por uma matriz, ou seja, $f=T_A$ para alguma matriz $A=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}\in M_2(\R)$. Então, para qualquer vetor $(x,y)\in \R^2$ temos:
	\[f(x,y)=T_A(x,y)=V(A([x,y]))=V\left(\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}\right)=V\left(\begin{pmatrix}
	ax+by\\cx+dy
	\end{pmatrix}\right)=(ax+by,cx+dy),\] como queríamos mostrar.
	
	\bigskip
	Por outro lado, suponha que $f(x,y)=(ax+by,cx+dy)$. Afirmamos que a matriz $A=\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}$ é tal que $f=T_A$. De fato, 
	\[T_A(x,y)=(ax+by,cx+dy)=f(x,y),\]o que encerra a prova.
\end{proof}

Finalmente, agora vamos resolver um problema que será central nesse curso: Será que toda função $f:\R^2\to \R^2$ que preserva somas e produtos por números é dada por matrizes?

\begin{theorem}
	Seja $f:\R^2\to\R^2$ uma função. Então $f(v+\lambda u)=f(v)+\lambda f(u)$ para todo $\lambda\in \R$ e todos $v,u\in \R^2$ se, e somente se, $f=T_A$ para alguma matriz $A\in M_2(\R)$.
\end{theorem}
\begin{proof}
	Já mostramos que se $f=T_A$, então $f(v+\lambda u)=f(v)+\lambda f(u)$.
	
	Suponha, então, que $f(v+\lambda u)=f(v)+\lambda f(u)$. Vamos mostrar que $f=T_A$ para alguma matriz $A$.
	
	Calculando $f(x,y)$ temos:
	\[\begin{array}{rll}
		f(x,y)&=f(x,0)+f(0,y)&\mbox{(já que $f$ preserva somas)}\\
		&=xf(1,0)+yf(0,1)&\mbox{(já que $f$ preserva multipli-}\\
		&&\mbox{cação por números)}.
	\end{array}\]Sejam então $f(1,0)=(a,b)$ e $f(0,1)=(c,d)$. Assim,
	\[\begin{array}{rl}
	f(x,y)&=xf(1,0)+yf(0,1)\\
	&=x(a,b)+y(c,d)\\
	&=(xa,ab)+(yc,yd)=(xa+yc,xb+yd)
	\end{array}\]e, pela proposição anterior, sabemos que $f=T_A$, onde $A=\begin{pmatrix}
	a&c\\b&d
	\end{pmatrix}$, como queríamos mostrar.
\end{proof}

\begin{df}
	Uma função $f:\R^2\to \R^2$ será dita \textbf{linear} se $f(v+\lambda u)=f(v)+\lambda f(u)$ para quaisquer $\lambda\in \R$ e $v,u\in\R^2$.
\end{df}

Então o teorema acima nos diz que as funções lineares são exatamente as funções ``multiplique o seu vetor por uma matriz''.

\begin{ex}
	Sejam $f,g:\R^2\to R^2$ funções lineares. Será que $g\circ f$ e $f\circ g$ também são lineares?
	
	Por exemplo, se $f=T_A$ e $g=T_B$, com
	\[A=\begin{pmatrix}
	1&3\\
	2&5
	\end{pmatrix}\mbox{ e } B=\begin{pmatrix}
	2&5\\
	1&3
	\end{pmatrix},\] podemos lembrar que $(f\circ g)(x,y)$ é simplesmente
	\[\begin{pmatrix}
	1&3\\
	2&5
	\end{pmatrix}\left(\begin{pmatrix}
	2&5\\
	1&3
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}\right)\]escrito em forma de vetor. Contudo, o produto de matrizes é associativo! Então esse produto é a mesma coisa que
	\[\left(\begin{pmatrix}
	1&3\\
	2&5
	\end{pmatrix}\begin{pmatrix}
	2&5\\
	1&3
	\end{pmatrix}\right)\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	5&14\\9&25
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix},\] ou seja, a função $f\circ g$ é dada pela matriz $AB=\begin{pmatrix}
	5&14\\9&25
	\end{pmatrix}$.
	
	Similarmente, a função $g\circ f$ leva um vetor $(x,y)$ no vetor dado pelo produto de matrizes
	\[\begin{pmatrix}
	2&5\\
	1&3
	\end{pmatrix}\left(\begin{pmatrix}
	1&3\\
	2&5
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}\right)\]escrito em forma de vetor. Contudo, como notamos acima, o produto de matrizes é associativo, então esse produto pode ser escrito como
	\[\left(\begin{pmatrix}
	2&5\\
	1&3
	\end{pmatrix}\begin{pmatrix}
	1&3\\
	2&5
	\end{pmatrix}\right)\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	12 & 31\\7&18
	\end{pmatrix},\]ou seja, a função $g\circ f$ é dada pela matriz $BA$.
	
	
	Em suma, isso nos diz o seguinte: A definição de multiplicação de matrizes é feita de forma que a composição de função lineares seja a função dada pelo produto das matrizes correspondentes.
	
	Ou, em outras palavras, poderíamos definir $AB$ como sendo a matriz tal que $T_A\circ T_B=T_{AB}$.
\end{ex}

\begin{exerc}
	Mostre, sem utilizar exemplos concretos, que se $f$ e $g$ são lineares, então $f\circ g$ e $g\circ f$ também são (dica: use o exemplo acima).
\end{exerc}

\section{Subespaços}
\subsection{Núcleo e imagem}

Agora vamos introduzir dois conceitos que vão nos ajudar a entender ainda melhor o espaço de vetores e transformações lineares.

\begin{df}
	Dada uma função linear $f:\R^2\to \R^2$ definimos o \textbf{núcleo de $f$} como sendo o conjunto $\Ker f$ dado por
	\[\Ker f:=\{v\in \R^2\mid f(v)=(0,0)\},\]ou seja, são os pontos de $\R^2$ que vão na origem quando aplicamos $f$.
\end{df}

\begin{rmk}
	O símbolo $\Ker$ vem da palavra inglesa \emph{kernel} que significa ``a parte macia de um grão'' ou ``a parte central''.
\end{rmk}

\begin{ex}
	Considere função $f:\R^2\to\R^2$ que leva $(x,y)$ em $(x,0)$, ou seja, $f(x,y)=(x,0)$. Claramente, $f$ é linear (verifique!) e é dada pela matriz $A=\begin{pmatrix}
	1&0\\0&0
	\end{pmatrix}$:
	\[\begin{pmatrix}
	1&0\\0&0
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	x\\0
	\end{pmatrix}.\]
	
	Quem é o núcleo de $f$? São todos os pontos $(x,y)\in \R^2$ tais que $f(x,y)=(0,0)$. Mas $f(x,y)=(x,0)$. Então $(x,0)=f(x,y)=(0,0)$ se, e somente se, $x=0$ -  ou seja,
	\[\Ker f=\{(x,y)\in \R^2\mid x=0\}.\]
	
	Será que isso nos diz alguma coisa sobre a matriz $A$? Bom, resolvendo o sistema $AX=0$ temos:
	\[\begin{pmatrix}
	1&0\\0&0
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	0\\0
	\end{pmatrix}\]ou seja, $x=0$ e $0y=0$. Isso nos diz que
	\[S_0:=\{(x,y)\in\R^2\mid x=0\}.\]
	
	Mas isso é a mesma coisa que $\Ker f$!
	
	Em outras palavras, o núcleo de uma função linear \textbf{nada mais é do que o conjunto solução do sistema homogêneo associado àquela função}.
	
	\tcblower
	Antes de avançarmos, vamos ver o que acontece quando temos uma matriz com uma linha nula:
	\[A=\begin{pmatrix}
	a&b\\0&0
	\end{pmatrix}.\] Nesse caso, o sistema homogêneo
	\[\begin{pmatrix}
	a&b\\0&0
	\end{pmatrix}\begin{pmatrix}
	x\\y
	\end{pmatrix}=\begin{pmatrix}
	0\\0
	\end{pmatrix}\]nos dá a equação $ax+by=0$... E só. Então se ambos $a=b=0$, isso não teria graça - estaríamos dizendo simplesmente que $0+0=0$.
	
	Vamos supor, então, que ou $a$ ou $b$ é diferente de 0 - por exemplo, $a$: Nesse caso, podemos isolar $x$, fazendo $x=\dfrac{-b}{a}y$.
	
	Assim, vemos que para cada possível valor de $y$ podemos encontrar um único valor de $x$ correspondente. Ora, isso nos diz que se uma matriz tem uma linha nula, então certamente o sistema homogêneo tem várias soluções além da trivial - basta dar valores para $y$ na fórmula acima!
\end{ex}

\begin{prop}
	Para qualquer função linear $f:\R^2\to\R^2$, $\Ker f\neq \varnothing$.
\end{prop}
\begin{proof}
	Como $f$ é linear, existe matriz $A$ tal que $f=T_A$. Mas, pelo exemplo acima, vimos que $\Ker f= S_0$, em que $S_0$ é o conjunto de soluções de $AX=0$. Contudo, já vimos que $(0,0)\in S_0$. Segue que $(0,0)\in \Ker f$ e, portanto, $\Ker f\neq\varnothing$.
	
	
	Alternativamente, podemos calcular explicitamente:
	
	\[f(0,0)=f(1,0)+f(-1,0)=f(1,0)-f(1,0)=(0,0)\]e chegar à mesma conclusão.
\end{proof}

Agora vamos usar o núcleo para inferir alguns resultados importantes da álgebra vetorial:

\begin{prop}
	Seja $f:\R^2\to\R^2$ linear. Então $f$ é injetiva se, e somente se, $\Ker f=\{(0,0)\}$.
\end{prop}

\begin{proof}
	Se $f$ é injetiva, então $f(v)=f(u)$ implica $v=u$. Escolha, então, qualquer $v\in \Ker f$ - ou seja, $f(v)=(0,0)$. Mas, pela proposição anterior, sabemos que $f(0,0)=(0,0)$. Agora, como $f$ é injetiva e $v$ e $(0,0)$ têm a mesma imagem, isso implica que $v=(0,0)$. Ora, nós mostramos que qualquer elemento $v$ no núcleo \textbf{tem} que ser $(0,0)$ - segue que o único elemento do núcleo é $(0,0)$.
	
	Por outro lado, suponha que $\Ker f=\{(0,0)\}$. Tome, então, $v,u\in\R^2$ tais que $f(v)=f(u)$. Se mostrarmos que $v=u$, teremos mostrado que $f$ é injetiva. Mas $f$ é linear, então dizer que $f(v)=f(u)$ é a mesma coisa que dizer que $f(v-u)=(0,0)$. Mas estamos supondo que $\Ker f=\{(0,0)\}$ - ou seja, se $f(w)=(0,0)$, então $w=(0,0)$. Como nós temos que $f(v-u)=(0,0)$, nossa hipótese de $\Ker f=\{(0,0)\}$ nos garante que $v-u=(0,0)$. Finalmente, isso é a mesma coisa que $v=u$. Logo, nós concluímos que $f$ é, de fato, injetiva, o que encerra a demonstração.
\end{proof}

\begin{ex}
	Vamos voltar a comparar núcleos e soluções do sistema homogêneo. O resultado que mostramos acima nos diz que uma função linear é injetiva se, e somente se, o núcleo é ``trivial'' - ou seja, o sistema homogêneo só possui a solução trivial $S_0=\{(0,0)\}$. Como podemos interpretar, do ponto de vista de matrizes, a injetividade de uma função linear?
	
	Pela proposição acima, uma função linear é injetiva se, e somente se, o único ponto que vai no zero é o zero. Em termos de matrizes, então, isso significa ``uma matriz $A$ é \textit{injetiva} se $AX=0$ implica $X=0$''. Em outras palavras, uma matriz é injetiva se o sistema homogêneo \textit{possui solução única} (a solução trivial).
	
	Mas já vimos acima que um sistema tem solução única se, e somente se, a forma escalonada da matriz $A$ não possui linhas de zeros.
	
	Juntando tudo isso, vemos que \textit{uma matriz corresponde a uma função linear injetiva se, e somente se, sua forma escalonada \textbf{não possui} linhas de zeros}.
	
	\tcblower
	
	Por exemplo, considere as matrizes 
	\[A=\begin{pmatrix}
	1&0\\0&1
	\end{pmatrix},\quad B=\begin{pmatrix}
	1&1\\
	-1&1
	\end{pmatrix},\quad C=\begin{pmatrix}
	2&3\\
	4&6
	\end{pmatrix}.\] $A$ já está escalonada, e não possui linhas de zeros, então certamente $T_A$ é injetiva.
	
	$B$ ainda não está escalonada:
	\[\begin{pmatrix}
	1&1\\-1&1
	\end{pmatrix}\rightsquigarrow\begin{pmatrix}
	1&1\\0&2
	\end{pmatrix}\rightsquigarrow\begin{pmatrix}
	1&1\\0&1
	\end{pmatrix}\rightsquigarrow\begin{pmatrix}
	1&0\\0&1
	\end{pmatrix}.\] Então $B$ escalonada é $A$, que já vimos que não possui linhas de zeros. Segue que $T_B$ é injetiva.
	
	$C$ também não está escalonada ainda:
	\[\begin{pmatrix}
	2&3\\4&6
	\end{pmatrix}\rightsquigarrow\begin{pmatrix}
	1&3/2\\4&6
	\end{pmatrix}\rightsquigarrow\begin{pmatrix}
	1&3/2\\0&0
	\end{pmatrix}.\] Então $C$ escalonada tem uma linha de zeros, ou seja, $T_C$ \textbf{não} é injetiva.
\end{ex}

A discussão e os exemplos acima sugerem que toda função linear injetiva possui inverso. Isso pode soar estranho a princípio: Por exemplo, esquecendo o adjetivo ``linear'', sabemos que uma função $f$ entre dois conjuntos quaisquer possui inversa se, e somente se, $f$ é injetiva e sobrejetiva. Dito de outra maneira, em geral \textit{não basta uma função ser injetiva para que ela seja inversível}.

Essa é uma das grandes vantagens de funções lineares: Com o acréscimo desse adjetivo ``linear'', podemos mostrar o seguinte resultado:

\begin{theorem}
	Seja $f:\R^2\to \R^2$ uma função linear. Então $f$ é inversível se, e somente se, $f$ é injetiva, o que acontece se, e somente se, $f$ é sobrejetiva.
\end{theorem}

Em outras palavras, no mundo dos vetores e funções lineares, \textit{ser injetivo, sobrejetivo e inversível é \textbf{a mesma coisa}}.

Vamos agora caminhar em direção a demonstra esse resultado. Para isso vamos precisar de alguns conceitos.

\begin{df}
	Seja $S\subset \R^2$ um subconjunto de $\R^2$ tal que para quaisquer $v,u\in S$ e qualquer $\lambda\in\R$ temos que $v+\lambda u\in S$. Então diremos que $S$ é um \textbf{subespaço de $\R^2$}.
\end{df}

\begin{rmk}
	Note que, por definição, se $v\in S$ e $S$ é subespaço de $\R^2$, então $\lambda v$ também está em $S$, para qualquer $\lambda\in \R$. Disso segue que se $S\neq \varnothing$, então $S$ tem infinitos elementos.
\end{rmk}

\begin{exerc}
	Mostre que $\{(0,0)\}$, $\{(x,y)\in \R^2\mid y=\lambda x \mbox{ para algum }\lambda\in \R\}$ e $\R^2$ são subespaços de $\R^2$.
\end{exerc}

Com isso, podemos finalmente ter um bom resultado:

\begin{prop}
	Para qualquer função linear $f:\R^2\to\R^2$, temos que $\Ker f$ é um subespaço de $\R^2$.
\end{prop}

\begin{proof}
	Tome $v,u\in \Ker f$ e $\lambda\in \R$. Vamos mostrar que $f(v+\lambda u)=(0,0)$ e, portanto, $v+\lambda u\in \Ker f$. Calculando:
	\[f(v+\lambda u)=f(v)+f(\lambda u)=f(v)+\lambda f(u),\]já que $f$ é linear. Agora, como $v,u\in \Ker f$, temos que $f(v)=f(u)=(0,0)$, logo
	\[f(v+\lambda u)=f(v)+\lambda f(u)=(0,0)+\lambda (0,0)=(0,0)+(0,0)=(0,0),\]ou seja, $v+\lambda u\in \Ker f$. 
	
	Segue que $\Ker f$ é subespaço de $\R^2$, como queríamos mostrar.
\end{proof}
\begin{cor}
	Seja $f:\R^2\to\R^2$ linear. Então $\Ker f$ ou tem um único ponto ($(0,0)$) ou tem infinitos pontos. 
\end{cor}

\begin{rmk}
	O resultado acima é uma justificativa para a afirmação de que um sistema linear ou não tem solução, ou tem uma solução ou tem infinitas soluções.
\end{rmk}

\begin{exerc}
	Prove a observação acima - ou seja, tome qualquer sistema linear $AX=B$ com $A\in M_2(\R)$ e mostre que o conjunto de soluções ou é vazio, ou tem um único ponto, ou tem infinitos pontos.
\end{exerc}

Temos, por outro lado, outro subespaço associado a uma função linear - a imagem desta:

\begin{df}
	Seja $f:X\to Y$ uma função entre os conjuntos $X$ e $Y$. Denotamos por $\im f$ a \textbf{imagem de $f$} o conjunto dado por 
	\[\im f:=\{v\in Y\mid \exists u\in X\mbox{ tal que }f(u)=v\}.\]
\end{df}

Ou seja, a imagem de uma função são todos os pontos que você pode obter aplicando aquela função.

\begin{ex}
	Considere a função $f:\N\to \N$ dada por $f(n)=n+1$. Qual a imagem de $f$? São todos os números naturais que podem ser escritos como ``algum número natural + 1''. Por exemplo, 2 está na imagem, pois $2=1+1$. Similarmente, 7 está na imagem, pois $7=6+1$. O único número natural que não está na imagem é o 0 - de fato, não existe nenhum número natural $n$ tal que $0=n+1$. Então
	\[\im f=\N-\{0\}=\{1,2,3,4,5,\cdots\}.\]
	\tcblower
	Considere agora a função $\lVert-\rVert:\R^2\to \R$ dada por $\lVert(x,y)\rVert:=\sqrt{x^2+y^2}$. Essa função é chamada de \textbf{norma} - ela mede o tamanho de um vetor em $\R^2$. Qual a imagem dessa função, ou seja, quais são todos os possíveis tamanhos de vetores?
	
	Olhando para a fórmula da função, vemos que a norma é a raiz quadrada (que é sempre não-negativa) da soma de dois números positivos (que é sempre não-negativa), ou seja, a imagem de $\lVert-\rVert$ são todos os números reais não-negativos.
	
	De fato, para qualquer número não-negativo $\lambda\in\R$ podemos exibir um vetor com essa norma: $(\lambda,0)$. Computando a norma de $(\lambda,0)$ temos:
	\[\lVert(\lambda,0)\rVert=\sqrt{\lambda^2+0^2}=\sqrt{\lambda^2}\]e como $\lambda\geq0$, temos finalmente $\sqrt{\lambda^2}=\lambda$, ou seja, existe (pelo menos) um vetor $v$ tal que $\lVert v\rVert=\lambda$ e, portanto, $\lambda\in\im \lVert-\rVert$.
\end{ex}

\begin{prop}
	Para qualquer função linear $f:\R^2\to \R^2$, temos que $\im f$ é um subespaço de $\R^2$.
\end{prop}
\begin{proof}
	Tome $v,u$ em $\im f$ e $\lambda\in \R$ qualquer. Vamos mostrar que $v+\lambda u$ também está na imagem.
	
	Por definição, se $v\in\im f$, então $v$ é imagem de alguém, ou seja, existe $v'\in\R^2$ tal que $f(v')=v$. Similarmente, como $u\in\im f$ ele é imagem de alguém, ou seja, existe $u'\in\R^2$ tal que $f(u')=u$.
	
	Afirmamos que $v+\lambda u$ é imagem de $v'+\lambda u'$ por $f$: De fato, \(f(v'+\lambda u')=f(v')+\lambda f(u')\), já que $f$ é linear. Agora, usando que $v=f(v')$ e $u=f(u')$ temos finalmente que $f(v'+\lambda u')=v+\lambda u$, donde podemos concluir que $v+\lambda u\in\im f$ e, portanto, $\im f$ é subespaço.
\end{proof}
\begin{cor}
	Para qualquer função linear $f:\R^2\to\R^2$, $(0,0)\in\im f$.
\end{cor}

\subsection{Geradores, dependência e independência linear}

Na seção anterior introduzimos o conceito de subespaços de $\R^2$ que são subconjuntos que se comportam ``como $\R^2$'' - ou seja, são fechados para soma e multiplicação por escalar. Será que subespaços herdam alguma outra propriedade interessante de $\R^2$?
\begin{ex}
	Considere qualquer vetor $(x,y)\in \R^2$. Esse vetor pode ser escrito de maneira \textbf{única} como $(x,y)=x(1,0)+y(0,1)$.
	
	Por outro lado, também podemos escrever de maneira única qualquer vetor $(x,y)$ de $\R^2$ como $(x,y)=\dfrac{x+y}{2}(1,1)+\dfrac{x-y}{2}(1,-1)$ - por exemplo, $(5,3)$ pode ser escrito tanto como $5(1,0)+3(0,1)$ quanto como $4(1,1)+(1,-1)$: De fato, $5(1,0)+3(0,1)=(5,0)+(0,3)=(5,3)$ e $4(1,1)+(1,-1)=(4,4)+(1,-1)=(5,3)$.
	
	De maneira geométrica, isso significa que para chegar em qualquer ponto no plano partindo da origem podemos dar passos horizontais e verticais ($(1,0)$ e $(0,1)$) ou passos nas direções nordeste e noroeste ($(1,1)$ e $(1,-1)$) (sempre lembrando que podemos dar passos para frente e para trás).
\end{ex}

\begin{df}
	Dados dois vetores $v,u\in\R^2$, dizemos que \textbf{$u$ é gerado por $v$} se existe algum número real $\lambda$ tal que $u=\lambda v$.
	
	Similarmente, dado um vetor $v$, o conjunto de todos os vetores gerados por $v$ será chamado de \textbf{conjunto gerado por $v$} e denotado por $\gen(v)$. Nesse caso diremos que $v$ é um \textbf{gerador de $\gen(v)$}.
\end{df}
\begin{df}
	Dados um vetor $u\in\R^2$ e uma coleção finita de vetores $V=\{v_1,v_2,\cdots,v_n\}\subseteq\R^2$, dizemos que \textbf{$u$ é gerado por $V$} se existe uma coleção de números reais $\{\lambda_1,\lambda_2,\cdots,\lambda_n\}\subseteq\R$ tal que
	\[u=\lambda_1v_1+\lambda_2v_2+\cdots+\lambda_nv_n.\]
	
	Similarmente, dada uma coleção de vetores $V$, o conjunto de todos os vetores gerados por $V$ será chamado de \textbf{conjunto gerado por $V$} e denorado por $\gen(V)$. Nesse caso diremos que $V$ é um \textbf{conjunto de geradores de $\gen(V)$} ou, equivalentemente, um \textbf{gerador de $\gen(V)$}.
\end{df}

\begin{rmk}
	A terminologia $\gen$ para se referir ao conjunto gerado vem do termo inglês \emph{generated}, ``gerado'' em português.
\end{rmk}

\begin{ex}
	Continuando o exemplo acima, vimos que qualquer vetor $(x,y)\in\R^2$ pode ser escrito tanto como $x(1,0)+y(0,1)$ quanto como $\dfrac{x+y}{2}(1,1)+\dfrac{x-y}{2}(1,-1)$. Ou seja, tanto $E=\{(1,0),(0,1)\}$ quanto $V=\{(1,1),(1,-1)\}$ são geradores de $\R^2$ - ou, em outras palavras, $\R^2=\gen(E)=\gen(V)$.
	
	Por outro lado, dado o vetor $v=(2,3)$, qual o conjunto gerado por $v$ - ou seja, quem é $\gen(v)$? Por definição, temos:
	\[\gen(v)=\{u\in\R^2\mid \exists\lambda\in\R\mbox{ tal que }u=\lambda v\},\]ou seja, $\gen(v)$ é o conjunto de todos os múltiplos de $v$. Por exemplo, $(4,6)$, $(-2,-3)$, $(2\pi,3\pi)$ e $(2000,3000)$ são gerados por $v$.
\end{ex}

\begin{prop}
	Para qualquer coleção finita de vetores $V$, temos que $\gen(V)$ é um subespaço de $\R^2$.
\end{prop}
\begin{proof}
	Vamos denotar $V=\{v_1,v_2,\cdots,v_n\}$ os elementos de $V$ (o que faz sentido já que, por hipótese, $V$ tem uma quantidade finita de elementos). Tome então $u$ e $w$ em $\gen(V)$, e qualquer número real $\alpha$.
	
	Como $u$ é gerado por $V$, por definição existem $\{\lambda_1,\lambda_2,\cdots,\lambda_n\}\subseteq\R$ tais que 
	\[u=\lambda_1v_1+\lambda_2v_2+\cdots+\lambda_nv_n.\]Similarmente, como $w$ é gerado por $V$, por definição existem $\{\mu_1,\mu_2,\cdots,\mu_n\}\subseteq\R$ tais que
	\[w=\mu_1v_1+\mu_2v_2+\cdots\mu_nv_n.\]Isso nos diz que multiplicando $w$ por $\alpha$ obtemos
	\[\alpha w=\alpha(\mu_1v_1+\mu_2v_2+\cdots\mu_nv_n)=(\alpha\mu_1)v_1+(\alpha\mu_2)v_2+\cdots(\alpha\mu_n)v_n\]o que nos diz que $\alpha w\in\gen(V)$, ou seja, $\gen(V)$ é fechado por multiplicação por escalares.
	
	Por outro lado, somando $u$ e $w$ obtemos
	\[u+w=(\lambda_1v_1+\lambda_2v_2+\cdots+\lambda_nv_n)+(\mu_1v_1+\mu_2v_2+\cdots\mu_nv_n)=(\lambda_1+\mu_1)v_1+(\lambda_2+\mu_2)v_2+\cdots+(\lambda_n+\mu_n)v_n\]o que nos diz que $u+w\in\gen(V)$, ou seja, $\gen(V)$ é fechado por somas.
	
	Juntando as duas conclusões, vemos que $\gen(V)$ é um subespaço, como queríamos mostrar.
\end{proof}

Com isso, então, vamos passar a classificar os subespaços de $\R^2$:

\begin{prop}
	O conjunto $\{(0,0)\}$ é um subespaço de $\R^2$.
\end{prop}
\begin{proof}
	Claramente $(0,0)+(0,0)=(0,0)$ e $\lambda(0,0)=(0,0)$ para qualquer $\lambda\in \R$. Segue que $\{(0,0)\}$ é fechado por somas e produtos por escalares.
\end{proof}
\begin{cor}
	$\{(0,0)\}$ é o único subespaço de $\R^2$ com apenas um ponto.
\end{cor}
\begin{proof}
	Já vimos que todo subespaço contém a origem. Então se um subespaço contém apenas um ponto, esse ponto é a origem, como queríamos mostrar.
\end{proof}

\begin{rmk}
	Muitas vezes vamos denotar o conjunto $\{(0,0)\}$ e o vetor $(0,0)$ simplesmente por $0$, quando não houver ambiguidade.
\end{rmk}

\begin{ex}
	Com o resultado acima, então, vemos que nossos subespaços sempre têm pelo menos um ponto. Será que é possível que um subespaço tenha exatamente dois pontos?
	
	Bom, se o subespaço $S$ tem dois pontos $v$ e $u$, ou $v$ ou $u$ é diferente de $(0,0)$. Suponha (sem perda de generalidade) que $v\neq(0,0)$. Nesse caso, como $S$ é subespaço, $2v\in S$. Mas como $S$ só tem dois pontos, ou $2v=u$, ou $2v=v$.
	
	Se $2v=v$, então subtraindo $v$ dos dois lados obtemos $v=(0,0)$, o que é absurdo, porque nós escolhemos $v\neq(0,0)$ acima. Então $2v$ tem que ser $u$.
	
	Mas como $v\neq(0,0)$, necessariamente $u=(0,0)$ e, portanto, $2v=(0,0)$. Mas isso nos diz, novamente, que $v=(0,0)$ que, como já vimos, contraria o fato de $v\neq(0,0)$.
	
	Ou seja, se nós fôssemos capazes
	 de construir um subespaço com exatamente dois pontos, nós seríamos capazes de mostrar que $0\neq 0$, o que é um absurdo! A única conclusão possível, então, é que \textit{não somos capazes de construir um subespaço com exatamente dois pontos}.
\end{ex}

Na verdade, o que fizemos acima foi uma prova do seguinte resultado:
\begin{prop}
	Seja $S\subseteq\R^2$. Se $S\neq 0$, então $S$ tem infinitos pontos.
\end{prop}
\begin{proof}
	Se $S\neq 0$, existe $v\in S$ diferente de $(0,0)$. Como $S$ é subespaço, $\lambda v\in S$ para todo $\lambda\in \R$. Como $\lambda v\neq \mu v$ se $\lambda\neq \mu$, temos que para cada número real temos um vetor diferente, e como existem infinitos números reais temos infinitos vetores em $S$.
\end{proof}

Isso nos dá uma intuição do próximo resultado:
\begin{prop}
	Toda reta passando pela origem é um subespaço gerado por um único vetor não-nulo, e todo subespaço gerado por um único vetor não-nulo é uma reta passando pela origem.
\end{prop}
\begin{proof}
	Tome $r$ uma reta passando pela origem. Então $r=\{(x,y)\in\R^2\mid ax+by=0\}$ para algum par de números reais $a,b\in \R$ não ambos nulos. Afirmamos que $r=\gen(v)$, em que $v=(-b,a)$.
	
	Para mostrar isso, vamos mostrar que todo ponto da reta $r$ é gerado por $v$, e que todo ponto gerado por $v$ está na reta $r$.
	
	Tome $u=\lambda v$ para algum $\lambda\in \R$, ou seja, $u=(-\lambda b,\lambda a)$. Então computando
	\[a(-\lambda b)+b(\lambda a)=-\lambda a b+\lambda ab=0\]vemos que $u\in r$.
	
	Por outro lado, tome $P\in r$, ou seja, $P=(p,q)$ com $ap+bq=0$. Como $r$ é reta, $a\neq0$ ou $b\neq 0$.
	
	Se $a\neq0$, então podemos reescrever $ap+bq=0$ como $p=\dfrac{-b}{a}q$ e, portanto, $P=(p,q)=\left(\dfrac{-b}{a}q,q\right)$. Disso temos que \(P=q\left(\dfrac{-b}{a},1\right)\) e, finalmente, $aP=q(-b,a)=qv$. Novamente, como $a\neq 0$, isso nos diz que $P=\dfrac{q}{a}v$, donde vemos que $P\in \gen(v)$.
	
	Similarmente se $b\neq 0$ podemos mostrar que $P=\dfrac{p}{b}v$, e chegar na mesma conclusão.
	
	Segue que $r=\gen v$, como queríamos mostrar.
\end{proof}

O próximo passo, a essa altura, seria mostrar que $\R^2$ é gerado por dois vetores não-nulos e que qualquer subespaço gerado por dois vetores não-nulos é $\R^2$. A primeira dessas afirmações é fácil: $\R^2$ é gerado por $\{(1,0),(0,1)\}$ que são dois vetores não-nulos. Contudo, a segunda afirmação é \textit{falsa}:

\begin{ex}
	Considere o conjunto $V=\{(1,1),(2,2)\}\subseteq\R^2$. Quem é $\gen(V)$?
	
	Por definição, são os vetores $v\in \R^2$ tais que existem números reais $\lambda_1,\lambda_2\in\R$ tais que \[v=\lambda_1(1,1)+\lambda_2(2,2).\]
	
	Primeira afirmação: $\R^2\neq \gen(V)$. Para ver isso, basta notar que o ponto $(1,0)$ não é gerado por $V$. Dito de outra maneira, suponha que existem números reais $x,y$ tais que $(1,0)=x(1,1)+y(2,2)$. Em particular, comparando as coordenadas, teríamos que $1=x+2y$ e $0=x+2y$. Disso, poderíamos concluir que $1=x+2y=0$, ou seja, $1=0$, um absurdo! Portanto não podem existir $x,y$ tais que $(1,0)=x(1,1)+y(2,2)$, ou seja, $(1,0)\notin\gen(V)$ e, portanto, $\R^2\neq \gen (V)$.
	
	Segunda afirmação: $\gen(V)=\gen((1,1))$ (e, portanto, uma reta, pois $(1,1)\neq 0$). Isso é mais simples: Tome qualquer $v\in \gen(V)$ e escreva $v=\lambda_1(1,1)+\lambda_2(2,2)$. Agora note que $(2,2)=2(1,1)$, e podemos reescrever a expressão anterior de $v$ como
	\[v=\lambda_1(1,1)+\lambda_2\cdot 2(1,1)=(\lambda_1+2\lambda_2)(1,1),\]ou seja, $v\in\gen ((1,1))$. 
	
	Por outro lado, dado qualquer $u\in\gen((1,1))$, escreva $u=\lambda (1,1)=\lambda(1,1)+0(2,2)$, logo $u\in\gen(V)$.
	
	Disso concluímos que $\gen(V)$ é uma reta, e certamente não pode ser o plano todo.
\end{ex}

Precisamos então de criar um critério para saber quando um conjunto com dois vetores gera ou não o plano todo.

\begin{df}
	Dada uma coleção finita de vetores não-nulos $V=\{v_1,v_2,\cdots,v_n\}$ diremos que ela é \textbf{linearmente dependente} se $v_i\in\gen(V-\{v_i\})$ para algum $i\in\{1,2,\cdots,n\}$, ou seja $V$ é linearmente dependente se algum dos vetores $v_i$ for gerado pelos outros.
	
	Similarmente, se a coleção $V$ não for linearmente dependente, diremos que $V$ é \textbf{linearmente independete}, ou seja, nenhum dos vetores $v_i$ é gerado pelos outros.
\end{df}

\begin{rmk}
	Vamos usar as abreviações \textbf{l.d.} e \textbf{l.i.} para linearmente dependente e linearmente independente, respectivamente.
\end{rmk}
\begin{ex}
	O exemplo que demos acima, com $V=\{(1,1),(2,2)\}$ é um exemplo de uma coleção l.d.: O vetor $(2,2)$ é gerado pelo vetor $(1,1)$.
	
	\tcblower
	A coleção $\{(1,0),(0,1),(2,3)\}$ é l.d. pois o vetor $(2,3)$ é gerado pela coleção $\{(1,0),(0,1)\}$.
	
	Por outro lado, a coleção $\{(1,0),(0,1)\}$ é l.i. pois $(1,0)$ não gera nem é gerado por $(0,1)$.
\end{ex}

\begin{rmk}
	Qualquer coleção unitária (i.e. com um só vetor não-nulo) é l.i. Por definição, considere a coleção $V=\{v\}$. Para essa coleção ser l.d. deveria haver algum vetor (diferente de $v$) em $V$ que gerasse $v$. Mas $V$ não tem nenhum vetor diferente de $v$, então não tem nenhum vetor diferente de $v$ que gere $v$ e, portanto, $V$ é l.i.
\end{rmk}

\begin{lemma}\label{lem:ld reduz pra li}
	Toda coleção finita de vetores l.d. pode ser reduzida em uma coleção l.i.
\end{lemma}
\begin{proof}
	Se a coleção $V$ é finita, ela tem uma quantidade de elementos, $n$. Como a coleção é l.d., existe algum vetor $v_1$ que é gerado pelos outros.
	
	Considere a coleção $V-\{v_1\}$. Ela tem $n-1$ elementos. Se for l.i., acabamos. Se não, existe algum vetor $v_2$ que é gerado pelos outros.
	
	Considere a coleção $V-\{v_1,v_2\}$. Ela tem $n-2$ elementos. Se for l.i., acabamos. Se não, existe algum vetor $v_3$ que é gerado pelos outros.
	
	E assim por diante.
	
	Contudo, já vimos que conjuntos unitários são l.i., então, no pior dos casos, vamos ter que remover $n-1$ vetores de $V$, ficando com o conjunto $V-\{v_1,v_2,\cdots,v_{n-1}\}=\{v_n\}$ que é l.i., como queríamos mostrar.
\end{proof}
\begin{lemma}\label{lem:ld sub li gen}
	Seja $V$ conjunto finito de vetores l.i. e $u\in\R^2$ vetor tal que $V\cup\{u\}$ é l.d. Então $\gen (V)=\gen(V\cup\{u\})$. 
\end{lemma}
\begin{proof}
	Tome qualquer $w\in\gen(V\cup\{u\})$. Vamos mostrar que $w\in\gen(V)$. 
	
	Se $V$ é l.i. com $n$ elementos e $V\cup \{u\}$ é l.d. temos duas possibilidades:
	\begin{enumerate}[1)]
		\item $u$ é gerado por $V$, ou;
		\item algum elemento de $V$ é gerado pelos outros elementos de $V$ e $\{u\}$.
		
		Nesse caso, vamos chamar esse elemento de $v_1$. Então, existem números reais $\{\lambda_2,\lambda_3,\cdots,\lambda_n,\mu\}$ tais que $$v_1=\lambda_2v_2+\lambda_3v_2+\cdots+\lambda_nv_n+\mu u.$$Se $\mu=0$, estaríamos dizendo que $v_1$ é gerado pelos elementos de $V$, o que contrariaria o fato de $V$ ser l.i. Então $\mu\neq 0$. Assim, podemos reescrever a expressão acima isolando $u$
		\[u=\frac{1}{\mu}v_1+\frac{-\lambda_2}{\mu}v_2+\cdots+\frac{-\lambda_n}{\mu}v_n\]e ver que $u\in\gen(V)$, ou seja, os dois casos são a mesma coisa.
		
		Agora, $w\in\gen(V\cup\{u\})$ implica que existem números reais $\{w_1,w_2,\cdots,w_n,w_u\}$ tais que
		\[w=w_1v_1+w_2v_2+\cdots+w_nv_n+w_uu.\] Mas como $u$ é gerado por $V$, existem $\{u_1,u_2,\cdots,u_n\}$ números reais tais que
		\[u=u_1v_1+u_2v_2+\cdots+u_nv_n\]e, substituindo acima, temos:
		\begin{align*}
			w&=w_1v_1+w_2v_2+\cdots+w_nv_n+w_uu\\
			&w_1v_1+w_2v_2+\cdots+w_nv_n+w_u(u_1v_1+u_2v_2+\cdots+u_nv_n)\\
			&(w_1+w_uu_1)v_1+(w_2+w_uu_2)v_2+\cdots+(w_n+w_uu_n)v_n,
		\end{align*}ou seja $w\in\gen(V)$, como queríamos mostrar.
	\end{enumerate}
\end{proof}
\begin{lemma}
	Se $V$ é finito e l.i., então $V-\{v\}$ também é l.i. para qualquer $v\in V$.
\end{lemma}
\begin{proof}
	Seja $V$ finito e l.i. $v\in V$. Em particular, $V-\{v\}$ também é finito. Seja $V-\{v\}=\{v_1,v_2,\cdots,v_n\}$. Suponha que $v_1$ é gerado pelos outros $v_i$, ou seja, existem números reais $\{\lambda_2,\lambda_3,\cdots,\lambda_n\}$ tais que $v_1=\lambda_2v_2+\lambda_3v_3+\cdots+\lambda_nv_n$. Em particular, poderíamos escrever $v_1=\lambda_2v_2+\lambda_3v_3+\cdots+\lambda_nv_n+0v$ e ver que $v_1$ já era gerado em $V$, o que contraria o fato de $V$ ser l.i. Segue que $v_1$ não pode ser gerado pelos outros $v_i$.
	
	Como temos um número finito de $v_i$, podemos repetir o mesmo raciocínio para eles e concluir que cada $v_j$ não podem ser gerados pelos outros $v_i$, ou seja, o conjunto $V-\{v\}$ é l.i., como queríamos mostrar.
\end{proof}

Finalmente, vamos enunciar o resultado que precisamos:

\begin{tcolorbox}[breakable,colback=red!5!white,colframe=red!80!white,title=\normalsize {\sc AVISO!},coltitle=black]
	A demonstração abaixo é bastante técnica.
	
	Não se preocupe em entendê-la por agora. Leia o enunciado do teorema e certifique-se de que entendeu.
	
	Se tiver interesse, por favor, leia e tente acompanhar a prova, mas não tenha vergonha em passar para frente caso tenha dificuldade.
\end{tcolorbox}
\begin{theorem}\label{thm:gen li mesmo tamanho}
	Sejam $V$ e $W$ dois conjuntos finitos de geradores l.i. para algum subespaço $S$. Então $\#V=\#W$.
\end{theorem}
\begin{proof}
	Como $V$ e $W$ são finitos, vamos dizer que $\#V=n$ e $\#W=m$. Escreva então
	\[V=\{v_1,v_2,\cdots,v_n\},\quad W=\{w_1,w_2,\cdots,w_m\}\]os elementos de $V$ e $W$. Como $V$ gera $S$ e $W\subseteq S$, podemos escrever cada $w_i\in W$ como
	\[w_i=\lambda_{i,1}v_1+\lambda_{i,2}v_2+\cdots+\lambda_{i,n}v_n.\] Dito de outra maneira, temos uma matriz $m\times n$
	\[M=\begin{pmatrix}
	\lambda_{1,1}&\lambda_{1,2}&\cdots&\lambda_{1,n}\\
	\lambda_{2,1}&\lambda_{2,2}&\cdots&\lambda_{2,n}\\
	\vdots&\vdots&\ddots&\vdots\\
	\lambda_{m,1}&\lambda_{m,2}&\cdots&\lambda_{m,n}
	\end{pmatrix}\]tal que
	\[\begin{pmatrix}
	\lambda_{1,1}&\lambda_{1,2}&\cdots&\lambda_{1,n}\\
	\lambda_{2,1}&\lambda_{2,2}&\cdots&\lambda_{2,n}\\
	\vdots&\vdots&\ddots&\vdots\\
	\lambda_{m,1}&\lambda_{m,2}&\cdots&\lambda_{m,n}
	\end{pmatrix}\begin{pmatrix}
	v_1\\v_2\\\vdots\\v_n
	\end{pmatrix}=\begin{pmatrix}
	w_1\\w_2\\\vdots\\w_m
	\end{pmatrix}.\]
	
	Analogamente, como $W$ gera $S$ e $V\subseteq S$, podemos escrever cada $v_i\in V$ como
	\[v_i=\mu_{i,1}w_1+\mu_{i,2}w_2+\cdots+\mu_{i,m}w_m.\]Dito de outra maneira, temos uma matriz $n\times m$
	\[N=\begin{pmatrix}
	\mu_{1,1}&\mu_{1,2}&\cdots&\mu_{1,m}\\
	\mu_{2,1}&\mu_{2,2}&\cdots&\mu_{2,m}\\
	\vdots&\vdots&\ddots&\vdots\\
	\mu_{n,1}&\mu_{n,2}&\cdots&\mu_{n,m}
	\end{pmatrix}\]tal que
	\[\begin{pmatrix}
	\mu_{1,1}&\mu_{1,2}&\cdots&\mu_{1,m}\\
	\mu_{2,1}&\mu_{2,2}&\cdots&\mu_{2,m}\\
	\vdots&\vdots&\ddots&\vdots\\
	\mu_{n,1}&\mu_{n,2}&\cdots&\mu_{n,m}
	\end{pmatrix}\begin{pmatrix}
	w_1\\w_2\\\vdots\\w_m
	\end{pmatrix}=\begin{pmatrix}
	v_1\\v_2\\\vdots\\v_n
	\end{pmatrix}.\]
	
	Substituindo a expressão obtida acima para a matriz $\begin{pmatrix}
	v_1\\v_2\\\vdots\\v_n
	\end{pmatrix}$ na equação anterior, temos:
	\[\begin{pmatrix}
	\lambda_{1,1}&\lambda_{1,2}&\cdots&\lambda_{1,n}\\
	\lambda_{2,1}&\lambda_{2,2}&\cdots&\lambda_{2,n}\\
	\vdots&\vdots&\ddots&\vdots\\
	\lambda_{m,1}&\lambda_{m,2}&\cdots&\lambda_{m,n}
	\end{pmatrix}\begin{pmatrix}
	\mu_{1,1}&\mu_{1,2}&\cdots&\mu_{1,m}\\
	\mu_{2,1}&\mu_{2,2}&\cdots&\mu_{2,m}\\
	\vdots&\vdots&\ddots&\vdots\\
	\mu_{n,1}&\mu_{n,2}&\cdots&\mu_{n,m}
	\end{pmatrix}\begin{pmatrix}
	w_1\\w_2\\\vdots\\w_m
	\end{pmatrix}=\begin{pmatrix}
	w_1\\w_2\\\vdots\\w_m
	\end{pmatrix}.\] Vamos reescrever a matriz $MN$ como
	\[MN=\begin{pmatrix}
	a_{1,1}&a_{1,2}&\cdots&a_{1,m}\\
	a_{2,1}&a_{2,2}&\cdots&a_{2,m}\\
	\vdots&\vdots&\ddots&\vdots\\
	a_{m,1}&a_{m,2}&\cdots&a_{m,m}
	\end{pmatrix}.\] Então podemos reescrever a equação matricial anterior como
	\[\begin{pmatrix}
	a_{1,1}&a_{1,2}&\cdots&a_{1,m}\\
	a_{2,1}&a_{2,2}&\cdots&a_{2,m}\\
	\vdots&\vdots&\ddots&\vdots\\
	a_{m,1}&a_{m,2}&\cdots&a_{m,m}
\end{pmatrix}\begin{pmatrix}
w_1\\w_2\\\vdots\\w_m
\end{pmatrix}=\begin{pmatrix}
w_1\\w_2\\\vdots\\w_m
\end{pmatrix}\]e obter as equações $w_i=a_{i,1}w_1+a_{i,2}w_2+\cdots+a_{i,m}w_m$, ou seja, $(1-a_{i,i})w_i=a_{i,1}w_1+a_{i,2}w_2+\cdots+a_{i,i-1}w_{i-1}+a_{i,i+1}w_{i+1}+\cdots+a_{i,m}w_m$. Se $1-a_{i,i}\neq 0$, podemos dividir ambos os lados por $1-a_{i,i}$ e concluir que $w_i$ é combinação linear dos outros elementos de $W$. 

Mas $W$ é l.i., o que nos levaria a um absurdo. Então $1-a_{i,i}=0$ e, portanto, $a_{i,i}=1$ para todos os valores de $i$, ou seja, a diagonal de $MN$ é composta de $1$s.

Contudo, as equações acima nos garantem que $(1-a_{i,i})w_i=a_{i,1}w_1+a_{i,2}w_2+\cdots+a_{i,i-1}w_{i-1}+a_{i,i+1}w_{i+1}+\cdots+a_{i,m}w_m$, e como já sabemos que $a_{i,i}=1$, temos que $a_{i,1}w_1+a_{i,2}w_2+\cdots+a_{i,i-1}w_{i-1}+a_{i,i+1}w_{i+1}+\cdots+a_{i,m}w_m=0$. 

Se algum dos $a_{i,j}$ for diferente de 0, seríamos capazes de isolar $w_j$ e escrever ele como combinação linear dos outros elementos de $W$. Mas $W$ é l.i., o que nos diz que isso é impossível - ou seja, $a_{i,j}=0$ sempre que $i\neq j$. Isso nos diz que nossa matriz $MN$ tem 0 em todo elemento fora da diagonal.

Juntando essas duas conclusões, vemos que $MN=I_m$, a matriz identidade $m\times m$.

Um raciocínio análogo nos permite concluir que $NM=I_n$, a matriz identidade $n\times n$.

Mas uma matriz possui inverso bilateral se, e somente se, a matriz é quadrada - segue que tanto $M$ quanto $N$ são quadradas, ou seja, $n=m$, como queríamos mostrar. 
\end{proof}

O que esse teorema nos garante é que sempre que nós escolhermos uma coleção l.i. de geradores de algum subespaço, seja ela qual for, ela vai ter sempre a mesma quantidade de elementos. Isso nos permite fazer a seguinte definição:

\begin{df}
	Seja $S$ um subespaço de $\R^2$. Definimos como \textbf{base} qualquer conjunto de vetores l.i. que geram $S$.
	
	Analogamente, definimos a \textbf{dimensão} de $S$ como sendo o número natural $\dim S$ definido por $$\dim S:=\#B,$$ em que $B$ é qualquer base de $S$.
\end{df}
\begin{ex}
	O subespaço $\{(0,0)\}$ só tem um vetor - $(0,0)$. Por definição, esse conjunto não é l.i. apesar de ser gerador. Assim, esse subespaço não possui base e, portanto, possui dimensão 0.
	
	\bigskip
	Por outro lado, considere o subespaço gerado pelo vetor $(2,3)$, que já vimos ser uma reta que passa pela origem e por $(2,3)$. Claramente o conjunto $\{(2,3)\}$ é l.i. (pois é unitário) e gera esse subespaço (por definição) - ou seja, é uma base. Segue que a reta que passa pela origem e pelo ponto $(2,3)$ tem dimensão 1.
	
	Não é difícil ver que qualquer reta que passa pela origem é um subespaço de dimensão 1.
	
	\bigskip
	Finalmente, considere $\R^2$. Já sabemos que $\R^2$ é gerado por $\{(1,0),(0,1)\}$ e que esse conjunto é l.i. - ou seja, uma base de $\R^2$. Segue que $\dim\R^2=2$.
	
	\bigskip
	Olha que interessante: Nós mostramos que subespaços gerados ou têm dimensão 0 (o subespaço 0), ou têm dimensão 1 (as retas pela origem), ou têm dimensão 2 (o plano todo).
\end{ex}

Para finalizar, mais alguns resultados técnicos.

\begin{lemma}
	Dada uma coleção finita de conjuntos não-nulos $V$ e um subespaço $S$ de dimensão $n$, as seguintes afirmações são equivalentes:
	\begin{enumerate}[(a)]
		\item $V$ é uma base para $S$;
		\item $V$ é l.i. e tem $n$ elementos;
		\item $V$ gera $S$ e tem $n$ elementos.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Claramente (a) implica tanto (b) quanto (c).
	
	Suponha (b), ou seja, que $V$ é l.i. e tem $n$ elementos. Se $V$ não gera, existe algum elemento $v\in S$ tal que $v\notin\gen(V)$ - em particular, o conjunto $V\cup\{v\}$ seria l.i. e teria $n+1$ elementos. 
	
	Isso é um absurdo, pois se $V\cup\{v\}$ gera $S$, temos então um conjunto de geradores l.i. com mais de $n$ elementos. Por outro lado, se $V\cap\{v\}$ não gera, isso é pior ainda - significa que precisamos adicionar ainda mais gente para gerar $S$, e no final vamos ficar com muito mais do que $n$ elementos.
	
	Como isso não pode acontecer, $V$ tem que já gerar $S$, ou seja, supondo (b) concluímos (a) e (c).
	
	\bigskip
	Analogamente, suponha (c), ou seja, $V$ gera e tem $n$ elementos. Se $V$ não é l.i., ou $V=\{0,0\}$ ou $V$ é l.d. Como estamos assumindo, por hipótese, que $V$ contém apenas vetores não nulos, podemos descartar o primeiro caso.
	
	Como $V$ é l.d., podemos reduzir $V$ até obter um conjunto $V'$ l.i. tal que $\gen(V)=\gen(V')$ (pelos lemas \ref{lem:ld reduz pra li} e \ref{lem:ld sub li gen}). Como $\gen(V)=S$, por hipótese, temos que $V'$ é um conjunto gerador l.i. com menos elementos do que $n$ - o que contraria o \Cref{thm:gen li mesmo tamanho}, um absurdo.
	
	Segue que $V$ é l.i., ou seja, (c) implica (a) e (b), o que encerra a demonstração.

\end{proof}

Finalmente, vamos mostrar que todo subespaço é gerado:
\begin{prop}
	Seja $S$ subespaço de $\R^2$. Então existe $V\subseteq S$ conjunto de geradores de $S$ - ou seja, $\gen(V)=S$.
\end{prop}
\begin{proof}
	Certamente $(0,0)\in S$. Se $S-\{(0,0)\}=\varnothing$, acabou: $S=0$ e portanto é gerado por $(0,0)$.
	
	Caso contrário, existe algum $v\in S$ diferente de $(0,0)$. Então $\gen(v)\subseteq S$, já que $S$ é subespaço. Se $S-\gen(v)=\varnothing$, acabou: mostramos que $S=\gen(v)$.
	
	Caso contrário, existe algum $w\in S$ diferente de $(0,0)$ e que não é gerado por $v$. Então $\gen({v,w})\subseteq S$, já que $S$ é subespaço.
	
	Contudo, como $\{v,w\}$ foi construído l.i., o lema acima nos garante que $\{v,w\}$ é base de $\R^2$, donde $\gen(\{v,w\})=\R^2$. Então temos $\R^2\subseteq S\subseteq\R^2$, ou seja, $S=\R^2$, e $S$ é, portanto, gerado por $\{v,w\}$, como queríamos mostrar.
\end{proof}

Juntando isso tudo, temos um grande corolário que encerra esta seção:
\begin{prop}
	Todos os subespaços de $\R^2$ são: $0$, retas que passam pela origem e $\R^2$.
\end{prop}
\begin{proof}
	Já vimos, no exemplo, que todo subespaço gerado tem dimensão 0, 1 ou 2. Contudo, a proposição acima nos diz que todo subespaço é gerado. O resultado se segue.
\end{proof}
\begin{exerc}
	Mostre que a dimensão do núcleo de uma transformação linear é exatamente o número de linhas nulas na forma escalonada da matriz dessa transformação linear.
\end{exerc}
\begin{exerc}
	Mostre que a dimensão da imagem de uma transformação linear é exatamente o número de linhas não-nulas na forma escalonada da matriz dessa transformação.
\end{exerc}
\begin{exerc}
	Junte os dois exercícios acima para concluir que dada uma transformação linear $f:\R^2\to\R^2$, temos que $\dim\Ker f+\dim\im f=\dim\R^2=2$.
\end{exerc}

\section{Produtos interno e vetorial}

\subsection{Projeção ortogonal}

Em muitos problemas de física é interessante decompor um vetor não como tendo uma coordenada $X$ e uma coordenada $Y$, mas em termos de outras coordenadas arbitrárias.

\begin{ex}
	Considere os vetores abaixo:
	\[
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	xmin=-2,
	xmax=7,
	ymin=-1,
	ymax=6,
	xtick={-1.0,0.0,...,6.0},
	ytick={-1.0,0.0,...,5.0},]
	\clip(-1.84,-1.54) rectangle (6.94,6.04);
	\draw [->] (0.,0.) -- (6.,3.);
	\draw [->] (0.,0.) -- (2.,5.);
%	\draw [->] (0.,0.) -- (-1.,2.);
	\begin{scriptsize}
	\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	\draw[color=black] (3.24,1.45) node {$u$};
	\draw[color=black] (1.28,2.61) node {$v$};
%	\draw[color=black] (-0.64,0.85) node {$w$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]em que $u=(6,3)$ e $v=(2,5)$.
	
	 A \textbf{projeção ortogonal de $v$ em $u$} é o vetor $\pi_{v,u}$ representado abaixo:
	 \[
	 \definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	 \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	 \begin{axis}[
	 x=1.0cm,y=1.0cm,
	 axis lines=middle,
	 xmin=-2,
	 xmax=7,
	 ymin=-1,
	 ymax=6,
	 xtick={-1.0,0.0,...,6.0},
	 ytick={-1.0,0.0,...,5.0},]
	 \clip(-1.84,-1.54) rectangle (6.94,6.04);
	 \draw [->] (0.,0.) -- (6.,3.);
	 \draw [->] (0.,0.) -- (2.,5.);
%	 \draw [->] (0.,0.) -- (-1.,2.);	 
	 \draw [dash pattern=on 3pt off 3pt] (3.608,1.804)-- (2.,5.);
	 \draw [->,color=red] (0.,0.) -- (3.608,1.804);
	 \begin{scriptsize}
	 \draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	 \draw[color=black] (5,2) node {$u$};
	 \draw[color=black] (1.28,2.61) node {$v$};
%	 \draw[color=black] (-0.64,0.85) node {$w$};	 
	 \draw[color=black] (1.96,0.5) node[color=red] {$\pi_{v,u}$};
	 \end{scriptsize}
	 \end{axis}
	 \end{tikzpicture}\]Esse vetor tem a seguinte propriedade especial: Considere a reta ortogonal a $u$ que passa pela origem e qualquer vetor $w$ nela:
	 \[
	 \definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	 \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	 \begin{axis}[
	 x=1.0cm,y=1.0cm,
	 axis lines=middle,
	 xmin=-2,
	 xmax=7,
	 ymin=-1,
	 ymax=6,
	 xtick={-1.0,0.0,...,6.0},
	 ytick={-1.0,0.0,...,5.0},]
	 \clip(-1.84,-1.54) rectangle (6.94,6.04);
	 \draw [->] (0.,0.) -- (6.,3.);
%	 \draw [->] (0.,0.) -- (2.,5.);
	 \draw [->] (0.,0.) -- (-1.,2.);	 
%	 \draw [dash pattern=on 3pt off 3pt] (3.608,1.804)-- (2.,5.);
	 \draw [dotted,domain=-5.98:15.46] plot(\x,{(-0.--6.*\x)/-3.});
%	 \draw [->,color=red] (0.,0.) -- (3.608,1.804);
	 \begin{scriptsize}
	 \draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	 \draw[color=black] (5,2) node {$u$};
%	 \draw[color=black] (1.28,2.61) node {$v$};
	 	 \draw[color=black] (-0.64,0.85) node {$w$};	 
%	 \draw[color=black] (1.96,0.5) node[color=red] {$\pi_{v,u}$};
	 \end{scriptsize}
	 \end{axis}
	 \end{tikzpicture}\]nesse caso, $w=(-1,2)$. Como $u\perp w$, claramente o conjunto $\{u,w\}$ é l.i. e, portanto, gera $\R^2$ - em particular, existem $\lambda_1,\lambda_2$ em $\R$ tais que $v=\lambda_1u+\lambda_2w$, como vemos abaixo:
	 \[
	 \definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	 \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	 \begin{axis}[
	 x=1.0cm,y=1.0cm,
	 axis lines=middle,
	 xmin=-2,
	 xmax=7,
	 ymin=-1,
	 ymax=6,
	 xtick={-1.0,0.0,...,6.0},
	 ytick={-1.0,0.0,...,5.0},]
	 \clip(-1.84,-1.54) rectangle (6.94,6.04);
	 \draw [->] (0.,0.) -- (6.,3.);
	 \draw [->] (0.,0.) -- (2.,5.);
	 \draw [->] (0.,0.) -- (-1.,2.);	 
	 \draw [dash pattern=on 3pt off 3pt] (3.608,1.804)-- (2.,5.);
	 \draw [dash pattern=on 3pt off 3pt] (-1.6,3.2)-- (2.,5.);
	 \draw [dotted,domain=-5.98:15.46] plot(\x,{(-0.--6.*\x)/-3.});
	 \draw [->,color=red] (0.,0.) -- (3.608,1.804);	 
	 \draw [->,color=blue] (0.,0.) -- (-1.6,3.2);
	 \begin{scriptsize}
	 \draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	 \draw[color=black] (5,2) node {$u$};
	 \draw[color=black] (1.28,2.61) node {$v$};
	 \draw[color=black] (-0.64,0.85) node {$w$};	 
	 \draw[color=black] (1.96,0.5) node[color=red] {$\lambda_1u$};
	 \draw[color=black] (-.8,2.5) node[color=blue] {$\lambda_2w$};
	 \end{scriptsize}
	 \end{axis}
	 \end{tikzpicture}\]Assim, a projeção ortogonal de $v$ em $u$ é definida como sendo $\pi_{v,u}:=\lambda_1u$, e a projeção ortogonal de $v$ em $w$ é definida como sendo $\pi_{v,w}:=\lambda_2w$.
	 \tcblower
	 Para calcular essas projeções, então, precisamos encontrar $\lambda_1$ e $\lambda_2$, ou seja, resolver a equação
	 \[(2,5)=\lambda_1(6,3)+\lambda_2(-1,2)\]que se traduz nas equações $2=6\lambda_1-\lambda_2$ e $5=3\lambda_1+2\lambda_2$. Ora, já sabemos que podemos representar isso por uma equação matricial
	 \[\begin{pmatrix}
	 6&-1\\3&2
	 \end{pmatrix}\begin{pmatrix}
	 \lambda_1\\\lambda_2
	 \end{pmatrix}=\begin{pmatrix}
	 2\\5
	 \end{pmatrix},\]ou seja, encontrar as projeções equivale a resolver o sistema linear acima.
	 
	 Vamos lá!
	 \begin{align*}
	 	\begin{augmatrix}{cc:c}
	 	6&-1&2\\3&2&5
	 	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
	 	6&-1&2\\0&5&8
	 	\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
 	30&0&18\\0&5&8
 \end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
 5&0&3\\0&5&8
 \end{augmatrix}
	 \end{align*}então nosso sistema tem solução única $5\lambda_1=3$ e $5\lambda_2=8$, ou seja, $\lambda_1=3/5$ e $\lambda_2=8/5$ (de fato, $6(3/5)-8/5=18/5-8/5=10/5=2$ e $3(3/5)+2(8/5)=9/5+16/5=25/5=5$). Com isso, então, podemos ver que $\pi_{v,u}=\lambda_1u=3/5(6,3)$ e $\pi_{v,w}=\lambda_2w=8/5(-1,2)$.
\end{ex}

\subsection{Produto interno}

\begin{ex}
	Continuando o exemplo acima, vamos calcular a projeção ortogonal de um vetor arbitrário $v=(v_1,v_2)$ em um vetor arbitrário $u=(u_1,u_2)$. Nesse caso, vamos usar $w=u^{\perp}=(-u_2,u_1)$ - isso pode ser visto notando que $\tan \theta=-\tan (\theta+90^\circ)$ - como podemos ver no desenho abaixo:
	
	\[\definecolor{zzttqq}{rgb}{0.6,0.2,0.}
	\definecolor{qqwuqq}{rgb}{0.,0.39215686274509803,0.}
	\definecolor{ffqqqq}{rgb}{1.,0.,0.}
	\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\begin{axis}[
	x=1.0cm,y=1.0cm,
	axis lines=middle,
	ymajorgrids=true,
	xmajorgrids=true,
	xmin=-3.9800000000000004,
	xmax=6.980000000000004,
	ymin=-1.0200000000000038,
	ymax=6.9799999999999995,
	xtick={-3.0,-2.0,...,6.0},
	ytick={-1.0,0.0,...,6.0},]
	\clip(-3.98,-1.02) rectangle (6.98,6.98);
	\draw[color=ffqqqq,fill=ffqqqq,fill opacity=0.10000000149011612] (0.3794733192202056,0.1897366596101028) -- (0.18973665961010283,0.5692099788303084) -- (-0.1897366596101028,0.3794733192202056) -- (0.,0.) -- cycle; 
	\draw [color=qqwuqq,fill=qqwuqq,fill opacity=0.10000000149011612] (0,0) -- (0.:0.6) arc (0.:26.56505117707799:0.6) -- cycle;
	\draw [color=qqwuqq,fill=qqwuqq,fill opacity=0.10000000149011612] (0,0) -- (0.:1.1) arc (0.:116.56505117707799:1.1) -- cycle;
	\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (6.,3.) -- (6.,0.) -- (0.,0.) -- cycle;
	\fill[color=zzttqq,fill=zzttqq,fill opacity=0.10000000149011612] (-3.,6.) -- (-3.,0.) -- (0.,0.) -- cycle;
	\draw [shift={(2.02,2.)},color=qqwuqq,fill=qqwuqq,fill opacity=0.10000000149011612] (0,0) -- (-153.43494882292202:0.6) arc (-153.43494882292202:-90.:0.6) -- cycle;
	\draw [shift={(-6.98,5.)},color=qqwuqq,fill=qqwuqq,fill opacity=0.10000000149011612] (0,0) -- (-90.:0.6) arc (-90.:-63.43494882292201:0.6) -- cycle;
	\draw [color=qqwuqq,fill=qqwuqq,fill opacity=0.10000000149011612] (0,0) -- (116.56505117707799:0.6) arc (116.56505117707799:180.:0.6) -- cycle;
	\draw [->] (0.,0.) -- (6.,3.);
	\draw [dash pattern=on 2pt off 2pt,domain=-3.98:6.98] plot(\x,{(-0.--6.*\x)/-3.});
	\draw [->] (0.,0.) -- (-3.,6.);
%	\draw [shift={(0.,0.)},color=qqwuqq] (0.:0.6) arc (0.:26.56505117707799:0.6);
	\draw[color=qqwuqq] (0.510955719470559,0.12062028328736464) -- (0.6569430678907181,0.1550832213694685);
	\draw [color=zzttqq] (6.,3.)-- (6.,0.);
	\draw [color=zzttqq] (6.,0.)-- (0.,0.);
	\draw [color=zzttqq] (0.,0.)-- (6.,3.);
	\draw [color=zzttqq] (-3.,6.)-- (-3.,0.);
	\draw [color=zzttqq] (-3.,0.)-- (0.,0.);
	\draw [color=zzttqq] (0.,0.)-- (-3.,6.);
%	\draw [shift={(6.,3.)},color=qqwuqq] (-153.43494882292202:0.6) arc (-153.43494882292202:-90.:0.6);
	\draw[color=qqwuqq] (5.775024455910556,2.5256467512900755) -- (5.710745729027857,2.3901172516586686);
	\draw[color=qqwuqq] (5.676337678186497,2.586638534163751) -- (5.583862729096923,2.4685352582105358);
%	\draw [shift={(-3.,6.)},color=qqwuqq] (-90.:0.6) arc (-90.:-63.43494882292201:0.6);
	\draw[color=qqwuqq] (-2.879379716712635,5.489044280529442) -- (-2.8449167786305316,5.343056932109282);
%	\draw [shift={(0.,0.)},color=qqwuqq] (116.56505117707799:0.6) arc (116.56505117707799:180.:0.6);
	\draw[color=qqwuqq] (-0.47435324870992435,0.22497554408944334) -- (-0.6098827483413313,0.28925427097214207);
	\draw[color=qqwuqq] (-0.4133614658362491,0.3236623218135027) -- (-0.531464741789463,0.41613727090307534);
	\begin{scriptsize}
	\draw [fill=uuuuuu] (0.,0.) circle (2.0pt);
	\draw[color=black] (3.1,1.23) node {$u$};
	\draw[color=black] (-1.92,2.75) node {$w$};
	\end{scriptsize}
	\end{axis}
	\end{tikzpicture}\]Assim, vamos calcular $\pi_{v,u}$ como sendo igual ao $\lambda_1\in\R$ tal que $v=\lambda_1u+\lambda_2w$, ou seja,
	\begin{align*}
		&\begin{augmatrix}{cc:c}
		u_1&-u_2&v_1\\u_2&u_1&v_2
		\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
		1&-u_2/u_1&v_1/u_1\\u_2&u_1&v_2
		\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
		1&-u_2/u_1&v_1/u_1\\0&u_1-u_2(-u_2/u_1)&v_2-u_2(v_1/u_1)
		\end{augmatrix}\rightsquigarrow\\&
		\rightsquigarrow\begin{augmatrix}{cc:c}
		1&-u_2/u_1&v_1/u_1\\0&(u_1^1+u_2^2)/u_1&(u_1v_2-u_2v_1)/u_1
		\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
		1&-u_2/u_1&v_1/u_1\\0&u_1^1+u_2^2&u_1v_2-u_2v_1
		\end{augmatrix}\rightsquigarrow\\&\rightsquigarrow\begin{augmatrix}{cc:c}
		1&-u_2/u_1&v_1/u_1\\0&1&(u_1v_2-u_2v_1)/(u_1^2+u_2^2)
		\end{augmatrix}\rightsquigarrow\begin{augmatrix}{cc:c}
		1&0&v_1/u_1+(u_2/u_1)(u_1v_2-u_2v_1)/(u_1^2+u_2^2)\\0&1&(u_1v_2-u_2v_1)/(u_1^2+u_2^2)
		\end{augmatrix}
	\end{align*}então nosso $\lambda_1$ vale... $v_1/u_1+(u_2/u_1)(u_1v_2-u_2v_1)/(u_1^2+u_2^2)$?! Ok, sem pânico, vamos resolver isso:
	\begin{align*}
		v_1/u_1+(u_2/u_1)(u_1v_2-u_2v_1)/(u_1^2+u_2^2)&=((v_1/u_1)(u_1^2+u_2^2)+(u_2/u_1)(u_1v_2-u_2v_1))/(u_1^2+u_2^2)\\
		&=(v_1u_1+v_1u_2^2/u_1+u_2v_2-u_2^2v_1/u_1)/(u_1^2+u_2^2)\\
		&=(v_1u_1+v_2u_2)/(u_1^2+u_2^2)
	\end{align*}Pronto! Agora sabemos que $\lambda_1=(v_1u_1+v_2u_2)/(u_1^2+u_2^2)$. Se notarmos que o denominador é simplesmente $\lVert u\rVert^2$, podemos escrever uma expressão fechada para $\pi_{v,u}$:
	\[\pi_{v,u}=(v_1u_1+v_2u_2)/\lVert u\rVert^2.\]
	
	O que significa o termo $v_1u_1+v_2u_2$ que apareceu acima?
	
	\bigskip
	Se pensarmos novamente em vetores como matrizes coluna, não é difícil ver que $v_1u_1+v_2u_2=[v]^t[u]$, onde $^t$ representa a matriz transposta, ou seja:
	\[\begin{pmatrix}
	v_1 &v_2
	\end{pmatrix}\begin{pmatrix}
	u_1\\u_2
	\end{pmatrix}=\begin{pmatrix}
	v_1u_1+v_2u_2
	\end{pmatrix}.\]
	
	Ou seja, usando o conceito de vetores como matrizes nós fomos capazes de inventar uma multiplicação de vetores cujo resultado é um escalar!
\end{ex}

\begin{df}
	Dados dois vetores $v,u\in\R^2$, definimos o \textbf{produto interno (ou escalar) de $v$ e $u$} como sendo o número real $\inprod{v,u}\in\R$ dado por
	\[\inprod{v,u}:=[v]^t[u].\]
\end{df}

\begin{prop}
	Para quaisquer vetores $v,u,w\in\R^2$ e qualquer número real $\lambda\in\R$ valem as seguintes propriedes:
	\begin{itemize}
		\item (Linearidade à esquerda) $\inprod{v+\lambda u,w}=\inprod{v,w}+\lambda\inprod{u,w}$;
		\item (Linearidade à direita) $\inprod{v,u+\lambda w}=\inprod{v,u}+\lambda\inprod{v,w}$;
		\item (Comutatividade) $\inprod{v,u}=\inprod{u,v}$;
		\item (Não-degenerado) $\inprod{v,v}\geq 0$ e a igualdade acontece apenas no caso $v=0$.
	\end{itemize}
\end{prop}

\begin{lemma}
	Para qualquer vetor $v\in\R^2$, temos que $\inprod{v,v}=\lVert v\rVert^2$.
\end{lemma}

\begin{ex}
	Vamos voltar ao exemplo motivador do produto interno: Projeções ortogonais.
	
	Agora, com o produto interno, dados dois vetores $v,u\in\R^2$ podemos escrever a projeção de $v$ em $u$ como sendo $\pi_{v,u}=\inprod{v,u}/\inprod{u,u}$ - dito de outra maneira, $\inprod{v,u}=\pi_{v,u}\inprod{u,u}$.
	
	O que acontece então se escolhermos $v$ ortogonal a $u$, ou seja, $v=\lambda u^\perp$, para algum $\lambda\in \R$? Nesse caso, temos:
	\[\inprod{v,u}=\inprod{\lambda u^\perp,u}=\lambda\inprod{u^\perp,u}=\lambda(-u_2u_1+u_1u_2)=\lambda(0)=0\]ou seja, se $v\perp u$, temos que $\inprod{v,u}=0$ e, portanto, $\pi_{v,u}=0$ - o que condiz com nossa expectativa geométrica!
	
	Por outro lado, se temos algum vetor $w\in\R^2$ tal que $\inprod{w,u}=0$, então temos que $w_1u_1+w_2u_2=0$, ou seja, $w_1/w_2=-u_2/u_1$ e, portanto $w\perp u$ - geométricamente isso é óbvio: Se algum vetor tem projeção de tamanho 0 em $u$, esse vetor só pode ser ortogonal a $u$!
\end{ex}

\begin{lemma}
	Dados dois vetores $v,u\in\R^2$, temos que $v\perp u$ se, e somente se, $\inprod{v,u}=0$.
\end{lemma}

Isso nos permite, então, verificar ortogonalidade sem fazer desenhos, usando apenas coordenadas!

Para encerrar esta subseção, vamos dar um aplicação interessante:
\begin{ex}
	Já vimos que uma reta em $\R^2$ pode ser descrita de duas maneiras distintas: como todas as soluções de uma equação do tipo $ax+by=0$ ou como todos os múltiplos de algum vetor $u$ fixado. Como essas definições se relacionam?
	
	Dito de outra maneira, como encontrar os coeficientes $a,b$ da equação conhecendo apenas o vetor $u$ que gera a reta; e como descobrir qual o vetor gerador de uma reta sabendo apenas os coeficientes da equação?
	
	Simples! Vamos mostrar que uma reta é dada pelo conjunto de soluções reais da equação $ax+by=0$ se, e somente se, é gerada pelo vetor $(a,b)^\perp$.
	
	Um lado é trivial: Considere $r=\{v\in\R^2\mid \exists\lambda\in \R\mbox{ tal que }v=\lambda(a,b)^\perp\}$ e lembre que $(a,b)^\perp=(-b,a)$. Ou seja, todo elemento de $r$ é da forma $\lambda(-b,a)$. Claramente elementos dessa forma são soluções da equação $ax+by=0$.
	
	Por outro lado, se $v$ é solução da equação $ax+by=0$, isso significa que $av_1+bv_2=0$, ou seja, $\inprod{(a,b),v}=0$ - ou seja, $(a,b)\perp v$ e, portanto, $v$ é gerado por $(a,b)^\perp$, como queríamos mostrar.
	
	\tcblower
	Resumindo, quando escrevemos uma reta usando sua \textit{equação} estamos, implicitamente, definindo uma reta como ``o conjunto de todos os vetores ortogonais a um vetor fixado'', e quando escrevemos uma reta com um \textit{subespaço gerado por um vetor} estamos definindo a reta como ``o conjunto de todos os vetores múltiplos de um vetor fixado''.
\end{ex}