\chapter{Basic set theory}
\section{Naive set theory}
\subsection{Axioms and you}

Most, if not all, concepts in mathematics are phrased in the language of set theory: Geometric figures are just collections of points, transformations between two different objects are the collections of all the transitional states inbetween etc.

Hence, it makes sense to give some more formal foothold when studying any area of maths by beginning with some basic set theory.

But then, why \textit{naive}?

Well, formal mathematics (that is, all contemporary and modern mathematics for more than a hundred years) is based on what we like to call \textit{axioms} - you can think of them as the ``rules of the game'', in some sense.

Let me give you all an example of a well-accepted axiom of Euclidean geometry:

\begin{blockenv}{axiom}
	Given any two distinct points, there is one, and only one, line through them.
\end{blockenv}

Some people say it's ``something that you can't prove", but it's not exactly that - axioms are either things that you don't \textit{want} to prove, and just want to assume as truth (maybe because it is, indeed, impossible to prove it) or things that are, in some vague sense, ``natural" or ``self-evident".

Either way, the correct mindset to approach axioms is to think of them as the building blocks with which you build maths - just like atoms are the building blocks of matter -: by combining different axioms in different ways you get different results - the so called ``theorems''.

That's what maths is all about: Working with axioms and already proven theorems to prove new theorems. It's kinda like a game of scrabble, where the axioms are not only the blocks you (and everyone else) has in their hands, but also the rules of the game and the game board, and the theorems are the words you can make - subject to the rules of the game, the pieces and the board.

Hence, \textit{naive} set theory is called so not because it is a theory of naive sets, but because it's a theory that's not properly formalized, and relies heavily on intuition and common sense.

In proper, axiomatic, set theory you'd have to define what is, and what isn't, a set. In naive set theory, however, we can just hand-wave it and say
\begin{blockenv}{naive axiom(?)}
	Any collection of things is a set.
\end{blockenv}

Now, as to \textit{why} this isn't formal, it's due to the fact that it leads to a logical contradiction - a paradox. We're gonna show this contradiction in what follows, but if it doesn't interest you (you filthy, you) you can just skip the next section. It's fine, I won't judge you (actually, I will).
\pagebreak
\subsection{Russell's Paradox}

Imagine that every random collection of random things is a set. Then it is only natural to consider the collection of all sets. But, since it is a collection (duh) it is also a set. But since it is the collection of all sets, it is an element of itself.

That's weird, ok? Try thinking of any sets - I'll give you plenty of time, don't worry - that are like that: they contain themselves as elements. You can't, right?

While that's not a contradiction, per se, it \textit{really is weird}.

So let us consider $N$ the collection of all non-weird sets - that is, the collection of all sets that do not contain themselves as elements.

Now, one naturally asks the question: Is $N$ itself a weird set? That is, $N\in N$?

Well, I don't know. But if it was, then, by definition, all elements of $N$ are non-weird sets, so $N$, being an element of $N$, would have to be a non-weird set - that is, $N\notin N$. So... If we assume that $N\in N$ we can logically infer that $N\notin N$...

Okay, maybe we made a mistake all along by assuming that $N\in N$! Yeah, that must be the case! Clearly, $N$ can't be a weird set!... But then, since $N$ isn't weird, it must be an element of $N$ (since $N$ contains \textit{all} non-weird sets)... That is, $N\in N$. So if we assume $N\notin N$ we can logically deduce that $N\in N$.

We have just proven that $N\in N$ and $N\notin N$ are \textit{logically equivalent}. But by the \textbf{Principle of Non-Contradiction} (something can't be simultaneously both true and false) those two can't be equivalent!

So, by assuming that there is a set containing all sets we can logically derive a contradiction - that, my friends, is the definition of a paradox.

This is the famous \textbf{Russell's Paradox} and it applies in broader contexts - it basically means that, from a logical POV, self-references are \textit{kinda weird and you shouldn't actually do that}.

For instance, if you put as an axiom that ``anything that can be stated can be proven", then you could ask ``can I prove that there is something that cannot be proven?" and the answer would have to be \textit{yes}, since you said (by axiom) that everything had to be provable. But that's a contradiction - by forcing everything to have a proof you have proven that you cannot prove everything.

This was proposed by philosopher-mathematician Bertrand Russell to show that maths really does need a formal framework to work with - otherwise we might be working in a system where contradictions arise (as we have seen).

There is, however, a solution to this. We have a set of axioms for set theory called the Zermello-Frankel axioms, which are a list of axioms that do not generate that kind of contradiction. It is, however, \textit{impossible} to prove whether it does or doesn't generate \textit{any} paradox (this is due to a bunch of hard maths/philosophy that is waaaaay out of the scope of this text).

Just know that if you ever see ZFC anywhere you can rest safe because you're working with a (relatively) safe set of axioms.
\pagebreak
\section{Basic results and properties of sets}
\subsection{Equality always}

As we have previously stated, a \textit{set} is a collection of objects. We will usually denote a set by a capital letter (not always), such as $X$, $A$ or $B$.

Since we cannot (as seen in the previous section) consider ``the set of all sets'', fix any set $X$. Now, $X$ might be any set - numbers, birds, colours, the numerous of ways you can insult someone's mum etc.

When we have an object that is in that set we say that it is an \textbf{element} of that set, and usually denote it by a non-capital letter (once again, not always). In symbols, if we want to say that $a$ is an element of $X$ we would write that as $a\in X$ - which should be read as ``$a$ is an element of $X$", ``$a$ is in $X$'' or even ``$X$ contains $a$ as an element".

\begin{ex}
	Let $E$ be the collection of all even integers. So $2\in E$ and $28\in E$, but $5\notin E$ (``$5$ isn't in $E$'', or ``$5$ isn't an even integer") and \textit{dog} $\notin E$ (because \textit{dog} is \textbf{not} an even integer). Actually, you can see this as a formal proof of the well known fact that all dogs are \textit{odd}.
\end{ex}

You can, however, take all the elements of a set and ask if they satisfy a certain condition.

\begin{ex}
	Following up on the previous example, let $\phi$ denote the proposition ``\textit{can be written in english with only three letters}''. Now we can consider the \textit{subset} of $E$ formed by all elements of $E$ that also satisfy $\phi$ (if $x\in E$ is such an element, we simply write $\phi(x)$ to denote ``$x$ satifies $\phi$''). This is written as follows:
	$$E_\phi:=\{x\in E\mid \phi(x)\}.$$
	
	Let us break this down bit-by-bit:
	\begin{itemize}
		\item The symbol $E_\phi$ is non-standard notation that we're introducing here to mean ``\textit{the set $E$ subject to the condition $\phi$}'';
		
		\item The symbol $:=$ means ``\textit{equals, by definition}". This can be used in two distinct ways: During a logical regression, we can use this symbol to justify one step by saying ``this thing that I'm claiming is true, is actually true by definition''; or we can use it to define new terms - we're basically saying ``the LHS is a new symbol whose meaning I'm defining to be the RHS'' - kinda like attributing a value to a variable.
		
		In this text we're \textbf{always} going to use this symbol with the second meaning - so in the preceding expression the $:=$ means ``I'm defining $E_\phi$ to mean $\{x\in E\mid \phi(x)\}$".
		
		\item The brackets, in mathematics, almost always denote a \textit{set}, and always are presented with the following structure: $\{A\mid B\}$.
		
		The $A$ part is \textit{what kind of elements does this set have}. In the example above, $x\in E$ means that the elements we're working with are even integers.
		
		The $B$ part is \textit{which condition these elements are subject to}. In the example above, $\phi(x)$ means that the elements of this set must satisfy $\phi$.
	\end{itemize}

Now that that's out of the way, what is $E_\phi$? What are \textit{the even integers that can be written in english using only three letters}? There are only three such numbers: \textbf{two}, \textbf{six} and \textbf{ten}. So we write $E_\phi=\{2,6,10\}$.
\end{ex}

\begin{df}
	Two sets $A$ and $B$ are said to be \textbf{equal} if they have the same elements. This means that every element of $A$ is an element of $B$, and every element of $B$ is an element of $A$.
	
	In this case we write $A=B$.
\end{df}

Let us give some examples of equalities.

\begin{ex}
	\begin{itemize}
		\item Let $A$ be the set of all animals that are wooly, fluffy and go \textit{baa}, and let $B$ be the set of all sheep. Clearly $A=B$.
		\item Let $A$ be the set of roots of the polynomial $x^2-x$ and let $B=\{0,1\}$. It is an easy exercise to see that these two sets are the same.
		\item However, $A=\N$ the set of all natural numbers, and $B=\Z^{\geq0}$ the set of non-negative integers, are \textbf{not} equal sets. You can see this in any proper course of number/set theory, but the elements of $\Z$ are always signed: $-2$, $+6$, $+1$ etc. (aside from $0$), whereas the elements of $\N$ are \textbf{not} signed: $1$, $6$ etc. So $1\notin\Z$ and $+1\notin\N$, and therefore $A\not=B$.
	\end{itemize}
\end{ex}

\begin{rmk}
	In mathematics, a \emph{definition} is the term we use to ``assign'' a new value to a certain term. In the definition above, we assigned a meaning to the phrase ``two sets are equal".
	
	Please be aware that this text will be filled with definitions of this kind, so take your time to get accostumed to them.
\end{rmk}

Notice, however, that we can sort of ``relax'' the conditions of the preceding definition. For instance, consider the following case:

\begin{ex}
	Let $A=\N$ the set of all natural numbers and $B=E$ the set of all even natural numbers. Notice that $A\not=B$ - for instance, $3$ is in $A$, but not in $B$ - so they can't be equal.
	
	On the other hand, notice that it is impossible to produce such a counterexample starting from $B$: No matter which element you choose in $B$ it will always be a natural number, of course, and therefore it will also be an element of $A$.
	
	So these two sets, although not-equal, are not \textit{entirely} different.
\end{ex}

\begin{df}
	Let $A$ and $B$ be two sets such that every element of $B$ is also an element of $A$. In this case, we say that \textbf{$A$ contains $B$ as a subset} - or more simply that \textbf{$B$ is a subset of $A$}, which we'll denote in symbols by $B\subseteq A$.
\end{df}

\begin{ex}
	\begin{itemize}
		\item In the preceding example, we see that $B\subseteq A$.
		\item Take any set $A$, and let $B=A$. We then ask the question: Is $B$ a subset of $A$? Well, by definition, $B\subseteq A$ if, and only if, every element of $B$ is also an element of $A$... But this is trivially true - since $B=A$!
		
		This gives us some insight on our first result:
	\end{itemize}
\end{ex}

\begin{prop}
	For any set $A$ we have that $A\subseteq A$.
\end{prop}
\begin{proof}
	We want to show that every element $a\in A$ is also an element of $A$. But that's trivial. The result follows.
\end{proof}

\begin{rmk}
	In mathematics, a \emph{proof} of a proposition/lemma/theorem/corolary is nothing more than a logical reasoning explaining why what we said is true. Proofs are to mathematics as scientific experiments are to sciences. This is what mathematicians do and work with all their lives.
	
	One could argue that maths is the science of reasoning and arguing.
\end{rmk}

Now we have our first non-trivial result:

\begin{prop}
	Let $A$ and $B$ be two sets. Then $A=B$ if, and only if, $A\subseteq B$ and $B\subseteq A$.
\end{prop}
\begin{proof}
	Assume that $A=B$. We want to show that $A\subseteq B$ and $B\subseteq A$, but this is trivial in light of the preceding proposition.
	
	\bigskip
	Assume now that $A\subseteq B$ and $B\subseteq A$. We want to show that $A=B$ - that is, every element of $A$ is an element of $B$, and every element of $B$ is an element of $A$.
	
	Notice, however, that the phrase ``every element of $A$ is an element of $B$" is the definition of the symbol $A\subseteq B$, and the phrase ``every element of $B$ is an element of $A$" is the definition of the symbol $B\subseteq A$ - both of which we are assuming to be true.
	
	Therefore, we have just proven that $A=B$, as stated, which finishes the proof.
\end{proof}

\begin{rmk}
	In mathematics, an \emph{if, and only if,} statement is the equivalent of a logical equivalence. Basically, whenever we say ``\emph{this} holds if, and only if, \emph{that} holds" what that means is that \emph{this} and \emph{that} are equivalent: \emph{this} is true precisely when \emph{that} is true, and \emph{this} is false precisely when \emph{that} is also false.
	
	Without going too much into propositional logic, we usually write ``$a$ if, and only if, $b$" in symbols as $a\iff b$, which is logically equivalent to saying that ``$a$ being true is sufficient for us to prove that $b$ is also true" and ``$b$ being true is sufficient for us to prove that $a$ is also true". In symbols we would write these, respectively, as $a\implies b$ and $b\implies a$ - which should be read as ``$a$ implies $b$" and ``$b$ implies $a$", respectively.
	
	That's what we did in the preceding proposition: If $a$=``$A=B$" and $b$=``$A\subseteq B$ and $B\subseteq A$", we proved that assuming $a$ we can conclude $b$, and that assuming $b$ we can conclude $a$ - that is, we proved that $a$ implies $b$ and $b$ implies $a$ - which is logically equivalent to proving that $a$ and $b$ are equivalent.
\end{rmk}

This proposition is the most common tool used by mathematicians to prove that two sets are equal: We simply prove that each one contains the other - therefore, they must be equal.

\begin{ex}
%	Let $A=\{0,1\}$ and $B$ be the set of all possible remainders you could get when dividing a natural number by $2$. For instance: $5:2$ is just $2\cdot 2+1$ - that means it has a remainder of $1$; $8:2$ is just $4\cdot 2+0$ - that means it has a remainder of $0$ etc.
%	
%	So it would seem from these two examples that the only possible remainders are $0$ and $1$ - that is, $A=B$ - but how can we \textit{prove} that?
%	
%	We'll just use the precedin proposition!
%	
%	Clearly $A\subseteq B$ - that is, clearly $0$ and $1$ are possible remainders when dividing by $2$, as the two examples we did show.
%	
%	Now, let us prove that $B\subseteq A$ - that is, that every such remainder is either $0$ or $1$.
%	
%	Take any natural number $n$ and divide it by $2$, getting something like $q\cdot 2+ r$ where $q$ is the quotient of this division, and $r$ is the remainder.
%	
%	What would happen if $r\geq 2$? Well, if $r\geq 2$ then $r-2$ is also an integer - let's call it $s$.
%	
%	This means - just by rearranging terms - that $s+2=r$. And so, we can rewrite $q\cdot 2+r$ as $q\cdot 2+s+2$, group up all the 2s together as $q\cdot 2+2+s$ and factor out the 2 as $(q+1)\cdot 2+s$.
%	
%	This is weird - this is saying that $q+1$ is also a quotient of $n$ divided by $2$, but quotients are uniquely defined!
%	
%	When you divide $n:2$ you should always get the same unique answer - which is $q$ - and we've just shown that you can get \textit{two different answers} (because clearly $q\neq q+1$, otherwise you could prove that $0=1$, just by canceling $q$ on both sides).
%	
%	Ok, so by assuming that $r\geq2$ we have arrived at a contradiction: we have proven that something which we know to be \textit{false} is actually \textit{true}. This must mean that our assumption that $r\geq 2$ has to be \textit{false} - that is, $r<2$ must be \textit{true}.
%	
%	Notice that this doesn't lead to any new contradictions (see Russell's Paradox, where either case led to a contradiction) so  $r<2$ being true is the only possible case which doesn't lead to a contradiction.
%	
%	What we've just shown is that if the remainder is anything \textit{aside} from $0$ and $1$ then our maths would be inconsistent, leading to paradoxes. Since we don't want that, we need the only possible remainders to be $0$ and $1$ - that is, we need $B\subseteq A$.
%	
%	This shows that $A=B$, just as stated.
Let $A$ be the set of roots of the polynomial $x^2-x$ - that is, the set of numbers $r$ such that $r^2-r=0$ - and $B=\{0,1\}$. We claim that $A=B$.

First, let us show that $B\subseteq A$ - that is, both 0 and 1 are roots of $x^2-x$. This is done by a simple verification:
\[0^2-0=0-0=0\quad\quad\mbox{and}\quad\quad 1^2-1=1-1=0\]so they are, indeed, roots of $x^2-x$ - and therefore, $B\subseteq A$.

Now, to prove that $A\subseteq B$ we need to show that those are the only two possible roots.

To do that, let $r$ be any root of $x^2-x$ - that is, $r^2-r=0$. But then, $r^2=r$, by adding $r$ on both sides, and we see that $r=0$ is indeed a solution to this equation ($0^2=0$). So if we assume that $r\neq0$ we can divide both sides by $r$ and get $\dfrac{r^2}{r}=\dfrac{r}{r}$ which is the same as $r=1$, which was a unique solution being $r=1$.

Hence we have proven that any root $r$ of $x^2-x$ is either 0 or 1, and therefore $A\subseteq B$.

Finally, since $A\subseteq B$ and $B\subseteq A$ we can finally say that $A=B$, as we had previously stated.
\end{ex}

\begin{df}
	We say that $A$ is a \textbf{proper subset} of $B$ if $A$ is a subset of $B$, but $B$ isn't a subset of $A$. In this case we use the symbol $A\subset B$.
\end{df}

\begin{ex}
	Consider $A=\N$ the set of natural numbers, and $B=E$ the set of even natural numbers. We clearly have $B\subseteq A$ and $A\not\subseteq B$, so we can see that $B$ is a \textit{proper} subset of $A$ - that is, $B\subset A$.
\end{ex}

Finally, we can use all that we've done so far to construct a very special set - the empty set.

\begin{ex}
	Let $\N$ be the set of natural numbers and let $\phi$ be the proposition ``is not a natural number". For instance, $\phi({\rm car})$ is just ``car is not a natural number", which is true.
	
	Now we can do just as we did before and consider
	\[\N_\phi:=\{n\in\N\mid \phi(n)\}\]that is, the set of all natural numbers which are not natural numbers.
	
	What \textbf{is} this set? Is there any natural number that isn't a natural number? Of course not! So this is a set \textit{which has no elements}.
	
	Take now $\Z$ the set of all integers and let $\psi$ be the proposition ``is not an integer". We can then define, once more,
	\[\Z_\psi:=\{n\in\Z\mid\psi(n)\}\]that is, the set of all integers which aren't integers.
	
	This set is, once again, empty.
	
	This begets the question: $\N_\phi=\Z_\psi$ - that is, are two empty sets always equal?
\end{ex}

\begin{df}
	Given any set $X$ we call the \textbf{empty set defined by $X$} to be the set of all elements of $X$ which aren't elements of $X$, denoted by $\varnothing_X$.
\end{df}

\begin{theorem}
	Given any two sets $A$ and $B$, then $\varnothing_A=\varnothing_B$.
\end{theorem}
\begin{proof}
	If they were different, then there would either be some element of $\varnothing_A$ which is not in $\varnothing_B$, or some element of $\varnothing_B$ which is not in $\varnothing_A$. But both of these are impossible, since both sets are empty.
	
	So they can't be different, and, therefore, $\varnothing_A=\varnothing_B$
\end{proof}
\begin{cor}
	For any set $A$, its empty set $\varnothing_A$ is uniquely determined.
\end{cor}
\begin{cor}
	There a unique empty set.
\end{cor}

\begin{rmk}
	In mathematics, a \emph{corolary} is a result that follows immediately from something that came before it - sometimes even foregoing a proof because of how immediate this conclusion is.
\end{rmk}

\begin{df}
	We're going to define the \textbf{unique empty set} to be the empty set of any set, which will be denoted in symbols by $\varnothing$.
\end{df}

\begin{prop}
	For any set $A$ we have that $\varnothing\subseteq A$. Furthermore, we have that $A\subseteq\varnothing$ if, and only if, $A=\varnothing$.
\end{prop}
\begin{proof}
	If $\varnothing\not\subseteq A$, then there'd be some element in $\varnothing$ that was not in $A$. But $\varnothing$ is empty, therefore $\varnothing\not\subseteq A$ is false, and hence $\varnothing \subseteq A$.
	
	\bigskip
	For the second statement, we clearly have $A\subseteq \varnothing$ if $A=\varnothing$, by definition of set equality.
	
	But if we assume that $A\subseteq\varnothing$, we can now use the first statement of this proof, which proves that $\varnothing\subseteq A$, to conclude, by definition of set equality, that $A=\varnothing$, and this finishes the proof.
\end{proof}
\pagebreak
\subsection{United we stand, intersected we... Fall?}

Now that we have a basic understanding of sets and subsets, we're going to build new sets from existing ones.

\begin{df}
	Let $A$ and $B$ be two sets. The \textbf{union} of $A$ and $B$ is another set - denoted by $A\cup B$ defined by the following properties:
	\begin{enumerate}[(a)]
		\item $A\cup B$ contains both $A$ and $B$ as subsets;
		\item Any other set $C$ that contains both $A$ and $B$ as subsets also contains $A\cup B$ as a subset.
	\end{enumerate}
\end{df}

First things first, let us show that this definition makes sense - that is, that given two sets, their union is a unique set:

\begin{lemma}
	Let $A$ and $B$ be two sets, and $C$ and $D$ be two sets satisfying the above definition. Then $C=D$.
\end{lemma}
\begin{proof}
	Since $C$ and $D$ are unions of $A$ and $B$, they contain both of them as subsets (item (a)). Now, since $C$ satisfies (a) and $D$ satisfies (b), we get that $D\subseteq C$. Similarly, since $D$ satisfies (a) and $C$ satisfies (b), we get that $C\subseteq D$.
	
	It follows that $C=D$, and so the union of two sets is indeed well-defined,
\end{proof}

With that out of the way, let us show some examples to build some intuition:

\begin{ex}
	Let $A$ be the set of all dogs and $B$ be the set of all cats. Let $C$ be the set of all animals. We ask: $C=A\cup B$?
	
	Certainly, $C$ satisfies (a) (since all dogs and all cats are animals), but does it satisfy (b)?
	
	Well, certainly not! Because the set $D$ of all mammals also contains $A$ and $B$, but it clearly doesn't contain $C$ (because not every animal is a mammal - for instance, there are birds).
	
	Now we ask: Ok, since $C$ is not the union of $A$ and $B$, maybe $D$ is?
	
	Well, no, because we can consider $E$ - the set of all mammal quadrupeds  - and see that it contains both $A$ and $B$ as susbets, but not $D$.
	
	And so on, and so forth...
\end{ex}

How can we make sure that we don't get an endless regression - that is, we're always inching closer to the result, but never truly getting there?

Well, in formal set theory, for instance ZFC, you can always use your axioms to guarantee the existence of such a set. Here, however, we're going to have to appeal to intuition:

\begin{prop}
	Given two sets $A$ and $B$ and any set $C$ containing both $A$ and $B$, their union is precisely the subset of $C$ given by the proposition $\phi=$ ``is in any one of the sets $A$ or $B$''.
\end{prop}
\begin{proof}
	First, we'll show that $A\subset C_\phi$ and $B\subseteq C_\phi$.
	
	To do that we'll just use the definition: Take $a\in A$ (resp. $b\in B$). Since $A\subseteq C$ (resp. $B\subseteq C$) we have that $a\in C$ (resp. $b\in C$). We then ask: is $\phi(a)$ (resp. $\phi(b)$) true? Well, it trivially is - $\phi(x)$ is true if, and only if $x$ is in $A$ or $B$ - and $a$ (resp. $b$) certainly is. Therefore, for any $a\in A$ (resp. $b\in B$) we can conclude $\phi(a)$ (resp. $\phi(b)$) - and therefore, $a\in C_\phi$ (resp. $b\in C_\phi$). This shows that $A\subseteq C_\phi$ (resp. $B\subseteq C_\phi$) - and therefore, $C_\phi$ satifies item (a) of the definition of set union.
	
	Now, take any set $D$ such that $D$ contains both $A$ and $B$ as subsets. If we show that $D$ also contains $C_\phi$ as a subset, we'll have shown that $C_\phi$ satisfies the definition of union - and therefore it must be the union.
	
	To do that, take any $x\in C_\phi$. Then, by definition, $\phi(x)$ is true - that is, $x\in A$ or $x\in B$. But since both $A\subseteq D$ and $B\subseteq D$ hold, it doesn't matter if $x\in A$ or $x\in B$ is true - as long as one of them is true, we can conclude that $x\in D$. And since this holds for any $x\in C_\phi$, we have just shown that $C_\phi\subseteq D$.
	
	Since the $D$ we chose was general, the result follows.	
\end{proof}

This is important: We now have a way to construct the union of two sets - just take any set containing both of them and restrict it to be only the elements from the original sets.

That, however, requires the existence of some set containing both of them - and that's where ZFC comes in: There's an axiom that states that there always exists a set containing any amount of other sets.

Since we're foregoing axioms here, we're going to provide a ``construction'' that should be enough for most purposes:

\begin{ex}
	Following up on the previous example, we can now see that $A\cup B$ is any one of ``the set of all animals which are cats or dogs'', ``the set of all mammals which are cats or dogs'' or ``the set of all mammal quadrupeds which are cats or dogs'' - any one of those work, by what we've already proven.
	
	We could, however, give a more explicit construction: $A\cup B$ is just the set of all cats and dogs.
\end{ex}
\begin{ex}
	Another, even more constructive example: Let $A=\{a,b,c\}$ and $B=\{c,d,e,f\}$. Then $A\cup B=\{a,b,c,d,e,f\}$ (prove it using the definition if you're not convinced).
\end{ex}

Finally, let's end our discussions on the union with the following alternative characterization of it:

\begin{lemma}
	Let $A$ and $B$ be sets. Then $x\in A\cup B$ if, and only if, $x\in A$ or $x\in B$.
\end{lemma}
\begin{proof}
	One side of this proof is trivial and follows from the definition of set union.
	
	Let us prove then that $x\in A\cup B$ implies $x\in A$ or $x\in B$.
	
	Define $N=\{x\in A\cup B\mid x\notin A\mbox{ and }x\notin B\}$ the collection of all elements of $A\cup B$ which are in neither $A$ nor $B$ - which is, by definition, a subset of $A\cup B$.
	
	We can now define $U=\{x\in A\cup B\mid x\notin N\}$ the collection of all elements of $A\cup B$ which are not in $N$ - which is, by definition, a subset of $A\cup B$.
	
	We claim that $U$ contains $A$ and $B$ as subsets. This is easy to see: Take $y$ in either $A$ or $B$ (doesn't matter which). Since $A\cup B$ contains both of them, $y\in A\cup B$. But since $y$ came from either $A$ or $B$, it cannot be in $N$ (by definition of $N$) - so it must be in $U$ (by definition of $U$). It follows that both $A$ and $B$ are contained in $U$.
	
	But this is a conundrum, because $A\cup B$ is \textit{contained} in every set that contains $A$ and $B$ (by definition of set union) - in particular, since $U$ contains $A$ and $B$ this means that $U$ \textit{also contains} $A\cup B$.
	
	This shows that $U=A\cup B$, and therefore $N=\varnothing$.
	
	Finally, to show that $x\in A\cup B$ implies $x\in A$ or $x\in B$, take any $x\in A\cup B$ and notice, by what we've done, that this is the same as saying that $x\in U$. But, then again, this is the same as saying that $x\notin N$ - that is $x$ is in $A$ or $B$, just as stated. This finishes the proof.
\end{proof}

\bigskip
Going in the opposite direction of unions, there is the concept of intersections. If unions take two sets to build a bigger one, interceptions take two sets to build a smaller one:

\begin{df}
		Let $A$ and $B$ be two sets. The \textbf{intersection} of $A$ and $B$ is another set - denoted by $A\cap B$ defined by the following properties:
	\begin{enumerate}[(a)]
		\item $A\cap B$ is contained in both $A$ and $B$ as a subset;
		\item Any other set $C$ that is contained both $A$ and $B$ as a subset is also contained $A\cap B$ as a subset.
	\end{enumerate}
\end{df}

\begin{rmk}
	Notice that the two definitions are basically the same, just changing, in some sense, the ``order'' of the inclusions $\subseteq$.
\end{rmk}

Now, let us proceed to prove essentially the same results for intersections as we did for unions:

\begin{lemma}
	Let $A$ and $B$ be two sets, and $C$ and $D$ be two sets satisfying the above definition. Then $C=D$.
\end{lemma}
\begin{proof}
	Since $C$ and $D$ are intersections of $A$ and $B$, they are contained in both of them as subsets (item (a)). Now, since $C$ satisfies (a) and $D$ satisfies (b), we get that $C\subseteq D$. Similarly, since $D$ satisfies (a) and $C$ satisfies (b), we get that $D\subseteq C$.
	
	It follows that $C=D$, and so the intersection of two sets is indeed well-defined,
\end{proof}

Contrary to unions, however, we cannot refine intersections. We can, however, still give a construction of the intersection:

\begin{lemma}
	Let $A$ and $B$ be sets. Then $x\in A\cap B$ if, and only if, $x\in A$ and $x\in B$.
\end{lemma}
\begin{proof}
	One side of this proof is trivial and follows from the definition of set intersection.
	
	Let us prove then that $x\in A$ and $x\in B$ implies $x\in A\cap B$.
	
	Define $N=\{x\in A\mbox{ and } x\in B\mid x\notin A\cap B\}$ the collection of all elements which are, at once, in both $A$ and $B$, but not in $A\cap B$ - which is, by definition, a subset of both $A$ and $B$.
	
	We can now define $I=\{x\in A\mbox{ and } x\in B\mid x\notin N\}$ the collection of all elements of both $A$ and $B$ which are not in $N$ - which is, by definition, a subset of both $A$ and $B$. This implies, by definition of set intersection, that $I\subseteq A\cap B$.
	
	We claim now that $I$ contains $A\cap B$ as a subset. This is easy to see: Take any $y\in A\cap B$. Since $A\cap B\subseteq A$ and $A\cap B\subseteq B$, we see that $y\in A$ and $y\in B$. So this $y$ is an element of both $A$ and $B$ which is in $A\cap B$ - which is the definition of an element of $I$. This shows that $y\in I$.
	
	But this is a conundrum, because $A\cap B$ \textit{contains} every set that is contained in both $A$ and $B$ (by definition of set intersection).
	
	This shows that $I=A\cap B$, and therefore $N=\varnothing$.
	
	Finally, to show that $x\in A$ and $x\in B$ implies $x\in A\cap B$, take any $x\in A$ and $x\in B$ and notice, by what we've done, that this is the same as saying that $x\notin N$ (since it is empty). But, then again, this is the same as saying that $x\in I$ - that is $x$ is in $A\cap B$, just as stated. This finishes the proof.
\end{proof}

Finally, let's do some examples:

\begin{ex}
	\begin{itemize}
		\item Let $A=\{1,2,3,4\}$ and $B=\{1,3,5,7,9\}$. Then, $A\cap B=\{1,3\}$.
		\item If $A$ is the set of all even integers, and $B$ is the set of all odd integers, then $A\cap B=\varnothing$.
		\item If $A$ is the set of all cats and $B$ is the set of all brown animals, then $A\cap B$ is the set of all brown cats.
	\end{itemize}
\end{ex}


\bigskip
As a final topic on this section, let us consider another construction.

\begin{df}
	Given any set $X$ we denote the \textbf{set of all subsets of $X$} by $\mc P(X)$ (or $2^X$) and call it the \textbf{power set} of $X$.
\end{df}
\begin{rmk}
	Note that, at this point, we have not defined products and sums of sets - even less exponents. So, for now, the symbol $2^X$ is just that - a symbol. It has no meaning resembling the powering of real numbers.
	
	We will, however, as this text progresses, show two reasons why this notation makes sense, and we'll expand it to be able to take any set to the power of any other set.
\end{rmk}

Okay, before anything else, let us do some examples:
\begin{ex}
	Let $A=\{1,2,3\}$. What is $\mc P(A)$? Well, by definition it is the set of all subsets of $A$. Well then - what are the subsets of $A$?
	
	We can list a few: $\varnothing$, $A$, $\{1\}$, $\{2\}$, $\{3\}$, $\{1,2\}$, $\{1,3\}$ and $\{2,3\}$. But are there any others?
	
	Well, assume $B\subseteq A$. Then we can ask if $B$ has any elements or not. If it doesn't, great!, because $B=\varnothing$, which we've already accounted for.
	
	If it does, we can ask if it contains $1$. And then, we can ask if it contains $2$ and $3$. And depending on those answers we can pinpoint $B$ exactly, and see that it is, indeed, in the list above (e.g., if it contains $1$ and $2$, but not $3$, then $B=\{1,2\}$, which is on the list above).
	
	At this point, it is easy to see that\[\mc P(A)=\{\varnothing,\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},A\}.\]
	
	The preceding reasoning, however, gives us our first insight into how to understand the symbol $2^A$: Making a subset of $A$ is the same as asking each element of $A$ if it is, or not, in there.
	
	Imagine the elements of $A$ are cards in a deck and you want to make a hand. Making a hand is the same as going through the deck, card by card, and choosing which cards you want to keep, or not.
	
	Since every card has two options (to be, or not to be), the amount of hands is precisely $2$ to the number of cards.
	
	Note that in this particular example, $2^A=\mc P(A)$ has precisely $2^3=8$ elements, while $A$ has precisely $3$ elements.
\end{ex}

Now that we're talking about power sets, we can define one of the most important concepts of set theory:

\begin{df}
	Let $X$ be a set and $2^X$ its power set. Given any $A\in 2^X$, we define its \textbf{complement} to be the set denoted by $X\setminus A$, which is given by\[X\setminus A:=\{x\in X\mid x\notin A\}.\]
\end{df}

That is, the complement of a set is the collection of all elements that \textit{do not} belong to that set.

\begin{ex}
	Following up on the previous example, let $B=\{1\}$. Then what is $A\setminus B$? Well, by definition, it's the collection of all elements of $A$ that are not in $B$ - that is, $2$ and $3$, so $A\setminus B=\{2,3\}$.
	
	Call $C=A\setminus B$. What is, then, $A\setminus C$? Once again, by definition, it's the set of all elements of $A$ which are not in $C$ - that is, $1$, so $A\setminus C=B$.
\end{ex}

And finally, just before wrapping up this section, let us give one final definition and example:

\begin{df}
	Let $A$ and $B$ be any two sets. We define $A\setminus B$ to be equal to $(A\cup B)\setminus B$ - that is, $A\setminus B$ is the complement of $B$ in $A\cup B$.
\end{df}
\begin{ex}
	Let $A=\{a,b,c,d,e,f,g,h,i,j\}$ and $B=\{a,e,i,o,u\}$. Then $A\setminus B$ is, by definition, the set of all elements of $A\cup B$ which are not in $B$. So writing $A\cup B=\{a,b,c,d,e,f,g,h,i,j,o,u\}$ we see that $A\setminus B$ is just $\{b,c,d,f,g,h,j\}$.
	
	Similarly, $B\setminus A$ is the set of all elements of $A\cup B$ which are not in $A$ - that is, $B\setminus A=\{o,u\}$.
\end{ex}

To really wrap up this section, then, we're gonna make a list of properties for the things we've just described. You're welcome to try to prove them, although most of them are really trivial (that is, they follow immediately from the definitions or a quick observation).

\begin{prop}\label{prop:union intersection difference properties}
	Let $A,B,C$ be any three subsets of a given, fixed, set $X$. Then the following properties always hold:
	\begin{multicols}{2}
		\begin{enumerate}[(1)]
			\item $A\cup B=B\cup A$;
			\item $A\cup(B\cup C)=(A\cup B)\cup C$;
			\item $A\cup\varnothing = A$;
			\item $A\cup X=X$;
			\item $A\cup A=A$;
			\item $A\cap B=B\cap A$;
			\item $A\cap(B\cap C)=(A\cap B)\cap C$;
			\item $A\cap\varnothing=\varnothing$;
			\item $A\cap X=A$;
			\item $A\cap A=A$;
			\item $A\cup(B\cap C)=(A\cup B)\cap(A\cup C)$;
			\item $A\cap(B\cup C)=(A\cap B)\cup(A\cap C)$;
			\item $A\cup B=A$ if, and only if, $B\subseteq A$;
			\item $A\cap B=B$ if, and only if, $B\subseteq A$;
			\item $A\subseteq B$ implies $A\setminus B=\varnothing$;
			\item $A\cap B=\varnothing$ implies $A\setminus B=A$ and $B\setminus A=B$;
			\item $X=(X\setminus A)\cup A$;
			\item $(A\setminus B)\cup(B\setminus A)\cup(A\cap B)=A\cup B$;		
			\item $(A\setminus B)\cap(B\setminus A)=\varnothing$;
			\item $X\setminus A\in 2^X$;
			\item $X\setminus (A\cap B)=(X\setminus A)\cup(X\setminus B)$;
			\item $X\setminus (A\cup B)=(X\setminus A)\cap(X\setminus B)$;			
			\item $A\setminus(A\setminus B)=B$;
			\item $A\setminus(A\cap B)=A\setminus B$.
		\end{enumerate}%
	\end{multicols}%
\end{prop}

\begin{rmk}
	In the preceding proposition, as well as in maths as a whole, we usually save the parenthesis to mean ``this should be done first''. For instance, $A\cup(B\cup C)$ means ``the union of $A$ and the union of $B$ and $C$", whereas $(A\cup B)\cup C$ means ``the union of the union of $A$ and $B$ and $C$".
\end{rmk}
\newpage
\section{Time to do some actual set theory, none of this introductory bullshit}
\subsection{How does this function}

Understanding functions is, basically, the most important thing in all of mathematics - and that's not an overstatement. Even if you forego set theory, the concept of a function still makes sense and it's still at the center of any mathematical discussion.

Since this is a naive introduction to set theory, we're not gonna bother with certain technicalities and simply define:

\begin{df}
	Let $A$ and $B$ be two sets. A formula $\phi$ is said to be of \textbf{function type} (or a \textbf{function}) from $A$ to $B$ if for any $a\in A$ there's a unique $b\in B$ such that $\phi(a,b)$.
	
	In that case, we will write that as $\phi(a)=b$ and say that \textbf{$b$ is the image of $a$ under $\phi$}.
\end{df}

\begin{ex}
	Let $A=B=\N$ the set of natural numbers, and let $\phi(x,y)=$ ``$y$ is the square of $x$". Then $\phi$ is clearly a function: for any $a\in A$, there is a unique $b\in B$ such that $\phi(a,b)$, and that $b$ is precisely $a^2$. So we write this as $\phi(a)=a^2$.
	
	Now, define $\psi(x,y)=\phi(y,x)$. Is $\psi$ also a function? The answer is no: Indeed, for any $a\in A$, there exists, at most, one $b\in B$ such that $\psi(a,b)$. But the thing is - there are some $a$ for which there is no $b$! For instance, for $a=3$, there is no $b$ such that $\psi(3,b)$. So $\psi$ can't be a function.
\end{ex}

\begin{df}
	Let $A,B$ be sets and $\phi$ be a function from $A$ to $B$. We will call $A$ the \textbf{domain} of the function and $B$ its \textbf{codomain}, somtimes written as $A=\mathrm{Dom}(\phi)$ and $B=\mathrm{Cod}(\phi)$.
	
	In this case, we wil also use the notation $\phi:A\to B$ or $A\arrow{\phi}B$ to say that ``$\phi$ is a function whose domain is $A$ and whose codomain is $B$".
\end{df}

\begin{df}
	Two functions $f,g:A\to B$ between the same two sets are said to be \textbf{equal} if $f(a)=g(a)$ for all $a\in A$. That is, $f(a,g(a))$ and $g(a,f(a))$ hold for all $a\in A$.
\end{df}

\begin{ex}
	Let $A=B=\R$ the set of real numbers, and let $f,g:A\to B$ be functions defined by $f(x)=\sqrt{x^2}$ and $g(x)=\begin{cases}
	x,\mbox{ if }x\geq0\\
	-x,\mbox{ otherwise.}
	\end{cases}$
	
	We claim that $f=g$.
	
	To see that, take any real number, $x\in\R$. Now, if $x\geq 0$, then $g(x)=x$. Furthermore, $f(x)=\sqrt{x^2}=x$, so $f(x)=g(x)$. On the other hand, if $x<0$, we have $g(x)=-x$ and $f(x)=\sqrt{x^2}=\sqrt{(-x)^2}$ and since $x<0$, $-x$ must be greater than 0, so $f(x)$ is simply $-x$.
	
	Therefore, $f(x)=g(x)$ for all $x\in\R$ (since any real number is either negative or non-negative), and we see that $f=g$, as stated.
\end{ex}

\begin{df}
	If $f:A\to B$ is a function such that $f(a)=b$ for some $a\in A$ and $b\in B$, we then say that \textbf{$f$ takes $a$ to $b$}, which will be written as $a\mapsto b$.
\end{df}

\begin{ex}
	The functions $f,g$ of the previous example can be rewritten as
	\begin{align*}
	f:A&\to B\\
	a&\mapsto \sqrt{a^2}
	\end{align*}
	and
	\begin{align*}
		g:A&\to B\\
		a&\mapsto\begin{cases}
		a, \mbox{ if }a\geq0\\
		-a,\mbox{otherwise.}
		\end{cases}
	\end{align*}	
\end{ex}

Before we move forward, a couple of important definitions:

\begin{df}
	Given any function $f:A\to B$, the set of all elements of $B$ which are image of some element of $A$ under $f$ will be called the \textbf{image of $A$ under $f$} (or just the image of $f$) and denoted by $f(A)$ (or $\im(f)$).
	
	Analogously, given any $X\subseteq A$, we denote the set of all elements of $B$ which are image of some element of $X$ under $f$ by \textbf{image of $f$ when restricted to $X$}, and denote it by $f(X)$.
\end{df}
\begin{prop}
	For any function $f:A\to B$, and any $X\subseteq A$, $f(X)\subseteq B$.
\end{prop}
\begin{proof}
	Trivial, by the definition of image of a function.
\end{proof}

\begin{df}
	Given any function $f:A\to B$ and any point $b\in f(A)$, we define the \textbf{inverse image of $b$ under $f$} to be the set $f^{-1}(b):=\{a\in A\mid f(a)=b\}$ of all points in $A$ whose image under $f$ is precisely $b$.
	
	Analogously, given any $Y\subseteq f(A)$, we define the \textbf{inverse image of $Y$ under $f$} to be the set $f^{-1}(X):=\{a\in A\mid f(a)\in Y\}$ of all points in $A$ whose image under $f$ is in $Y$.
\end{df}
\begin{prop}
	For any function $f:A\to B$, and any $Y\in f(A)$, $f^{-1}(Y)\subseteq A$.
\end{prop}
\begin{proof}
	Trivial, by the definition of inverse image of a function.
\end{proof}

Finally we can start working with some very important classes of functions: Injections, surjections and bijections.

\begin{df}
	A function $f:A\to B$ is called an \textbf{injection} if $f(a)=f(a')$ implies $a=a'$.
\end{df}

\begin{rmk}
	This is logically equivalent to saying that a function is an injection if diferent points of the domain have different images in the codomain.
\end{rmk}

\begin{ex}
	Let $f,g:\{1,2,3\}\to\{a,b,c,d\}$ be defined by: $f(1)=a,f(2)=b,f(3)=c$ and $g(1)=g(2)=g(3)=d$. Then $f$ is injective and $g$ is clearly not injective.
	
	\bigskip 
	Let, now, $h:\R\to\R$ be defined by $h(x)=x^2$. Is $h$ injective?
	
	Well, suppose $x$ and $x'$ are such that $h(x)=h(x')$. This means that $x^2=x'^2$. Taking square roots on both sides we get that $\lvert x\rvert=\lvert x'\rvert$ - which can be further simplified to mean $x=\pm x'$. In other words, we see that if two points have the same square, then they must differ only by a sign. That's good and all, but also shows us that two numbers that differ by a sign have the same image under $h$ - and therefore $h$ cannot be injective.
	
	For instance, $2\neq -2$, but $h(2)=h(-2)=4$.
\end{ex}

\begin{df}
	A function $f:A\to B$ is called a \textbf{surjection} if for any $b\in B$ there is some $a\in A$ such that $f(a)=b$.
\end{df}

\begin{ex}
	Following up on the previous example, neither $f$ nor $g$ are surjections: $d\in \{a,b,c,d\}$ isn't in the image of any point over both $f$ and $g$.
	
	Let us then define $f':\{a,b,c,d\}\to \{1,2,3\}$ by putting $f'(a)=1,f'(b)=2,f'(c)=3,f'(d)=3$. Now, $f'$ is indeed a surjection.
	
	\bigskip
	Notice that $h$ too isn't a surjection: $-1$ isn't the image of any real number under $h$. However, if we define $h':\R\to\R^{\geq0}$, where $\R^{\geq0}$ is the set of all non-negative real numbers, by putting $h'(x):=h(x)$, we see that $h'$ is, now, a surjection.
\end{ex}

\begin{rmk}
	Note that, in the example above, we defined $h'$ by putting $h'(x):=h(x)$. Does that mean that $h'=h$?
	
	The answer is \textbf{no}: By the definition of function equality, for two functions to be equal they must have the same domain and codomain.
	
	This is a very important distinction, and one that most mathematicians and students rarely pay attenttion to.
\end{rmk}

Finally, we can define:
\begin{df}
	A function $f:A\to B$ is called a \textbf{bijection} if it is both an injection and a surjection.
\end{df}

\begin{ex}
	None of the previous examples are bijections, so we have to come up with new examples.
	
	Let $f:\{a,b,c\}\to\{1,2,3\}$ be defined by $f(a)=1,f(b)=2,f(c)=3$. Then $f$ is both injective and surjective, and, therefore, a bijection by definition.
	
	\bigskip
	Let $g:\R^{\geq0}\to\R^{\geq0}$ be defined by $g(x)=x^2$. Then $g$ is both injective (since there's only one sign on the domain) and surjective (since that are no negatives on the codomain), and, therefore, bijective by definition.
	
	\bigskip
	Let $h:\{a,b,c\}\to\{a,b,c\}$ be defined by $h(a)=a,h(b)=c,h(c)=b$. Is $h$ a bijection? Well, it clearly is both injective and surjective, so it has to be by definition.
	
	Notice that bijections don't have to abide by our expectations (such is life).
\end{ex}

\begin{lemma}\label{lem: inj bij im}
	If $f:A\to B$ is injective, then there is a bijection $g:A\to f(A)$.
\end{lemma}
\begin{proof}
	Let $g:A\to f(A)$ be defined by $g(a):=f(a)$ for all $a\in A$. 
	\begin{itemize}
		\item $g$ is injective:
		
		To see that, take $a,a'\in A$ such that $g(a)=g(a')$. Then, by definition, this implies $f(a)=f(a')$, and since $f$ is injective, this in turn implies $a=a'$, so $g$ is injective.
		
		\item $g$ is surjective:
		
		To see that, take any $b\in f(A)$. By definition of image, there exists some $a\in A$ such that $b=f(a)$. But now, by defintion of $g$, this means that $b=g(a)$.
		
		We have just shown that every point in the codomain of $g$ is the image of some point in the domain of $g$ under $g$ - this means that $g$ is surjective.
	\end{itemize}

Since $g$ is both injective and surjective, it is, by definition, a bijection, which ends the proof.
\end{proof}

We can use this lemma to easily determine whether a function is, or isn't, an injection.

\begin{ex}
	Let $f:\{a,b,c\}\to\{1,2\}$ be defined by $f(a)=1,f(b)=2,f(c)=1$. Is $f$ injective?
	
	Well, $f$ takes two different points ($a$ and $c$) to the same point ($1$), so it can't be injective.
	
	\bigskip
	Actually - is it possible for there to be an injective function from $\{a,b,c\}$ to $\{1,2\}$?
	
	Let's try making one: First, we choose an image for $a$ - it can be either $1$ or $2$ - doesn't matter which. Now, to choose an image for $b$ we can't choose the same point as we chose for $a$ - otherwise $f$ won't be injective. So $b$'s image is now uniquely determined: the only point left in $\{1,2\}$ after we take out $f(a)$. Finally, when we try to choose an image for $c$, it can't be $f(a)$, nor can in be $f(b)$ (otherwise, $f$ wouldn't be injective). But $\{1,2\}=\{f(a),f(b)\}$ - that is, if $f(c)$ can't be $f(a)$ and it can't be $f(b)$, then there's \textbf{nothing} that it can be!
	
	But, on the other hand, since $f$ is a function, we \textbf{have to} take $c$ somewhere. This means that we \textbf{have to} repeat either $f(a)$ or $f(b)$.
	
	This shows that there are no injective functions from $\{a,b,c\}$ to $\{1,2\}$.
\end{ex}

\begin{lemma}\label{lem: surj iso im}
	If $f:A\to B$ is surjective, then there is a bijection $g:f(A)\to B$.
\end{lemma}
\begin{proof}
	Let $g:f(A)\to B$ be defined by $g(b):=b$ for all $b\in f(A)$.
	\begin{itemize}
		\item $g$ is surjective:
		
		To see that, take any $b\in B$. Since $f$ is surjective, for each point in $B$ there is at least one point in $A$ which is its inverse image under $f$ - in particular, there is some $a\in A$ such that $f(a)=b$. But this means that $b\in f(A)$, by definition of image of $f$. Now, since $b\in f(A)$, we see that $g(b)=b$ and, therefore, $g$ is surjective.
		
		\item $g$ is injective:
		
		To see that, take any two points $b,b'\in f(A)$ such that $g(b)=g(b')$. But, by definition of $g$, this is the same as saying $g=g'$ - therefore $g$ is injective.		
	\end{itemize}

Since $g$ is both injective and surjective, it is, by definition, a bijection, which ends the proof.
\end{proof}

Analogously to injections, this lemma gives us a clear cut method for distinguishing surjections:

\begin{ex}
	Let $f:\{1,2\}\to\{a,b,c\}$ be given by $f(1)=a$ and $f(2)=b$. Clearly, then, $f$ isn't surjective, because there is one point in its codomain ($c$) which is not the image of any point of the domain under $f$.
	
	\bigskip
	And then we ask: Can there ever be a surjective function from $\{1,2\}$ to $\{a,b,c\}$?
	
	Once again, let's try building one: First, we choose $f(1)$. It can be anything, so choose anything. Now to choose $f(2)$, there's also no restrictions, but remember that we're trying to make a function that ``covers'' $\{a,b,c\}$ with guys from $\{1,2\}$, so even though we could put $f(2):=f(1)$, it makes sense to choose $f(2)$ to be anything aside from $f(1)$... And we're done. 
	
	Notice, however, that no matter \textbf{how} we do that choice, there'll always be some point left in $\{a,b,c\}$. Therefore, there can be no surjections from $\{1,2\}$ to $\{a,b,c\}$.
\end{ex}

These last two examples give us a nice intuition of what injections and surjections measure: Injections measure how much ``smaller'' the domain is, when compared to the codomain, and surjections measure how much ``bigger'' the codomain is, when compared to the domain.

This allows us to consider one final example:

\begin{ex}
	Let $f:\{a,b,c\}\to\{1,2,3\}$ be a function. Can $f$ be a bijection?
	
	Let's try: First, we choose any of $\{1,2,3\}$ to be $f(a)$. Now, since we want $f$ to be a bijection, it needs to be injective and surjective, so we can't choose $f(b)=f(a)$, so choose $f(b)$ to be any of $\{1,2,3\}\setminus \{f(a)\}$. Again, by the same reasoning, choose $f(c)$ to be any of $\{1,2,3\}\setminus\{f(a),f(b)\}$ - which isn't really a choice, since there's only one point left.
	
	And we're done! By construction, $f(a)\neq f(b)$, $f(a)\neq f(c)$ and $f(b)\neq f(c)$ (so $f$ is injective) and all of $\{1,2,3\}$ have inverse images.
	
	This is a strong intuition that we want to build at this point:
	
	\textbf{Bijections between two sets tell us if they have the same amount of points}. In many ways, then, bijections can be thought of as a relabeling of your set - or even, in some cases, as the \textit{definitive and improved} notion of set equality.
	
	And it makes sense - why should the sets $\{a,b,c\}$ and $\{1,2,3\}$ be treated as being different?
	
	You might argue that $1+2=3$, but $a+b$ doesn't even make sense - but the point here is that even $1+2$ doesn't make sense. There's no operations being taken into consideration, nothing. Just sets with elements. The only information we have is that ``$\{a,b,c\}$ is a set with three distinct things inside it'' and that ``$\{1,2,3\}$ is a set with three distinct things inside it''. \textit{What} those things are doesn't really matter to us from a set-theoretical POV. What matters is that there are some things. 
\end{ex}

To expand in that idea - that bijections are the new equality - we're gonna start a more technical subsection.

The reader is encouraged to \textbf{not} skip this section, although I don't own you, so you do you. This subsection will have many proofs, so it's good for practicing your proofs, but not only that - the reasoning employed here is central to understanding what's behind many of the most intricate results in linear algebra.
\newpage
\subsection{Bijection is the new equality}

\begin{df}
	Given any two functions $f:A\to B$ and $g:B\to C$, we call the function $g\circ f:A\to C$ defined by $(g\circ f)(a):=g(f(a))$ the \textbf{composition} of $f$ and $g$.
\end{df}

\begin{df}
	Given any set $X$, we call the function $\id_X:X\to X$ defined by $\id_X(x):=x$ the \textbf{identity function} of $X$.
\end{df}

\begin{df}
	Given a function $f:A\to B$, we say that \textbf{$f$ is an isomorphism} if there is some function $g:B\to A$ such that $f\circ g=\id_B$ and $g\circ f=\id_A$. In this case, we say that $g$ is an \textbf{inverse} for $f$.
\end{df}

\begin{prop}\label{prop: composition is associative}
	Function composition is associative - that is, if $A\arrow{f}B\arrow{g}C\arrow{h}D$, then $h\circ(g\circ f)=(h\circ g)\circ f$.
\end{prop}
\begin{proof}
	Take $a\in A$. Then,
	\begin{align*}
		(h\circ(g\circ f))(a)&=h((g\circ f)(a))\\
		&=h(g(f(a)))\\
		&=(h\circ g)(f(a))=((h\circ g)\circ f)(a)
	\end{align*}and therefore $(h\circ(g\circ f))(a)=((h\circ g)\circ f)(a)$ for any $a\in A$ which, by the definition of function equality, implies that $h\circ(g\circ f)=(h\circ g)\circ f$.
\end{proof}

\begin{prop}
	Given any function $f:A\to B$, we have that $f=\id_B\circ f=f\circ\id_A$.
\end{prop}
\begin{proof}
	Take any $a\in A$. Then:
	\[(\id_B\circ f)(a)=\id_B(f(a))=f(a)=f(\id(a))=(f\circ\id_A)(a)\]which implies, by the definition of function equality, that $\id_B\circ f=f=f\circ \id_A$.
\end{proof}

\begin{prop}
	Let $f:A\to B$ be an isomorphism, and let $g,h:B\to A$ be two inverses for $f$. Then $g=h$.
\end{prop}
\begin{proof}
	This follows from the two preceding propositions and the definition of isomorphism:
	\[g=g\circ\id_B=g\circ(f\circ h)=(g\circ f)\circ h=\id_A\circ h=h.\]
\end{proof}

\begin{df}
	Given an isomorphism $f$, we will denote its (unique!) inverse by $f^{-1}$.
\end{df}

\begin{df}
	A function $f:A\to B$ is called a \textbf{monomorphism} if given any other two functions $g,h:C\to A$, we have that $f\circ g=f\circ h$ implies $g=h$.
\end{df}

\begin{ex}
	Let $f:\{1,2,3\}\to \{a,b,c,d\}$ be defined by $f(1)=a, f(2)=b,f(3)=c$. We claim that $f$ is a monomorphism.
	
	To see that, take any $g,h:C\to\{1,2,3\}$ such that $f\circ g=f\circ h$. In particular, for any $x\in C$ we have that $f(g(x))=f(h(x))$. Well, this means that $f(g(x))$ is either $a$, $b$ or $c$. In either case, we know precisely who $g(x)$ is (for instance, if $f(g(x)))=b$, then $g(x)=2$, since 2 is the only point which is taken to $b$ via $f$).
	
	But since $f(g(x))=f(h(x))$, there's a unique $u\in\{1,2,3\}$ such that $y=g(x)=h(x)$. In particular, $g(x)=h(x)$.
	
	This shows that $g=h$, and, therefore, $f$ is a monomorphism.
\end{ex}

\begin{theorem}\label{thm:mono is inj}
A function is a monomorphism if, and only if, it is an injection.
\end{theorem}
\begin{proof}
	Assume that \(f:A\to B\) is injective. Then given \(g,h:C\to A\) such that \(f\circ g= f\circ h\) we want to show that $g=h$. Since \(f\) is injective, \(f(g(c))=f(h(c))\) implies \(g(c)=h(c)\), for all \(c\in C\). It follows, then, that \(g= h\) and \(f\) is monic.
	
	\bigskip
	Conversely, if \(f:A\to B\) is monic, define \(g,h:\{c\}\to A\) by putting \(g(c)=a\) and \(h(c)=a'\) for two \(a\neq a'\in A\) fixed. Now, since \(f\) is monic, by assumption, and since \(g\neq h\), we have that \(f\circ g\neq f\circ h\) (otherwise we would have $f$ monic, $f\circ g=f\circ h$ and $g\neq h$ all being true, which is impossible). But then:
	\[f(a')=f(h(c))=f\circ h(c)\neq f\circ g(c)=f(g(c))=f(a)\]that is, \(a\neq a'\) assures us that \(f(a)\neq f(a')\), and so \(f\) is injective.
	
	Notice that we used the fact that there are two distinct points in $A$: $a$ and $a'$. If, however, $A$ has only one point it is even simpler: Any function from a set with a single point has to be injective - in particular, monomorphisms whose domain are a single point are injective.
	
	This finishes the proof.
\end{proof}

\begin{lemma}\label{lem:iso is mono}
	Every isomorphism is a monomorphism.
\end{lemma}
\begin{proof}
	Let $f:A\to B$ be an isomorphism and $g,h:C\to A$ any two functions such that $f\circ g=f\circ h$. Then, since $f$ is an isomorphism, there is a unique inverse $f^{-1}:B\to A$ such that $\id_A=f^{-1}\circ f$ and $\id_B=f\circ f^{-1}$. Therefore:
	\[g=\id_A\circ g=(f^{-1}\circ f)\circ g=f^{-1}\circ (f\circ g)=f^{-1}\circ (f\circ h)=(f^{-1}\circ f)\circ h=\id_A\circ h=h\]and we see that $f$ is monic.
\end{proof}

\begin{df}
	A function $f:A\to B$ is called an \textbf{epimorphism} if given any other two functions $g,h:B\to C$, we have that $g\circ f=h\circ f$ implies $g=h$.
\end{df}

\begin{ex}
	Let $f:\{a,b,c\}\to\{1,2\}$ be defined by $f(a)=f(b)=1$ and $f(c)=2$. We claim that $f$ is epic.
	
	To see that, take any two functions $g,h:\{1,2\}\to C$ such that $g\circ f=h\circ f$. We want to show that $g=h$ - that is, for any $x\in\{1,2\}$, we have that $g(x)=h(x)$.
	
	But since $f$ is surjective (check!), there is some $y\in\{a,b,c\}$ such that, then $x=f(y)$, so
	\[g(x)=g(f(y))=(g\circ f)(y)=(h\circ f)(y)=h(f(y))=h(x)\]and therefore we see that $g=h$, which proves that $f$ is indeed an epimorphism.
\end{ex}

\begin{theorem}\label{thm: epi is surj}
	A function $f:A\to B$ is an epimorphism if, and only if, it is a surjection.
\end{theorem}
\begin{proof}
	First, let us assume that \(f:A\to B\) is surjective. Then, given \(g,h:B\to C\) such that\(g\circ f= h\circ f\) we wish to show that $g=h$. We can just proceed as above: Proving that $g=h$ is the same as proving that for all $x\in B$, we have that $g(x)=h(x)$, but since $f$ is surjective, by assumption, we have that there is some $y\in A$ such that $x=f(y)$. It follows then that
	\[g(x)=g(f(y))=(g\circ f)(y)=(h\circ f)(y)=h(f(y))=h(x)\]and therefore $h=g$, which shows that $f$ is epic.
	
	 Conversely, assume that $f$ is epic, and let $C=\{c,c'\}$. Now pick a point $b\in B$ and define $g,h:B\to C$ by putting $g(x)=c$ for all $x\in B$, $h(b)=c'$ and $h(x)=c$ if $x\neq b$.
	 
	 Now, $g(b)\neq h(b)$, so $g\neq h$. Since $f$ is epic, we must then have that $g\circ f\neq h\circ f$, by definition of epimorphism. This means that there is some $a\in A$ such that $g(f(ay))\neq h(f(a))$.
	 
	 Since $g$ takes everyone to $c$, the only possible value of $h(f(a))$ that could be different from that is is $h(f(a))=c'$, but the only element of $B$ that is taken to $c'$ by $h$ is $b$ - this means that $f(a)=b$.
	 
	 We have just proven that given any $b\in B$ there is some $a\in A$ such that $f(a)=b$ - that is, $f$ is surjective, which finishes the proof.
\end{proof}

\begin{lemma}\label{lem:iso is epi}
	Every isomorphism is an epimorphism.
\end{lemma}
\begin{proof}
	Let $f:A\to B$ be an isomorphism and $g,h:B\to C$ two functions such that $g\circ f=h\circ f$. Since $f$ is an isomorphism, it has an inverse $f^{-1}:B\to A$. Therefore:
	\[g=g\circ\id_B=g\circ(f\circ f^{-1})=(g\circ f)\circ f^{-1}=(h\circ f)\circ f^{-1}=h\circ(f\circ f^{-1})=h\circ\id_B=h\]and we see that $g=h$, which proves that $f$ is an epimorphism.
\end{proof}

\begin{df}
	A function $f:A\to B$ is called a \textbf{bimorphism} if it is a mono-epimorphism.
\end{df}

Clearly, by what we've already shown, bijections and bimorphisms are the same thing. However, we can do one better than that:

\begin{lemma}\label{lem: mono left-inverse}
	Every monomorphism $f:A\to B$ has a left-inverse, that is, a function $g:B\to A$ such that $\id_A=g\circ f$, and every function which has a left-inverse is a monomorphism.
\end{lemma}
\begin{proof}
	Let $f:A\to B$ be a monomorphism and consider $f(A)$. Since $f$ is monic, by \cref{thm:mono is inj} we see that $f$ is injective, which means that for all $b\in f(A)$ we have that $f^{-1}(b)$ is a single point in $A$.
	
	We then define $g:B\to A$ by
	$$g(b)=\begin{cases}
	f^{-1}(b),&\mbox{ if }b\in f(A)\\
	a,&\mbox{ otherwise,}
	\end{cases}$$where $a\in A$ is any (literally any) element of $A$.
	
	We claim that this $g$ is a left-inverse for $f$. Indeed, for any $x\in A$ we have
	\[(g\circ f)(x)=g(f(x))=f^{-1}(f(x))=x=\id_A(x)\]since $f(x)\in f(A)$ for all $x\in A$. Therefore, we have shown that for all $x$ we have $(g\circ f)(x)=x=\id_A(x)$ - which implies, by the definition of function equality, that $g\circ f=\id_A$.
	
	\bigskip
	Take now $f:A\to B$ a function that has a left-inverse $g:B\to A$ - that is, $\id_A=g\circ f$. Now take two functions $h,j:C\to A$ such that $f\circ h=f\circ j$. We want to show that $h=j$ (and therefore, $f$ is monic).
	
	Since $f\circ h=f\circ j$, we can compose $g$ on the left on both sides of the equation to obtain $g\circ (f\circ h)=g\circ(f\circ j)$, which, by \cref{prop: composition is associative}, is the same as $(g\circ f)\circ h=(g\circ f)\circ j$, and since $g$ is a left-inverse to $f$, we can further affirm that this is the same as $\id_A\circ h=\id_A\circ j$. Finally, by the definition of $\id_A$, we see that this implies $h=j$ - and therefore $f$ is monic, as stated.
	
	This finishes the proof.
\end{proof}

\begin{lemma}\label{lem: epi right-inverse}
	Every epimorphism $f:A\to B$ has a right-inverse, that is, a function $g:B\to A$ such that $\id_B=f\circ g$, and every function which has a right-inverse is an epimorphism.
\end{lemma}
\begin{proof}
	Let $f:A\to B$ be an epimorphism, and consider $f(A)$ its image. By \cref{thm: epi is surj}, we know that $g$ is a surjection. Since $g$ is a surjection, then, for every $b\in B$ the set $f^{-1}(b)$ is well defined (by definition of surjection).
	
	Now, choose $a_b\in f^{-1}(b)$ for each $b\in B$ (here the index is simply so we know where it came from), and consider the function $g:B\to A$ taking each $b$ to the $a_b$ we chose above.
	
	This is clearly a function (check!), and so we can do, for evrey $b\in B$:
	\[(f\circ g)(b)=f(g(b))=f(a_b)=b=\id_B(b)\]and therefore $f\circ g$ and $\id_B$ are equal in every point - which means that they're equal, and $g$ is a right-inverse for $f$, as stated.
	
	\bigskip
	Take now $f:A\to B$ a function with a right-inverse $g:B\to A$ - that is, $f\circ g=\id_B$. Now take two functions $h,j:B\to C$ such that $h\circ f=j\circ f$. We want to show that $h=j$ (and, therefore, $f$ is epic).
	
	Since $h\circ f=j\circ f$, we can compose $g$ on the right on both sides of the equation to obtain $(h\circ f)\circ g=(j\circ f)\circ g$, which, by \cref{prop: composition is associative}, is the same as $h\circ(f\circ g)=j\circ(f\circ g)$, and since $g$ is a right-inverse to $f$, we can further affirm that this is the same as $h\circ\id_B=j\circ\id_B$. Finally, by the definition of $\id_B$, we see that this implies $h=j$ - and therefore $f$ is epic, as stated.
	
	This finishes the proof.
\end{proof}

\begin{lemma}\label{lem:left- = right-inverse}
	If a function $f:A\to B$ is such that $g,h:B\to A$ are a left- and a right-inverse, respectively, then $g=h$, $f$ is an isomorphism and $g$ is its inverse.
\end{lemma}
\begin{proof}
	It follows trivially by the following computation:
	\[g=g\circ\id_B=g\circ(f\circ h)=(g\circ f)\circ h=\id_A\circ h=h,\]which shows at once that $g=h$. This means that $\id_A=g\circ f$ and $\id_B=f\circ g$ - and therefore $g$ is an inverse to $f$, which shows that $f$ is an isomorphism, as stated.
\end{proof}

\begin{theorem}
	A function $f$ is an isomorphism if, and only if, it is a bimorphism.
\end{theorem}
\begin{proof}
	In light of \cref{lem:iso is epi,lem:iso is mono}, we see that every isomorphism is monic and epic and, therefore, a bimorphism.
	
	Conversely, by \cref{lem: mono left-inverse,lem: epi right-inverse} we see that any bimorphism has both a left- and a right-inverse. But now, \cref{lem:left- = right-inverse} shows us that since every bimorphism has a left- and a right-inverse, it must be an isomorphism, which ends the proof.
\end{proof}
\begin{cor}
	Every bijection has a unique inverse.
\end{cor}

And now, finally, to end this section, some technical results that appear all the time in mathematics.

\begin{lemma}
	$A\arrow{f}B\arrow{g}C$ be two functions. Then the following hold:
		\begin{enumerate}[(a)]
			\item If both $f$ and $g$ are monic, then so is $g\circ f$;
			\item If both $f$ and $g$ are epic, then so is $g\circ f$;
			\item If both $f$ and $g$ are iso, then so is $g\circ f$;
			\item If $g\circ f$ is monic, then so is $f$;
			\item If $g\circ f$ is epic, then so is $g$;
			\item If $g\circ f$ and $f$ are iso, then so is $g$;
			\item If $g\circ f$ and $g$ are iso, then so is $f$.
		\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{enumerate}[(a)]
		\item Assume both $f$ and $g$ are monic, and let $f':B\to A$ and $g':C\to B$ be their respective left-inverses. We claim that $f'\circ g'$ is a left-inverse to $g\circ f$. Indeed:
		\[(f'\circ g')\circ(g\circ f)=f'\circ(g'\circ g)\circ f=f'\circ\id_B\circ f=f'\circ f=\id_A\]so $g\circ f$ is monic.
		
		\item Assume both $f$ and $g$ are epic, and let $f':B\to A$ and $g':C\to B$ be their respective right-inverses. We claim that $f'\circ g'$ is a right-inverse to $g\circ f$. Indeed:
		\[(g\circ f)\circ(f'\circ g')=g\circ(f\circ f')\circ g'=g\circ\id_B\circ g'=g\circ g'=\id_C\]so $g\circ f$ is epic.
		
		\item Follows immediately from (a) and (b).
		
		\item If $g\circ f$ is monic, by \cref{lem: mono left-inverse} we see that there is some function $h:C\to A$ that is a left-inverse to $g\circ f$ - that is, $\id_A=h\circ(g\circ f)$. But now, by \cref{prop: composition is associative}, we see that $h\circ(g\circ f)=(h\circ g)\circ f$ and, therefore, $\id_A=(h\circ g)\circ f$ and we see that $h\circ g$ is a left-inverse for $f$. Now the converse of \cref{lem: mono left-inverse} tells us that since $f$ has a left-inverse, it must be monic.
		
		\item If $g\circ f$ is epic, by \cref{lem: epi right-inverse} we see that there is some function $h:B\to C$ that is a right-inverse to $g\circ f$ - that is, $\id_B=(g\circ f)\circ h$. But now, by \cref{prop: composition is associative}, we see that $(g\circ f)\circ h=g\circ(f\circ h)$ and, therefore, $\id_B=g\circ(f\circ h)$ and we see that $f\circ h$ is a right-inverse to $g$. Now, the converse of \cref{lem: epi right-inverse} tells us that since $g$ has a right-inverse, it must be epic.
		
		\item If $f$ is iso, then so is $f^{-1}$ (its inverse is precisely $f$). Therefore, the equality $g\circ f=g\circ f$ yields the equality $(g\circ f)\circ f^{-1}=g$ by composing $f^{-1}$ to the right on both sides of the equality. Now we use item (c) to conclude that since $g$ is the composition of two isomorphisms, it is also an isomorphism.
		
		\item If $g$ is iso, then so is $g^{-1}$ (its inverse is precisely $g$). Therefore, the equality $g\circ f=g\circ f$ yields the equality $g^{-1}\circ(g\circ f)=f$ by composing $g^{-1}$ to the left on both sides of the equality. Now we use item (c) to conclude that since $f$ is the composition of two isomorphisms, it is also an isomorphism.
	\end{enumerate}

This ends the proof.
\end{proof}

And finally we end this section with a definition.

\begin{df}
	Two sets are said to have \textbf{the same cardinality} if they are isomorphic - that is, if there is an isomorphism between them. If $A$ and $B$ have the same cardinality, we will represent that in symbols by $\#A=\#B$.
\end{df}

\begin{rmk}
	Notice that for finite sets, $\#A$ is precisely the formalization of the intuitive notion of ``number of elements of $A$''.
\end{rmk}
\newpage
\subsection{The {\(A\)} and {\(\Omega\)} of sets}

Now that we have dealt with functions and their properties, we can use them to define new sets.

Before that, though, let's have a quick talk about \textit{universal properties}.

A \textit{universal property} is a way of defining something by saying it's somewhat singular in the universe. For instance, when defining the union and intersection of two sets, we didn't use the classical definition, but, instead, used a universal property to define those sets.

Here's the advantage of working with universal properties:

\begin{df}
	We say that a set $X$ is \textbf{initial} if there is a unique function from $X$ to any other set.
	
	Similarly, we say that $X$ is \textbf{terminal} if there is a unique function from any other set to $X$.
\end{df}
\begin{blockenv}{Meta-theorem}
	All universal properties can be coded in terms of initial/terminal objects on a specific class of sets.
\end{blockenv}

For instance, the union of $A$ and $B$ is the \textit{initial} set in the class of all sets containing $A$ and $B$. Analogously, the intersection of $A$ and $B$ is the \textit{terminal} set in the class of all sets contained in $A$ and $B$.

\begin{blockenv}{Meta-theorem}
	All sets defined by universal properties are uniquely defined (up to isomorphism).
\end{blockenv}

This means that if $X$ is initial/terminal regarding a certain class of sets, then it is the unique set in that class that is initial/terminal (not counting sets that are isomorphic to $X$).

\begin{proof}
	Let $X$ and $Y$ be two sets which are initial regarding a certain class of sets. Since $X$ is initial, there's a unique function $!_Y:X\to Y$. Since $Y$ is initial, there's a unique function $!_X:X\to Y$. This gives us, by composition, a function $!_X\circ!_Y:X\to X$ and a function $!_Y\circ!_X:Y\to Y$.
	
	Now remember that for any set, there's always its identity map. So we have $\id_X:X\to X$ and $\id_Y:Y\to Y$.
	
	But since $X$ is initial, there's a unique function from $X$ to itself. Since there's always an identity function, that function must be the unique map. But we've just shown that $!_X\circ!_Y$ is also a map from $X$ to itself. It follows then that $\id_X=!_X\circ!_Y$.
	
	Arguing similarly for $Y$ we can show that $\id_Y=!_Y\circ!_X$, and, therefore, $!_Y$ is an inverse for $!_X$, and hence they are isomorphisms.
	
	\bigskip
	The proof for the terminal case is identical and left as an exercise to the reader.
\end{proof}

This is another reason why isomorphism is a better notion of set equality - because sets defined by universal properties are unique, up to isomorphism.

Finally, before defining new sets using universal properties, let us prove a couple of interesting results:

\begin{lemma}
	The set $\varnothing$ is the initial set of all sets.
\end{lemma}
\begin{proof}
	There clearly is only one function from $\varnothing$ to any other set.
	
	To see this, think of what would have to go wrong for there to be two different functions, $f$ and $g$: We'd have to have one element $x$ of $\varnothing$ such that $f(x)\neq g(x)$. But $\varnothing$ doesn't have any elements, so any two functions defined on it must be equal.
\end{proof}

\begin{lemma}
	The set $\{a\}$ is the terminal set of all sets.
\end{lemma}
\begin{proof}
	There clearly is only one function from any set to $\{a\}$: The function sending all elements of your domain to $a$.
\end{proof}
\begin{cor}
	There is only one function from any singleton (i.e. a set with a single element) to $\{a\}$.
\end{cor}
\begin{proof}
	Follows trivially by the preceding lemma.
\end{proof}
\begin{cor}
	Any two singletons are isomorphic.
\end{cor}
\begin{proof}
	It is also trivial to prove that any other singleton is also a terminal object. Therefore, it must be isomorphic to $\{a\}$.
	
	Take, then, two singletons $*$ and $\bullet$, and do:
	\[*\leftrightarrow\{a\}\leftrightarrow\bullet,\]where the $\leftrightarrow$ denote the unique isomorphism between the two sets. This is a composition of isomorphisms and, therefore, an isomorphism.
\end{proof}
\begin{cor}
	All terminal objects are singletons.
\end{cor}
\begin{proof}
	Take $T$ any terminal object.
	
	By the preceding lemma, it must be isomorphic to a singleton $\{a\}$. This means that there is a bijection $f:\{a\}\to T$. This means, by \cref{lem: inj bij im} and \cref{lem: surj iso im}, that the image of this isomorphism is isomorphic to $\{a\}$ (since isomorphisms are injective), and the image is also equal to $T$ (since isomorphisms are surjective).
	
	But since the image is a singleton ($\im f=\{f(a)\}$), we have that $T$ is a singleton as well, which ends the proof.
\end{proof}

\begin{rmk}
	For reasons that will become clearer further ahead, for here onwards we're gonna denote the unique initial set by $0$ and the unique terminal set by $1$ (sometimes by $*$ to avoid misconceptions and misunderstandings).
\end{rmk}

\newpage
\subsection{Multiplying sets}
Now that this is done, let us define new sets using universal properties:

\begin{df}
	Let $X$ and $Y$ be two sets. We define the \textbf{product of $X$ and $Y$} to be the set $X\prod Y$ which is terminal in the class of sets with functions to both $X$ and $Y$ - this means that:
	\begin{enumerate}[i.]
		\item There are functions $\pi_X:X\prod Y\to X$ and $\pi_Y:X\prod Y\to Y$;
		\item If $Z$ is some set with functions $p_X:Z\to X$ and $p_Y:Z\to Y$, then there is a unique function $p:Z\to X\prod Y$ such that $p_X=\pi_X\circ p$ and $p_Y=\pi_Y\circ p$.
	\end{enumerate}
\end{df}

We usually denote this saying that the following diagram commutes:
\[\begin{tikzcd}
& Z \arrow[ldd, "p_X"', bend right] \arrow[rdd, "p_Y", bend left] \arrow[d, "\exists!p", dashed] &   \\
& X\prod Y \arrow[ld, "\pi_X"] \arrow[rd, "\pi_Y"]                                               &   \\
X &                                                                                                & Y
\end{tikzcd}\]
This means that no matter which path we take on the diagram, the end result should be the same.

Now, this may seem very abstract and weird at first. But I assure you that you already know what that is.

\begin{df}
	Let $X$ and $Y$ be two sets. We define the \textbf{cartesian product of $X$ and $Y$} to be the set $X\times Y$ of all ordered pairs $(x,y)$ such that $x\in X$ and $y\in Y$.
\end{df}

This is a well-known definition.

\begin{ex}
	Let $A=\{a,b,c\}$ and $B=\{1,2\}$. Then $A\times B=\{(a,1),(a,2),(b,1),(b,2),(c,1),(c,2)\}$ is the cartesian product of $A$ and $B$.
	
	Let $C=\{$yellow pants, brown pants, shorts$\}$ and $D=\{$crop top, black shirt, sweater, jacket$\}$. Then $C\times D$ is the set of all possible combinations of pants types ($C$) and shirt types ($D$) in your closet.
\end{ex}

Now, since the two previous definitions have such similar names they must be related in some way, right? Well...
\begin{prop}
	For any two sets $X$ and $Y$, their cartesian product $X\times Y$ is the product $X\prod Y$.
\end{prop}
\begin{proof}
	We have to show two things: (i.) There are functions $\pi_X:X\times Y\to X$ and $\pi_Y:X\times Y \to Y$ and; (ii.) It is terminal with that property.
	
	\bigskip
	To show (i.), let us define $\pi_X$ and $\pi_Y$ as follows: $\pi_X(x,y):=x$ and $\pi_Y(x,y):=y$ for all $(x,y)\in X\times Y$. These are clearly functions from $X\times Y$ to both $X$ and $Y$, so (i.) is done.
	
	\bigskip
	Now take any other set $Z$ with functions $p_X:Z\to X$ and $p_Y:Z\to Y$. We want to build a function $p:Z\to X\times Y$ such that $p_X=\pi_X\circ p$ and $p_Y=\pi_Y\circ p$, and show that it is the unique function with that property.
	
	Well, take $z\in Z$ and follow it around: If we apply $p_X$ to $z$ we get $p_X(z)\in X$, and, similarly, applying $p_Y$ we get $p_Y(z)\in Y$. Since $p_X(z)$ is in $X$ and $p_Y(z)$ is in $Y$, by definition of cartesian product we have that $(p_X(z),p_Y(z))$ is in $X\times Y$.
	
	Now, clearly we have that $\pi_X(p_X(z),p_Y(z))=p_X(z)$ and $\pi_Y(p_X(z),p_Y(z))=p_Y(z)$ (this is precisely how we defined $\pi_X$ and $\pi_Y$ above). So it is obvious what we must define $p:Z\to X\times Y$ to be:
	
	\[p(z):=(p_X(z),p_Y(z))\]for all $z\in Z$.
	
	This is clearly a function from $Z$ to $X\times Y$ and, by definition, $p_X=\pi_X\circ p$ and $p_Y=\pi_Y\circ p$.
	
	
	\bigskip
	To finish this, we need to show that this $p$ is unique. Well, suppose there is another function, $q:Z\to X\times Y$ such that $p_X=\pi_X\circ q$ and $p_Y=\pi_Y\circ q$. But then, for every $z\in Z$ we'd have that $p_X(z)=\pi_X(q(z))$ and $p_Y(z)=\pi_Y(q(z))$.
	
	This means that $q(z)$ is a point in $X\times Y$ whose $X$-coordinate is $p_X(z)$ (this is what the equation $p_X(z)=\pi_X(q(z))$ tells us), and whose $Y$-coordinate is $p_Y$ (this is what the equation \(p_Y(z)=\pi_Y(q(z))\) tells us).
	
	Therefore, $q(z)=(p_X(z),p_Y(z))$ and the RHS is just $p(z)$, by definition. So we have $q(z)=p(z)$ for all $z\in Z$ - which implies, by the definition of function equality, that $q=p$.
	
	It follows then that the $p$ we've defined is the unique function with that property, so $X\times Y$ is indeed the product of $X$ and $Y$, which ends the proof.
\end{proof}

What's the advantage of defining via universal property instead of just outright using the classical definition? Well...

It's easy to prove that, using the classical definition, we can take any finite number of sets $\{A_i\}_{i\leq n}$ and take their product $A_1\times A_2\times\cdots\times A_n$ to be the iterated product:

\bigskip
$A_1$ and $A_2$ are well-defined, so $A_1\times A_2$ is well-defined.

But now, $A_1\times A_2$ and $A_3$ are well-defined, so $(A_1\times A_2)\times A_3$ is well-defined.

And so on, up to $A_n$.

\bigskip
But try doing that for infinitely many sets. Heck, try doing that for an uncountable amount of sets. 

It's not easy to see how to even \textit{define} such an operation if the set of indices isn't, say, ordered.

However, using the universal property, we can define the product of \textit{any} amount of sets:

\begin{df}
	Let $\{A_i\}_{i\in I}$ be a collection of sets indexed by another set $I$ (which can be infinite or finite, countable or uncountable, doesn't matter, as long as it's a set). We define $\prod_{i\in I}A_i$ to be the set given by:
	\begin{enumerate}[i.]
		\item There is a function $\pi_n:\prod_{i\in I}A_i\to A_n$ for each $n\in I$;
		\item If $Z$ is a set with a function $p_n:Z\to A_n$ for each $n\in I$, then there's a unique function $p:Z\to\prod_{i\in I}A_i$ such that $p_n=\pi_n\circ p$ for each $n\in I$.
	\end{enumerate}
\end{df}

Which is just the \textit{same} definition we used for the product of two sets, but generalized for \textit{any} amount of sets.

This is another great reason to prefer definitions via universal properties instead of explicit ones.

\begin{df}
	Given three sets $A$, $B$ and $C$ with functions $f:A\to B$ and $g:A\to C$, the unique function from $A$ to $B\times C$ induced by the definition of product will be called the \textbf{product map of $f$ and $g$} and denoted by $f\times g:A\to B\times C$.
\end{df}

By definition, $(f\times g)(a):=(f(a),g(a))$ for any $a\in A$.

\begin{ex}
	Let $\R$ be the set of real numbers, $f,g:\R\to \R$ be defined by $f:=\id_\R$ and $g(x):=x^2$ for any $x\in\R$. Then the product map $f\times g:\R\to\R\times \R$ is the map $x\mapsto(x,x^2)$ for any $x\in \R$.
\end{ex}

\begin{df}
	The product map of $\id_X$ and $\id_X$, for any set $X$ will be called the \textbf{diagonal map of $X$} and denoted by $\Delta_X:X\to X\times X$.
	
	It is, as in the above definition, the unique map commuting the diagram
	\[\begin{tikzcd}
	& X \arrow[ldd, "\id_X"', bend right] \arrow[rdd, "\id_X", bend left] \arrow[d, "\exists!\Delta_X"] &   \\
	& X\times X \arrow[ld, "\pi^1_X"] \arrow[rd, "\pi^2_X"']                                            &   \\
	X &                                                                                                   & X.
	\end{tikzcd}\]
\end{df}

\begin{ex}
	Let $A=\{a,b,c\}$. Let's calculate $\Delta_A$.
	
		By definition, we must have $\id_A=\pi^1_A\circ\Delta_A$ and $\id_A=\pi^1_A\circ\Delta_A$. So take any $a\in A$.
	
	We know that, from the first equation, we must have $a=(\pi^1_A\circ\Delta_A)(a)=\pi_A^1(\Delta_A(a))$, so the first coordinate of $\Delta_A(a)$ must be $a$.
	
	Similarly, the second equation gives us $a=(\pi^2_A\circ\Delta_A)(a)=\pi^2_A(\Delta_A(a))$ and so, the second coordinate of $\Delta_A(a)$ must also be $a$.
	
	Since $\Delta_A(a)=(\pi_A^1(\Delta_A(a)),\pi_A^2(\Delta_A(a)))$, we get that $\Delta_A(a)=(a,a)$ for any $a\in A$.
	
	This is why it's called the \textit{diagonal} map.
\end{ex}

\begin{rmk}
	From here onwards, the symbol $\iso$ will mean ``is isomorphic to'' - so $A\iso B$ should be read as ``$A$ is isomorphic to $B$''.
\end{rmk}

Let us then prove some nice properties of products:

\begin{lemma}\label{lem:product properties}
		For any three sets $A,B$ and $C$ the following hold:
	\begin{enumerate}[(a)]
		\item $A\times B\iso B\times A$ (but not equal);
		\item $A\times(B\times C)\iso (A\times B)\times C$ (but not equal);
		\item $A\times 0=0$;
		\item $A\times 1\iso A$ (but not equal).
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{enumerate}[(a)]
		\item Since $A\times B$ is the product of $A$ and $B$, there are functions $\pi_A:A\times B\to A$ and $\pi_B:A\times B\to B$. Similarly, since $B\times A$ is the product of $B$ and $A$, there are functions $\pi'_B:B\times A\to B$ and $\pi'_A:B\times A\to A$.
		
		Now, $A\times B$ has a function to $B$ and a function to $A$, so, by definition of product, there's a unique function $\phi:A\times B\to B\times A$ such that $\pi_A=\pi'_A\circ\phi$ and $\pi_B=\pi'_B\circ\phi$. Analogously, since $B\times A$ has functions to $A$ and $B$, there's a unique function $\psi:B\times A\to A\times B$ such that $\pi'_A=\pi_A\circ\psi$ and $\pi'_B=\pi_B\circ \psi$.
		
		As done previously, we can consider the functions $\id_{A\times B}$ and $\psi\circ\phi$, both from $A\times B$ to itself. Notice that $\pi_A=\pi_A\circ\id_{A\times B}$ and $\pi_B=\pi_B\circ\id_{A\times B}$. Notice also that $$\pi_A=\pi'_A\circ\phi=(\pi_A\circ\psi)\circ\phi=\pi_A\circ(\psi\circ\phi)$$and$$\pi_B=\pi'_B\circ\phi=(\pi_B\circ\psi)\circ\phi=\pi_B\circ(\psi\circ\phi),$$so both $\id_{A\times B}$ and $\psi\circ\phi$ commute the diagram
		\[\begin{tikzcd}
		& A\times B \arrow[d, "\id_{A\times B}",shift left] \arrow[d, "\psi\circ\phi"',shift right] \arrow[ldd, "\pi_A"', bend right] \arrow[rdd, "\pi_B", bend left] &   \\
		& A\times B \arrow[ld, "\pi_A"] \arrow[rd, "\pi_B"]                                                                                    &   \\
		A &                                                                                                                                      & B.
		\end{tikzcd}\] But by definition of product, there's a unique function from any set (including $A\times B$ itself) to $A\times B$ for which that holds. Hence, these two functions must be the same - that is, $\psi\circ\phi=\id_{A\times B}$.
		
		Arguing analogously, we can prove that $\id_{B\times A}$ and $\phi\circ\psi$ are two functions from $B\times A$ to itself which commute the corresponding diagram and, once again, by definition of product, they must be the same - that is, $\phi\circ\psi=\id_{B\times A}$.
		
		Hence, $\phi$ and $\psi$ are inverses and, therefore, isomorphisms.
		
		\item We'll show that both $A\times(B\times C)$ and $(A\times B)\times C$ are isomorphic to $A\times B\times C$, so they must be isomorphic (since composition of isomorphisms is isomorphism).
		
		Actually, we'll only show one of these and leave the other one as an exercise to you, reader.
		
		Let
		\[\begin{tikzcd}[row sep=tiny,column sep=tiny]
		&  & A\times(B\times C) \arrow[lldd, "\pi_A"'] \arrow[rd, "{\pi_{B,C}}"] &                                                    &   \\
		&  &                                                                     & B\times C \arrow[rd, "\pi_C"'] \arrow[ld, "\pi_B"] &   \\
		A &  & B                                                                   &                                                    & C
		\end{tikzcd}\]be the functions from $A\times(B\times C)$ to each one of $A$, $B$ and $C$ (which exist by definition of all the products involved).
		
		Similarly, let
		\[\begin{tikzcd}
		& A\times B\times C \arrow[ld, "\pi'_A"] \arrow[d, "\pi'_B"] \arrow[rd, "\pi'_C"'] &   \\
		A & B                                                                               & C
		\end{tikzcd}\]be the functions from $A\times B\times C$ to each one of $A$, $B$ and $C$ (which exist by definition of product).
		
		Now, by definition of $A\times B\times C$, there's a unique $\phi:A\times(B\times C)\to A\times B\times C$ such that $\pi_A=\pi'_A\circ\phi$, $\pi_B\circ\pi_{B,C}=\pi'_B\circ\phi$ and $\pi_C\circ\pi_{B,C}=\pi'_C\circ\phi$.
		
		But, by definition of $B\times C$, there's a unique $\psi_{B,C}:A\times B\times C\to B\times C$ such that $\pi'_B=\pi_B\circ\psi_{B,C}$ and $\pi'_C=\pi_C\circ\psi_{B,C}$.
		
		Finally, by definition of $A\times (B\times C)$, the above $\psi_{B,C}$ together with $\pi'_A$ show that there's a unique $\psi:A\times B\times C\to A\times(B\times C)$ such that $\pi'_A=\pi_A\circ\psi$ and $\psi_{B,C}=\pi_{B,C}\circ\psi$.
		
		Now, it's easy to see that using all of the equations above we get that
		\begin{align*}
			\pi'_A\circ(\phi\circ\psi)&=(\pi'_A\circ\phi)\circ\psi\\&=\pi_A\circ\psi=\pi'_A
		\end{align*}
		\begin{align*}
		\pi'_B\circ(\phi\circ\psi)&=(\pi'_B\circ\phi)\circ\psi\\
		&=(\pi_B\circ\pi_{B,C})\circ\psi\\
		&=\pi_B\circ(\pi_{B,C}\circ\psi)=\pi_B\circ\psi_{B,C}=\pi'_B
		\end{align*}
		\begin{align*}
		\pi'_C\circ(\phi\circ\psi)&=(\pi'_C\circ\phi)\circ\psi\\
		&=(\pi_C\circ\pi_{B,C})\circ\psi\\
		&=\pi_C\circ(\pi_{B,C}\circ\psi)=\pi_C\circ\psi_{B,C}=\pi'_C
		\end{align*}and, like before, this shows that $\phi\circ\psi=\id_{A\times B\times C}$ - since $\id_{A\times B\times C}$ is the unique function satisfying those three equalities.
		
		Similarly:
		\begin{align*}
			\pi_A\circ(\psi\circ\phi)&=(\pi_A\circ\psi)\circ\phi\\
			&=\pi'_A\circ\phi=\pi_A
		\end{align*}
		\begin{align*}
			(\pi_B\circ\pi_{B,C})\circ(\psi\circ\phi)&=\pi_B\circ(\pi_{B,C}\circ(\psi\circ\phi))\\
			&=\pi_B\circ((\pi_{B,C}\circ\psi)\circ\phi)\\
			&=\pi_B\circ(\psi_{B,C}\circ\phi)\\
			&=(\pi_B\circ\psi_{B,C})\circ\phi\\
			&=\pi'_B\circ\phi=\pi_B\circ\pi_{B,C}
		\end{align*}
		\begin{align*}
		(\pi_C\circ\pi_{B,C})\circ(\psi\circ\phi)&=\pi_C\circ(\pi_{B,C}\circ(\psi\circ\phi))\\
		&=\pi_C\circ((\pi_{B,C}\circ\psi)\circ\phi)\\
		&=\pi_C\circ(\psi_{B,C}\circ\phi)\\
		&=(\pi_C\circ\psi_{B,C})\circ\phi\\
		&=\pi'_C\circ\phi=\pi_C\circ\pi_{B,C}
		\end{align*}and now the last two equalities show that since $\pi_{B,C}$ and $\pi_{B,C}\circ\psi\circ\phi$ are two maps from $A\times(B\times C)$ to $B\times C$ commuting the diagram
		\[\begin{tikzcd}
		& A\times(B\times C) \arrow[ldd, "{\pi_B\circ\pi_{B,C}}"', bend right] \arrow[d, "{\pi_{B,C}}",shift left] \arrow[rdd, "{\pi_C\circ\pi_{B,C}}", bend left] \arrow[d, "{\pi_{B,C}\circ\psi\circ\psi}"',shift right] &   \\
		& B\times C \arrow[ld, "\pi_B"] \arrow[rd, "\pi_C"']                                                                                                                                       &   \\
		B &                                                                                                                                                                                          & C
		\end{tikzcd}\]and since, by definition of product, that map must be unique, it follows that $\pi_{B,C}=\pi_{B,C}\circ\psi\circ\phi$.
		
		But this, on the other hand, shows that both $\id_{A\times(B\times C)}$ and $\psi\circ\psi$ commute the diagram
		\[\begin{tikzcd}
		& A\times(B\times C) \arrow[ldd, "\pi_A"', bend right] \arrow[d, "{\id_{A\times(B\times C)}}",shift left] \arrow[rdd, "{\pi_{B,C}}", bend left] \arrow[d, "\psi\circ\psi"',shift right] &           \\
		& A\times(B\times C) \arrow[ld, "\pi_A"] \arrow[rd, "{\pi_{B,C}}"']                                                                               &           \\
		A &                                                                                                                                                 & B\times C
		\end{tikzcd}\]which, once again, implies that $\psi\circ\phi=\id_{A\times(B\times C)}$, by definition of product.
		
		This shows that $\psi$ and $\phi$ are mutually inverse and, therefore, isomorphisms.
		
		\item This is equivalent to proving that if there is a function between some set $Z$ and $0$, then $Z=0$.
		
		To see this, consider such a function $f:Z\to 0$. If $Z\neq 0$, then there's at least one point $z\in Z$ - and, therefore, $f(z)\in \im(f)\subseteq 0$. But $0$ is empty, so it has no elements, but we've just showed that if $Z\neq 0$, then $f(z)\in 0$, which is a contradiction. It follows that $Z\neq0$ is false, and, so, $Z=0$.
		
		Now, we're going to prove that $0$ is the product of $A$ and $0$ using the definition:
		
		First, see that since $0$ is initial, there's a unique function $!_A:0\to A$ and a unique function $!_0:0\to 0$, but since $\id_0$ always exists, we must have $!_0=\id_0$.
		
		Now let $B$ be a set with functions $f:B\to A$ and $g:B\to 0$. Then $B=0$ (since $g$ is a function to $0$). It follows, then, that $f=!_A$ and $g=!_0$. Clearly, then, the function $!_0:0\to 0$ commutes the diagram
		\[\begin{tikzcd}
		& 0 \arrow[d, "!_0"] \arrow[ldd, "!_A"', bend right] \arrow[rdd, "!_0", bend left] &   \\
		& 0 \arrow[ld, "!_A"] \arrow[rd, "!_0"']                                        &   \\
		A &                                                                               & 0
		\end{tikzcd}\]for $!_A$ and $!_A\circ!_0$ are two functions from 0 to $A$, so they must be equal (since $0$ is initial), and similarly for $!_0\circ!_0$ and $!_0$. 
		
		We have just proven that $0$ satisfies the universal property defining $A\times 0$, so we must have $0\iso A\times 0$, but the only set isomorphic to $0$ is $0$, so we get $0=A\times 0$.
		
		\item Similarly, to prove this result we'll use that $A$ satisfies the definition of product of $A$ and $1$:
		
		First, notice that there's a function $\id_A:A\to A$ and a unique function $!^A:A\to 1$ (since $1$ is terminal).
		
		Now, suppose $B$ is a set with functions $f:B\to A$ and $g:B\to 1$. Since $1$ is terminal, $g:=!^B$ the unique function from $B$ to $1$. 
		
		It is now easy to see that $f:B\to A$ commutes the diagram
		\[\begin{tikzcd}
		& B \arrow[d, "f"] \arrow[ldd, "f"', bend right] \arrow[rdd, "!^B", bend left] &   \\
		& A \arrow[ld, "\id_A"] \arrow[rd, "!^A"']                                     &   \\
		A &                                                                              & 1
		\end{tikzcd}\]for $f=\id_A\circ f$, trivially, and $!^B$ and $!^A\circ f$ are two functions from $B$ to $1$, so they must be equal (since $1$ is terminal).
		
		We've just shown that $A$ satisfies the universal property which defines $A\times 1$, so, since universal properties define sets uniquely up to isomorphism, we get $A\iso A\times 1$.
	\end{enumerate}
\end{proof}

\begin{rmk}
	From here onwards, whenever we multiply a set by itself, we'll take inspiration on numbers and denote the product of $n$ copies of any set $A$ by $A^n$. Note that this makes sense because of the item (b) above.
\end{rmk}

If you think about it, there's no immediate reason why these four statements should be true. Yet, the fact that they are true makes it way more reasonable for our naming it the ``product'' of two sets: Because it looks just like number multiplication.

\newpage
\subsection{Adding sets?}
Well, now that we have multiplication, you know what to do next, right?

\begin{df}
	Let $X$ and $Y$ be two sets. We define the \textbf{coproduct of $X$ and $Y$} to be the set $X\coprod Y$ which is initial in the class of sets with functions from both $X$ and $Y$ - this means that:
	\begin{enumerate}[i.]
		\item There are functions $\iota_X:X\to X\coprod Y$ and $\iota_Y:Y\to X\coprod Y$;
		\item If $Z$ is some set with functions $i_X:X\to Z$ and $i_Y:Y\to Z$, then there is a unique function $i:X\coprod Y\to Z$ such that $i_X=i\circ\iota_X$ and $i_Y=i\circ\iota_Y$.
	\end{enumerate}
\end{df}

We usually denote this saying that the following diagram commutes:
\[\begin{tikzcd}
X \arrow[rd, "\iota_X"'] \arrow[rdd, "i_Z"', bend right] &                                           & Y \arrow[ld, "\iota_Y"] \arrow[ldd, "i_Y", bend left] \\
& X\coprod Y \arrow[d, "\exists!i", dashed] &                                                       \\
& Z                                         &                                                      
\end{tikzcd}\]

Once again, this all sounds too abstract. What's this so called ``coproduct''? Well, it might not seem obvious at first...

\begin{df}
	Let $A$ and $B$ be two sets. We define the \textbf{disjoin union} of $A$ and $B$ to be the set $A\sqcup B$ of all elements in either $A$ or $B$, but disregarding equalities.
\end{df}

This is till too abstract, so lets give an example:

\begin{ex}
	Consider the sets $A=\{a,b\}$ and $B=\{1,2,3\}$. Now, since $A\cap B=\varnothing$, we see that the set $A\cup B$ is just taking all elements of $A$ and $B$, and putting them all in the same box: $A\cup B=\{a,b,1,2,3\}$.
	
	If, however, we take the sets $C=\{a,b,c,d,e\}$ and $D=\{a,e,i,o,u\}$, we see that $C\cup D=\{a,b,c,d,e,i,o,u\}$ which is \textbf{not} the same as just taking every element in $C$ and $D$ and putting them all in the same box. That happens because, contrary to the first case where $A\cap B=\varnothing$, in this case we have $C\cap D=\{a,e\}\neq\varnothing$.
	
	In other words, there is at least one $x\in C$ and one $y\in D$ such that $x=y$.
	
	We can, however, fix that problem by ``coloring'' the elements.
	
	\bigskip
	Imagine that $C$ was a large blue paint can, and $D$ was a giant red paint can. Now, putting the elements from $C$ and $D$ together, we can see that the $a$ and the $e$ that came from $C$ are blue, whereas the corresponding elements from $D$ are red.
	
	This is the idea behind disjoint unions:
	
	$C$, as a set, is clearly isomorphic to the set $C_C:=\{a_C,b_C,c_C,d_C,e_C\}$, which has ``the same elements, but painted in the color $C$''. Similarly, the set $D$ is clearly isomorphic to the set $D_D:=\{a_D,e_D,i_D,o_D,u_D\}$, which has ``the same elements, but painted in the color $D$''.
	
	Now, instead of computing $C\cup D$, we compute $$C_C\cup D_D=\{a_C,b_C,c_C,d_C,e_C,a_D,e_D,i_D,o_D,u_D\},$$ which is precisely what we would get by taking all elements of $C$ and $D$ and putting them in a box, disregarding equalities.
	
	So we put $C\sqcup D:=C_C\cup D_D$, and call it the \textbf{disjoint union of $C$ and $D$}.
\end{ex}

This final argument in the example suggests the following result:

\begin{lemma}\label{lem:sqcup is indexed union}
	For any two sets $X$ and $Y$ we have that $X\sqcup Y\iso (X\times\{0\})\cup (Y\times\{1\})$.
\end{lemma}
\begin{proof}
	It follows directly from the example above and from the fact that we can denote any element in $X\times\{0\}$ (which is of the form $(x,0)$, by definition) as $x_0$, just as a shorthand notation (and similarly for $Y\times\{1\}$), so we can just write $X_0:=X\times\{0\}$ and $Y_1:=Y\times\{1\}$..
	
	\bigskip
	Define $f:X\sqcup Y\to X_0\cup Y_1$ by putting
	\[f(a):=\begin{cases}
	a_0,&\mbox{ if }a\in X\\
	a_1,&\mbox{ if }a\in Y.
	\end{cases}\]
	
	\begin{itemize}
		\item \textbf{$f$ is injective:}
		
		Take $a,b\in X\sqcup Y$ such that $f(a)=f(b)$. We have four different possibilities:
		
		\begin{enumerate}
			\item If $a\in X$ and $b\in X$, we see that $a_0=f(a)=f(b)=b_0$ and hence $a=b$.
			
			\item If $a\in X$ and $b\in Y$, we see that $a_0=f(a)=f(b)=b_1$, which is absurd because the second coordinate of $b_1$ is $1$, and the second coordinate of $a_X$ is $0$, and $0\neq 1$, so $b_1\neq a_0$. So this case cannot happen.
			
			\item Similarly, if $a\in Y$ and $b\in X$ we'd get that same contradiction, so this case also cannot happen.
			
			\item Finally, if $a\in Y$ and $b\in Y$, just like the first case we can see that $a_1=b_1$, and hence $a=b$.			
		\end{enumerate}
		
		
		
		Since in all cases, the only possible situation which doesn't lead to a contradiction is $a=b$, we see that $f$ is injective.
		
		\item \textbf{$f$ is surjective:}
		
		Take $c\in X_0\cup Y_1$. We want to show that there's some $a\in X\sqcup Y$ such that $f(a)=c$.
		
		Since $X_0\cap Y_1=\varnothing$, $c$ must lie in either $X_0$ or $Y_1$ - that is, either $c=x_0$ for some $a\in X$ or $c=y_1$ for some $y\in Y$. But this means that $c$ is either $f(x)$, for some $x\in X$ or $f(y)$, for some $y\in Y$.
		
		No matter which one of these hold, there's always one $a\in X\sqcup Y$ such that $f(a)=c$, for any $c\in X_0\cup Y_1$. This proves that $f$ is surjective.
	\end{itemize}

Since $f$ is both injective and surjective, it is a bijection and, therefore, an isomorphism.

This ends the proof.
\end{proof}

Now, an important question at this point is \textit{WHY?}. I mean, why do we need this new notion of union, when the old one suited us just fine? Well, let me present you some results to convince you on that:

\begin{prop}
	For any two sets $X$ and $Y$, their disjoint union $X\sqcup Y$ is the coproduct $X\coprod Y$.
\end{prop}
\begin{proof}
	We have to show two things:
	\begin{enumerate}[(i.)]
		\item There are functions $\iota_X:X\to X\sqcup Y$ and $\iota_Y:Y\to X\sqcup Y$.
		
		This can easily be seen by considering the \cref{lem:sqcup is indexed union}, and so we can put $\iota_X(x):=x_0$ and $\iota_Y(y):=y_1$ for any $x\in X$ and any $y\in Y$.
		
		\item Let $Z$ be any set with functions $i_X:X\to Z$ and $i_Y:Y\to Z$. We want to show that there's a unique function $i:X\sqcup Y\to Z$ such that $i_X=i\circ\iota_X$ and $i_Y=i\circ\iota_Y$.
		
		This is easy: Define $i:X\sqcup Y\to Z$ by putting
		\[i(a):=\begin{cases}
		i_X(x),&\mbox{ if }a=x_0\\
		i_Y(y),&\mbox{ if }a=y_1.		
		\end{cases}\]
		
		Clearly then we have:
		\[(i\circ\iota_X)(x)=i(x_0)=i_X(x)\]and
		\[(i\circ\iota_Y)(y)=i(y_1)=i_Y(y),\]so our $i$ satisfies the equalities.
		
		\bigskip
		Now to prove that it is unique: Suppose there's some $j:X\sqcup Y\to Z$ that also satisfies the equalities. In particular, we have that $(j\circ \iota_X)(x)=(i\circ\iota_X)(x)$ and $(j\circ \iota_Y)(y)=(i\circ\iota_Y)(y)$.
		
		But the RHS of the first equation evaluates to $i(x_0)$, by definition, and the RHS of the second equation evaluates to $i(y_1)$, also by definition.
		
		But, on the other hand, the LHS evaluates to $j(x_0)$ and $j(y_1)$, respectively, again by definition.
		
		Finally, since every element of $X\sqcup Y$ is either in $X_0$ or in $Y_1$, it follows that the functions $j$ and $i$ are equal for all elements of $X_0$ and $Y_1$ - and, therefore, equal for all of $X\sqcup Y$ - which implies, by definition of equality, $j=i$. This shows that there's a unique function satisfying the equalities
	\end{enumerate}

	Finally, (i.) and (ii.) together show that $X\sqcup Y$ satisfies the universal property defining the coproduct of $X$ and $Y$. This finishes the proof.
\end{proof}

Just like with products, we can use coproducts to generalize disjoint unions to infinitely many sets:

\begin{df}
	Let $\{A_i\}_{i\in I}$ be a collection of sets indexed by another set $I$ (which can be infinite or finite, countable or uncountable, doesn't matter, as long as it's a set). We define $\coprod_{i\in I}A_i$ to be the set given by:
	\begin{enumerate}[i.]
		\item There is a function $\iota_n:A_n\to \coprod_{i\in I}A_i$ for each $n\in I$;
		\item If $Z$ is a set with a function $i_n:A_n\to Z$ for each $n\in I$, then there's a unique function $i:\coprod_{i\in I}A_i\to Z$ such that $i_n=i\circ\iota_n$ for each $n\in I$.
	\end{enumerate}
\end{df}

And we can also get a similar concept to the diagonal map:

\begin{df}
	Given three sets $A$, $B$ and $C$ with functions $f:A\to C$ and $g:B\to C$, the unique function from $A\sqcup B$ to $C$ induced by the definition of coproduct will be called the \textbf{coproduct map of $f$ and $g$} and denoted by $f\sqcup g:A\sqcup B\to C$.
\end{df}

By definition, $(f\sqcup g)(x):=\begin{cases}
f(x),& \mbox{ if }x\in A\\
g(x),& \mbox{ if }x\in B.
\end{cases}$

\begin{ex}
	Let $\R$ be the set of real numbers, $f,g:\R\to \R$ be defined by $f:=\id_\R$ and $g(x):=x^2$ for any $x\in\R$. Then the coproduct map $f\sqcup g:\R\sqcup\R\to\R$ is the map $x\mapsto\begin{cases}
	x,&\mbox{ if }x\in\R_0\\
	x^2,&\mbox{ if }x\in\R_1
	\end{cases}$ for any $x\in \R\sqcup\R$.
\end{ex}

\begin{df}
	The coproduct map of $\id_X$ and $\id_X$, for any set $X$ will be called the \textbf{fold map of $X$} and denoted by $\nabla_X:X\sqcup X\to X$.
	
	It is, as in the above definition, the unique map commuting the diagram
	\[\begin{tikzcd}
	X \arrow[rdd, "\id_X"', bend right] \arrow[rd, "\iota_0"'] &                                         & X \arrow[ldd, "\id_X", bend left] \arrow[ld, "\iota_1"] \\
	& X\sqcup X \arrow[d, "\exists!\nabla_X"] &                                                         \\
	& X                                       &                                                        
	\end{tikzcd}\]
\end{df}

\begin{ex}
	Let $A=\{a,b,c\}$. Let's calculate $\nabla_A$.
	
	By definition, we must have $\id_A=\nabla_A\circ\iota_0$ and $\id_A=\nabla_A\circ\iota_1$. So take any $a\in A$.
	
	We know that, from the first equation, we must have $a=(\nabla_A\circ\iota_0)(a)=\nabla_A(\iota_0(a))=\nabla_A(a_0)$.
	
	Similarly, the second equation gives us $a=(\nabla_A\circ\iota_1)(a)=\nabla_A(\iota_1(a))=\nabla_A(a_1)$.
	
	So $\nabla_A$ is the function that ignores color. It's basically the colorblind function - in its eyes, $x_0$ and $x_1$ are just $x$.
	
	It is, in some sense, folding $x_0$ and $x_1$ on top of eachother - gluing them together. Hence why it's called the \textit{fold} map.
\end{ex}

Now, finally, we'll state similar results to the ones in the product section:

\begin{lemma}
	For any three sets $A,B$ and $C$ the following hold:
	\begin{enumerate}[(a)]
		\item $A\sqcup B\iso B\sqcup A$ (but not equal);
		\item $A\sqcup(B\sqcup C)\iso (A\sqcup B)\sqcup C$ (but not equal);
		\item $A\sqcup 0\iso A$ (but not equal).
	\end{enumerate}
\end{lemma}
\begin{proof}
	\begin{enumerate}[(a)]
		\item It is basically the same proof as in the case of products, so we'll leave it to the reader.
		
		\item This follows from the fact that union is associative, from $X\sqcup Y\iso X_0\cup Y_1$ and from the fact that all sigletons are isomorphic.
		
		Specifically, write $A\sqcup(B\sqcup C)$ as $A_0\cup(B\sqcup C)_1$, and writing $B\sqcup C\iso B_0\cup C_1$, we can further rewrite $A\sqcup(B\sqcup C)$ as $A_0\cup B_{0,1}\cup C_{1,1}$ - which, upon closer inspection, is just $A_0\cup B_1\cup C_2$.
		
		Similarly, we can write $(A\sqcup B)\sqcup C$ as $(A\sqcup B)_0\cup C_1$, and writing $A\sqcup B\iso A_0\cup B_1$, we can further rewrite $(A\sqcup B)\sqcup C$ as $A_{0,0}\cup B_{1,0}\cup C_1$ - which, upon closer inspection, is just $A_0\cup B_1\cup C_2$.
		
		The result now follows trivially.
		
		\item This is proven by showing that $A$ satisfies the universal property defining $A\sqcup 0$ - and is, therefore, isomorphic to it.
		
		First, notice that there are functions $\id_A:A\to A$ and $!_A:0\to A$.
		
		Now, let $B$ be a set with functions $f:A\to B$ and $g:0\to B$. Since $0$ is initial, $g=!_B$ the unique function from $0$ to $B$. So it's easy to see that $f:A\to B$ makes it so that the following diagram commutes
		\[\begin{tikzcd}
		A \arrow[rdd, "f"', bend right] \arrow[rd, "\id_A"'] &                  & 0 \arrow[ldd, "!_B", bend left] \arrow[ld, "!_A"] \\
		& A \arrow[d, "f"] &                                                   \\
		& B                &                                                  
		\end{tikzcd}\]so $A$ satisfies the universal property defining $A\sqcup 0$ and is, therefore, isomorphic to it, just as stated.
		
		\bigskip
		An alternative proof could be given:
		
		Since $A\sqcup 0\iso A_0\cup 0_1$, we can use \cref{lem:product properties}(c) to see that $0_1$, which is just fancy notation for $0\times\{1\}$, is just $0$ (since it is 0 times a set). So $A\sqcup0\iso A_0\cup 0$, and we can now use \cref{prop:union intersection difference properties}(3) and see that $A_0\cup0=A_0$, which is clearly isomorphic to $A$. This shows that $A\sqcup 0\iso A$, as stated.
	\end{enumerate}

This proves the result.
\end{proof}

If you think about it, there's no immediate reason why these three statements should be true. Yet, the fact that they are true makes it way more reasonable for our naming it the ``coproduct'' of two sets: Because it looks just like number addition.

The next subsection will be all about that: How to deal with sets as numbers, and numbers as sets.

\newpage
\subsection{Everything is a set!}

The motivation for this section couldn't be simpler and more direct: We want to give a proper, formal definition of \textit{numbers}.

The reasoning for this is simple: What is a number? What is the number 2, for instance, and why does it not equal, say, 4?

Through the history of mathematics, this has been tried to solve in many ways. For instance, Bertrand Russell (the same one from Russell's Paradox) proposed that the number 2, for instance, should be defined as being the collection of all collections with two elements. Of course, the way I put it here isn't very proper, or formal, but the general ideal is there: You take all collections in the universe, and group them together into mini-collections following a simple rule: Two collections belong in the same mini-collection if, and only if, they have the same amount of elements. And then you say that a \textit{number} is each one of those mini-collections.

Now, we can agree or disagree that this is a good definition, but it sure as hell is a very interesting definition, at least. Think about it - if you go to any dictionary, it'll say that a number is a descriptive of quantity, and a quantity is the number of elements - which is a circular definition. Russell managed to sidestep that problem using the clever idea of grouping all collections with the same amount of elements together.

But then, we can ask: How can you count elements in a set without using numbers? Or, maybe a simpler problem: How to know if two sets have the same amount of elements without using numbers?

What Russell proposed as a solution to that problem is precisely what we're going to use: Bijections.

For Russell, two sets would be said to have the same amount of elements if you could make pairings of elements on both sets such that every element on the first set had a unique match on the second set, and that there was no elements left on either set after this process.

This idea is so simple, yet so brilliant, that even though it's not what we usually think of as numbers today, as mathematicians, it's still a very rich idea on the concept of numbers - so much so that it has been used as an alternative to the classic teaching method for introducing the concept of numbers to small children.

\bigskip
What we're going to do, however, is a tad different. We're going to use the so called \textbf{Peano's Axioms}, which is a list of axioms (which will be three in this text) that is widely accepted in mathematics as being the ``best formalization of the set of natural numbers''.

One of the reasons, and I could argue the \textit{best} reason, why it is considered as such, comes from the fact that it is \textit{categorical}, that is, it admits a unique model (up to isomorphisms, of course).

Basically, what that means is: Suppose I give you a list of axioms. What guarantee is there that there is something that is able to satisfy all those axioms at the same time? For instance, the list
\begin{itemize}
	\item All of its elements are sets;
	\item Every set is in there;
	\item There are no sets in there.
\end{itemize} admits no \textit{model} - that is, there's nothing in the universe that satisfies all these three properties at the same time (you can actually take any two of these and it would work, but with all three at once, it's impossible).

Hence the name model - it's a way of \textit{modelling} your list of conditions - taking it from something abstract and \textit{modelling} it as something concrete.

Now, it's a known fact in mathematical logic that Peano's Axioms (which we're going to present) do admit a model (which we're going to construct), and that it is the \textit{only} model (up to set isomorphism) for those axioms.

Without further ado, then, here we go.

\begin{df}[Peano's Axioms]
	Let $N$ be a set satisfying
	\begin{enumerate}
		\item[(PA1)] There is a point $0\in N$ called \emph{zero}.
		\item[(PA2)] There is an injective function $s:N\to N$ called \emph{successor} such that $0\notin \im(s)$;
		\item[(PA3)] If $X\subseteq N$ is a set containing $0$ such that for every $x\in X$ we have that $s(x)\in X$, then $X=N$. 
	\end{enumerate}

Then $N$ is called the \textbf{set of natural numbers}.
\end{df}

Now, there is no way, at first glance, to tell if these axioms do, or do not, induce a paradox.

The best way to do that is to produce a model for it - for if a list of axioms can be modelled, then it is consistent (i.e., it has no paradoxes).

So, we're going to do just that: Produce a model for it.

\begin{df}[Zero]
	We'll call the set $\varnothing$ the \textbf{zero set} and denote it as $0$.
\end{df}

\begin{df}[Successor]
	For any set $X$ we define it's \textbf{successor} to be the set $s(X):=X\cup\{X\}$.
\end{df}

Now, let's see what is this successor all about.

\begin{ex}
	Let $A=\{a,b\}$. What is $s(A)$? Well, by definition, $s(A)=A\cup\{A\}=\{a,b,\{a,b\}\}$.
	
	Similarly, if $B=\{1,2,3,4\}$, then $s(B)=\{1,2,3,4,\{1,2,3,4\}\}$.
	
	Notice that in both of these cases, the successor of a set has exactly one more element than the set itself - just like in the naturals, the successor of $n$ is precisely $n+1$.
	
	\bigskip
	What is then $s(0)$? Well, once again, by definition, $s(0)=0\cup\{0\}$, but we know that $0\cup A=A$ for any set $A$! So $s(0)=\{0\}$ - a singleton!
	
	Let's call $1:=s(0)=\{0\}$.
	
	Now we can ask what is $s(1)$, and it will be $1\cup\{1\}=\{0\}\cup\{1\}=\{0,1\}$.
	
	Let's call $2:=s(1)=\{0,1\}$...
	
	You can see where we're going with this, right?
\end{ex}

\begin{df}
	Let $S$ denote any set containing $0$ and closed under taking successors (that is, $x\in S$ implies $s(x)\in S$). We define $\N$ to be the set given by
	\[\N:=\bigcap_{N\in \mc N} N\]where $\mc N:=\{X\in \mc P(S)\mid x\in X\implies s(x)\in X\}$.
\end{df}

In some sense, $\N$, as defined above, is ``the smallest set containing zero and all its successors''.

We can finally model Peano's Axioms:
\begin{theorem}
	The set $\N$ satisfies all of (PA1), (PA2) and (PA3).
\end{theorem}
\begin{proof}
	By definition, $\N$ satisfies (PA1) and (PA2).
	
	To prove (PA3), first notice that if we take $X\subseteq\N$ containing $0$ and closed by successors, then, by definition of $\N$, we see that $\N\subseteq X$ (since $\N$ is the smallest set with those properties).
	
	Since we have both $X\subseteq \N$ and $\N\subseteq X$, we can conclude that $X=\N$, and so (PA3) holds.
	
	This shows that $\N$ is a model for Peano's Axioms, as we wanted.
\end{proof}

With this, we see that Peano's Axioms are consistent (i.e., don't derive any logical contradictions) and, therefore, we can work with them.

So, from now on, every natural number $n$ should be thought of as the set $\{0,1,2,3,\cdots,n-1\}$.

Since, however, our main goal here is to get to Linear Algebra, we're not going to spend time defining the addition, subtraction, multiplication and division operations on $\N$, and we're going to assume that the reader is sufficiently familiar with them.

We're also not going to construct the integers, rationals or real numbers, not because it would be unfeasible, but because it doesn't suit the text.

\bigskip
Now that we have numbers, we can finally begin our study on cardinality, which will be central to our studies when we eventually get to linear algebra.

\begin{df}
	Let $X$ be a set. We say that $X$ is \textbf{finite} if there is some $n\in \N$ such that $X\iso n$. In this case, we say that \textbf{$X$ has $n$ elements}, and denote it with the symbol $\#X=n$.
\end{df}

\begin{ex}
	Consider $A=\{a,b\}$. Then $A$ is finite, because $A\iso 2$. To see that, consider the function $f:A\to 2$ taking $a\mapsto 0$ and $b\mapsto 1$. It clearly is bijective, by definition, so it is an isomorphism.
	
	\bigskip
	Conversely, the set $\N$ isn't finite. To see that, suppose it was - that is, there is some $n\in \N$ such that $n\iso \N$. This would imply that $n$ satisfies Peano's Axioms - in particular, $n$ would be closed by successors.
	
	But $n=\{0,1,2,3,\cdots,n-1\}$, and, clearly, for all $x\in n$, aside from $x=n-1$, we see that $s(x)\in n$. But if $n\iso \N$, we would have to have that $s(n-1)\in n$. But $s(n-1)=n$. So we would be saying that $n\in n$, which, by our previous discussion on Russell's Paradox, \textbf{cannot} happen.
	
	Therefore, if $\N\iso n$, for some $n\in \N$, we would get a paradox akin to Russell's Paradox. Since we don't want that, we can't have $\N\iso n$, no matter which $n\in \N$ we choose.
	
	It follows that $\N$ is not finite.
\end{ex}

\begin{prop}
	If $f:X\to X$ is injective, for some finite set $X$, then $f$ is automatically surjective.
\end{prop}
\begin{proof}
	If $X=0$ it is true (since the only function between 0 and 0 is the identity, which is a bijection).
	
	Suppose it is true for some finite $X$. We will prove that it holds true also for $s(X)$.
	
	Let $f:s(X)\to s(X)$ be an injective function, and consider $f(s(X))$ its image.
	
	Now, since $f$ is injective, we have that $f(s(X))\iso s(X)$, and since $X\iso n$, we have that $s(X)\iso n+1$, and, therefore, $f(s(X))\iso n+1$ - and hence it is also finite.
	
	If $f(s(X))=s(X)$, we're done. Otherwise, there's some $x\in s(X)$ such that $x\in s(X)\setminus f(s(X))$ - that is, $x$ is in $s(X)$, but not in $f(s(X))$.
	
	Consider then the set $X':=s(X)\setminus\{x\}$. Clearly, $X'\iso X\iso n$.
	
	This allows us to define $f':X'\to X'$ by putting $f'(x'):=f(x')$, for all $x'\in X'$. Now this function is clearly injective and, therefore, since we're assuming the result holds true for $X$ (which is isomorphic to $X'$) we can conclude that $f'$ is surjective.
\end{proof}