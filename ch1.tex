\chapter{Basic set theory}
\section{Naive set theory}
\subsection{Axioms and you}

Most, if not all, concepts in mathematics are phrased in the language of set theory: Geometric figures are just collections of points, transformations between two different objects are the collections of all the transitional states inbetween etc.

Hence, it makes sense to give some more formal foothold when studying any area of maths by beginning with some basic set theory.

But then, why \textit{naive}?

Well, formal mathematics (that is, all contemporary and modern mathematics for more than a hundred years) is based on what we like to call \textit{axioms} - you can think of them as the ``rules of the game'', in some sense.

Let me give you all an example of a well-accepted axiom of Euclidean geometry:

\begin{blockenv}{Axiom}
	Given any two distinct points, there is one, and only one, line through them.
\end{blockenv}

Some people say it's ``something that you can't prove", but it's not exactly that - axioms are either things that you don't \textit{want} to prove, and just want to assume as truth (maybe because it is, indeed, impossible to prove it) or things that are, in some vague sense, ``natural" or ``self-evident".

Either way, the correct mindset to approach axioms is to think of them as the building blocks with which you build maths - just like atoms are the building blocks of matter -: by combining different axioms in different ways you get different results - the so called ``theorems''.

That's what maths is all about: Working with axioms and already proven theorems to prove new theorems. It's kinda like a game of scrabble, where the axioms are not only the blocks you (and everyone else) has in their hands, but also the rules of the game and the game board, and the theorems are the words you can make - subject to the rules of the game, the pieces and the board.

Hence, \textit{naive} set theory is called so not because it is a theory of naive sets, but because it's a theory that's not properly formalized, and relies heavily on intuition and common sense.

In proper, axiomatic, set theory you'd have to define what is, and what isn't, a set. In naive set theory, however, we can just hand-wave it and say
\begin{blockenv}{Naive axiom(?)}
	Any collection of things is a set.
\end{blockenv}

Now, as to \textit{why} this isn't formal, it's due to the fact that it leads to a logical contradiction - a paradox. We're gonna show this contradiction in what follows, but if it doesn't interest you (you filthy, you) you can just skip the next section. It's fine, I won't judge you (actually, I will).

\subsection{Russell's Paradox}

Imagine that every random collection of random things is a set. Then it is only natural to consider the collection of all sets. But, since it is a collection (duh) it is also a set. But since it is the collection of all sets, it is an element of itself.

That's weird, ok? Try thinking of any sets - I'll give you plenty of time, don't worry - that are like that: they contain themselves as elements. You can't, right?

While that's not a contradiction, per se, it \textit{really is weird}.

So let us consider $N$ the collection of all non-weird sets - that is, the collection of all sets that do not contain themselves as elements.

Now, one naturally asks the question: Is $N$ itself a weird set? That is, $N\in N$?

Well, I don't know. But if it was, then, by definition, all elements of $N$ are non-weird sets, so $N$, being an element of $N$, would have to be a non-weird set - that is, $N\notin N$. So... If we assume that $N\in N$ we can logically infer that $N\notin N$...

Okay, maybe we made a mistake all along by assuming that $N\in N$! Yeah, that must be the case! Clearly, $N$ can't be a weird set!... But then, since $N$ isn't weird, it must be an element of $N$ (since $N$ contains \textit{all} non-weird sets)... That is, $N\in N$. So if we assume $N\notin N$ we can logically deduce that $N\in N$.

We have just proven that $N\in N$ and $N\notin N$ are \textit{logically equivalent}. But by the \textbf{Principle of Non-Contradiction} (something can't be simultaneously both true and false) those two can't be equivalent!

So, by assuming that there is a set containing all sets we can logically derive a contradiction - that, my friends, is the definition of a paradox.

This is the famous \textbf{Russell's Paradox} and it applies in broader contexts - it basically means that, from a logical POV, self-references are \textit{kinda weird and you shouldn't actually do that}.

For instance, if you put as an axiom that ``anything that can be stated can be proven", then you could ask ``can I prove that there is something that cannot be proven?" and the answer would have to be \textit{yes}, since you said (by axiom) that everything had to be provable. But that's a contradiction - by forcing everything to have a proof you have proven that you cannot prove everything.

This was proposed by philosopher-mathematician Bertrand Russell to show that maths really does need a formal framework to work with - otherwise we might be working in a system where contradictions arise (as we have seen).

There is, however, a solution to this. We have a set of axioms for set theory called the Zermello-Frankel axioms, which are a list of axioms that do not generate that kind of contradiction. It is, however, \textit{impossible} to prove whether it does or doesn't generate \textit{any} paradox (this is due to a bunch of hard maths/philosophy that is waaaaay out of the scope of this text).

Just know that if you ever see ZFC anywhere you can rest safe because you're working with a (relatively) safe set of axioms.

\section{Basic results and properties of sets}
\subsection{Equality always}

As we have previously stated, a \textit{set} is a collection of objects. We will usually denote a set by a capital letter (not always), such as $X$, $A$ or $B$.

Since we cannot (as seen in the previous section) consider ``the set of all sets'', fix any set $X$. Now, $X$ might be any set - numbers, birds, colours, the numerous of ways you can insult someone's mum etc.

When we have an object that is in that set we say that it is an \textbf{element} of that set, and usually denote it by a non-capital letter (once again, not always). In symbols, if we want to say that $a$ is an element of $X$ we would write that as $a\in X$ - which should be read as ``$a$ is an element of $X$", ``$a$ is in $X$'' or even ``$X$ contains $a$ as an element".

\begin{ex}
	Let $E$ be the collection of all even integers. So $2\in E$ and $28\in E$, but $5\notin E$ (``$5$ isn't in $E$'', or ``$5$ isn't an even integer") and \textit{dog} $\notin E$ (because \textit{dog} is \textbf{not} an even integer). Actually, you can see this as a formal proof of the well known fact that all dogs are \textit{odd}.
\end{ex}

You can, however, take all the elements of a set and ask if they satisfy a certain condition.

\begin{ex}
	Following up on the previous example, let $\phi$ denote the proposition ``\textit{can be written in english with only three letters}''. Now we can consider the \textit{subset} of $E$ formed by all elements of $E$ that also satisfy $\phi$ (if $x\in E$ is such an element, we simply write $\phi(x)$ to denote ``$x$ satifies $\phi$''). This is written as follows:
	$$E_\phi:=\{x\in E\mid \phi(x)\}.$$
	
	Let us break this down bit-by-bit:
	\begin{itemize}
		\item The symbol $E_\phi$ is non-standard notation that we're introducing here to mean ``\textit{the set $E$ subject to the condition $\phi$}'';
		
		\item The symbol $:=$ means ``\textit{equals, by definition}". This can be used in two distinct ways: During a logical regression, we can use this symbol to justify one step by saying ``this thing that I'm claiming is true, is actually true by definition''; or we can use it to define new terms - we're basically saying ``the LHS is a new symbol whose meaning I'm defining to be the RHS'' - kinda like attributing a value to a variable.
		
		In this text we're \textbf{always} going to use this symbol with the second meaning - so in the preceding expression the $:=$ means ``I'm defining $E_\phi$ to mean $\{x\in E\mid \phi(x)\}$".
		
		\item The brackets, in mathematics, almost always denote a \textit{set}, and always are presented with the following structure: $\{A\mid B\}$.
		
		The $A$ part is \textit{what kind of elements does this set have}. In the example above, $x\in E$ means that the elements we're working with are even integers.
		
		The $B$ part is \textit{which condition these elements are subject to}. In the example above, $\phi(x)$ means that the elements of this set must satisfy $\phi$.
	\end{itemize}

Now that that's out of the way, what is $E_\phi$? What are \textit{the even integers that can be written in english using only three letters}? There are only three such numbers: \textbf{two}, \textbf{six} and \textbf{ten}. So we write $E_\phi=\{2,6,10\}$.
\end{ex}

\begin{df}
	Two sets $A$ and $B$ are said to be \textbf{equal} if they have the same elements. This means that every element of $A$ is an element of $B$, and every element of $B$ is an element of $A$.
	
	In this case we write $A=B$.
\end{df}

Let us give some examples of equalities.

\begin{ex}
	\begin{itemize}
		\item Let $A$ be the set of all animals that are wooly, fluffy and go \textit{baa}, and let $B$ be the set of all sheep. Clearly $A=B$.
		\item Let $A$ be the set of roots of the polynomial $x^2-x$ and let $B=\{0,1\}$. It is an easy exercise to see that these two sets are the same.
		\item However, $A=\N$ the set of all natural numbers, and $B=\Z^{\geq0}$ the set of non-negative integers, are \textbf{not} equal sets. You can see this in any proper course of number/set theory, but the elements of $\Z$ are always signed: $-2$, $+6$, $+1$ etc. (aside from $0$), whereas the elements of $\N$ are \textbf{not} signed: $1$, $6$ etc. So $1\notin\Z$ and $+1\notin\N$, and therefore $A\not=B$.
	\end{itemize}
\end{ex}

\begin{rmk}
	In mathematics, a \emph{definition} is the term we use to ``assign'' a new value to a certain term. In the definition above, we assigned a meaning to the phrase ``two sets are equal".
	
	Please be aware that this text will be filled with definitions of this kind, so take your time to get accostumed to them.
\end{rmk}

Notice, however, that we can sort of ``relax'' the conditions of the preceding definition. For instance, consider the following case:

\begin{ex}
	Let $A=\N$ the set of all natural numbers and $B=E$ the set of all even natural numbers. Notice that $A\not=B$ - for instance, $3$ is in $A$, but not in $B$ - so they can't be equal.
	
	On the other hand, notice that it is impossible to produce such a counterexample starting from $B$: No matter which element you choose in $B$ it will always be a natural number, of course, and therefore it will also be an element of $A$.
	
	So these two sets, although not-equal, are not \textit{entirely} different.
\end{ex}

\begin{df}
	Let $A$ and $B$ be two sets such that every element of $B$ is also an element of $A$. In this case, we say that \textbf{$A$ contains $B$ as a subset} - or more simply that \textbf{$B$ is a subset of $A$}, which we'll denote in symbols by $B\subseteq A$.
\end{df}

\begin{ex}
	\begin{itemize}
		\item In the preceding example, we see that $B\subseteq A$.
		\item Take any set $A$, and let $B=A$. We then ask the question: Is $B$ a subset of $A$? Well, by definition, $B\subseteq A$ if, and only if, every element of $B$ is also an element of $A$... But this is trivially true - since $B=A$!
		
		This gives us some insight on our first result:
	\end{itemize}
\end{ex}

\begin{prop}
	For any set $A$ we have that $A\subseteq A$.
\end{prop}
\begin{proof}
	We want to show that every element $a\in A$ is also an element of $A$. But that's trivial. The result follows.
\end{proof}

\begin{rmk}
	In mathematics, a \emph{proof} of a proposition/lemma/theorem/corolary is nothing more than a logical reasoning explaining why what we said is true. Proofs are to mathematics as scientific experiments are to sciences. This is what mathematicians do and work with all their lives.
	
	One could argue that maths is the science of reasoning and arguing.
\end{rmk}

Now we have our first non-trivial result:

\begin{prop}
	Let $A$ and $B$ be two sets. Then $A=B$ if, and only if, $A\subseteq B$ and $B\subseteq A$.
\end{prop}
\begin{proof}
	Assume that $A=B$. We want to show that $A\subseteq B$ and $B\subseteq A$, but this is trivial in light of the preceding proposition.
	
	\bigskip
	Assume now that $A\subseteq B$ and $B\subseteq A$. We want to show that $A=B$ - that is, every element of $A$ is an element of $B$, and every element of $B$ is an element of $A$.
	
	Notice, however, that the phrase ``every element of $A$ is an element of $B$" is the definition of the symbol $A\subseteq B$, and the phrase ``every element of $B$ is an element of $A$" is the definition of the symbol $B\subseteq A$ - both of which we are assuming to be true.
	
	Therefore, we have just proven that $A=B$, as stated, which finishes the proof.
\end{proof}

\begin{rmk}
	In mathematics, an \emph{if, and only if,} statement is the equivalent of a logical equivalence. Basically, whenever we say ``\emph{this} holds if, and only if, \emph{that} holds" what that means is that \emph{this} and \emph{that} are equivalent: \emph{this} is true precisely when \emph{that} is true, and \emph{this} is false precisely when \emph{that} is also false.
	
	Without going too much into propositional logic, we usually write ``$a$ if, and only if, $b$" in symbols as $a\iff b$, which is logically equivalent to saying that ``$a$ being true is sufficient for us to prove that $b$ is also true" and ``$b$ being true is sufficient for us to prove that $a$ is also true". In symbols we would write these, respectively, as $a\implies b$ and $b\implies a$ - which should be read as ``$a$ implies $b$" and ``$b$ implies $a$", respectively.
	
	That's what we did in the preceding proposition: If $a$=``$A=B$" and $b$=``$A\subseteq B$ and $B\subseteq A$", we proved that assuming $a$ we can conclude $b$, and that assuming $b$ we can conclude $a$ - that is, we proved that $a$ implies $b$ and $b$ implies $a$ - which is logically equivalent to proving that $a$ and $b$ are equivalent.
\end{rmk}

This proposition is the most common tool used by mathematicians to prove that two sets are equal: We simply prove that each one contains the other - therefore, they must be equal.

\begin{ex}
%	Let $A=\{0,1\}$ and $B$ be the set of all possible remainders you could get when dividing a natural number by $2$. For instance: $5:2$ is just $2\cdot 2+1$ - that means it has a remainder of $1$; $8:2$ is just $4\cdot 2+0$ - that means it has a remainder of $0$ etc.
%	
%	So it would seem from these two examples that the only possible remainders are $0$ and $1$ - that is, $A=B$ - but how can we \textit{prove} that?
%	
%	We'll just use the precedin proposition!
%	
%	Clearly $A\subseteq B$ - that is, clearly $0$ and $1$ are possible remainders when dividing by $2$, as the two examples we did show.
%	
%	Now, let us prove that $B\subseteq A$ - that is, that every such remainder is either $0$ or $1$.
%	
%	Take any natural number $n$ and divide it by $2$, getting something like $q\cdot 2+ r$ where $q$ is the quotient of this division, and $r$ is the remainder.
%	
%	What would happen if $r\geq 2$? Well, if $r\geq 2$ then $r-2$ is also an integer - let's call it $s$.
%	
%	This means - just by rearranging terms - that $s+2=r$. And so, we can rewrite $q\cdot 2+r$ as $q\cdot 2+s+2$, group up all the 2s together as $q\cdot 2+2+s$ and factor out the 2 as $(q+1)\cdot 2+s$.
%	
%	This is weird - this is saying that $q+1$ is also a quotient of $n$ divided by $2$, but quotients are uniquely defined!
%	
%	When you divide $n:2$ you should always get the same unique answer - which is $q$ - and we've just shown that you can get \textit{two different answers} (because clearly $q\neq q+1$, otherwise you could prove that $0=1$, just by canceling $q$ on both sides).
%	
%	Ok, so by assuming that $r\geq2$ we have arrived at a contradiction: we have proven that something which we know to be \textit{false} is actually \textit{true}. This must mean that our assumption that $r\geq 2$ has to be \textit{false} - that is, $r<2$ must be \textit{true}.
%	
%	Notice that this doesn't lead to any new contradictions (see Russell's Paradox, where either case led to a contradiction) so  $r<2$ being true is the only possible case which doesn't lead to a contradiction.
%	
%	What we've just shown is that if the remainder is anything \textit{aside} from $0$ and $1$ then our maths would be inconsistent, leading to paradoxes. Since we don't want that, we need the only possible remainders to be $0$ and $1$ - that is, we need $B\subseteq A$.
%	
%	This shows that $A=B$, just as stated.
Let $A$ be the set of roots of the polynomial $x^2-x$ - that is, the set of numbers $r$ such that $r^2-r=0$ - and $B=\{0,1\}$. We claim that $A=B$.

First, let us show that $B\subseteq A$ - that is, both 0 and 1 are roots of $x^2-x$. This is done by a simple verification:
\[0^2-0=0-0=0\quad\quad\mbox{and}\quad\quad 1^2-1=1-1=0\]so they are, indeed, roots of $x^2-x$ - and therefore, $B\subseteq A$.

Now, to prove that $A\subseteq B$ we need to show that those are the only two possible roots.

To do that, let $r$ be any root of $x^2-x$ - that is, $r^2-r=0$. But then, $r^2=r$, by adding $r$ on both sides, and we see that $r=0$ is indeed a solution to this equation ($0^2=0$). So if we assume that $r\neq0$ we can divide both sides by $r$ and get $\dfrac{r^2}{r}=\dfrac{r}{r}$ which is the same as $r=1$, which was a unique solution being $r=1$.

Hence we have proven that any root $r$ of $x^2-x$ is either 0 or 1, and therefore $A\subseteq B$.

Finally, since $A\subseteq B$ and $B\subseteq A$ we can finally say that $A=B$, as we had previously stated.
\end{ex}

\begin{df}
	We say that $A$ is a \textbf{proper subset} of $B$ if $A$ is a subset of $B$, but $B$ isn't a subset of $A$. In this case we use the symbol $A\subset B$.
\end{df}

\begin{ex}
	Consider $A=\N$ the set of natural numbers, and $B=E$ the set of even natural numbers. We clearly have $B\subseteq A$ and $A\not\subseteq B$, so we can see that $B$ is a \textit{proper} subset of $A$ - that is, $B\subset A$.
\end{ex}

Finally, we can use all that we've done so far to construct a very special set - the empty set.

\begin{ex}
	Let $\N$ be the set of natural numbers and let $\phi$ be the proposition ``is not a natural number". For instance, $\phi({\rm car})$ is just ``car is not a natural number", which is true.
	
	Now we can do just as we did before and consider
	\[\N_\phi:=\{n\in\N\mid \phi(n)\}\]that is, the set of all natural numbers which are not natural numbers.
	
	What \textbf{is} this set? Is there any natural number that isn't a natural number? Of course not! So this is a set \textit{which has no elements}.
	
	Take now $\Z$ the set of all integers and let $\psi$ be the proposition ``is not an integer". We can then define, once more,
	\[\Z_\psi:=\{n\in\Z\mid\psi(n)\}\]that is, the set of all integers which aren't integers.
	
	This set is, once again, empty.
	
	This begets the question: $\N_\phi=\Z_\psi$ - that is, are two empty sets always equal?
\end{ex}

\begin{df}
	Given any set $X$ we call the \textbf{empty set defined by $X$} to be the set of all elements of $X$ which aren't elements of $X$, denoted by $\varnothing_X$.
\end{df}

\begin{theorem}
	Given any two sets $A$ and $B$, then $\varnothing_A=\varnothing_B$.
\end{theorem}
\begin{proof}
	If they were different, then there would either be some element of $\varnothing_A$ which is not in $\varnothing_B$, or some element of $\varnothing_B$ which is not in $\varnothing_A$. But both of these are impossible, since both sets are empty.
	
	So they can't be different, and, therefore, $\varnothing_A=\varnothing_B$
\end{proof}
\begin{cor}
	For any set $A$, its empty set $\varnothing_A$ is uniquely determined.
\end{cor}
\begin{cor}
	There a unique empty set.
\end{cor}

\begin{rmk}
	In mathematics, a \emph{corolary} is a result that follows immediately from something that came before it - sometimes even foregoing a proof because of how immediate this conclusion is.
\end{rmk}

\begin{df}
	We're going to define the \textbf{unique empty set} to be the empty set of any set, which will be denoted in symbols by $\varnothing$.
\end{df}

\begin{prop}
	For any set $A$ we have that $\varnothing\subseteq A$. Furthermore, we have that $A\subseteq\varnothing$ if, and only if, $A=\varnothing$.
\end{prop}
\begin{proof}
	If $\varnothing\not\subseteq A$, then there'd be some element in $\varnothing$ that was not in $A$. But $\varnothing$ is empty, therefore $\varnothing\not\subseteq A$ is false, and hence $\varnothing \subseteq A$.
	
	\bigskip
	For the second statement, we clearly have $A\subseteq \varnothing$ if $A=\varnothing$, by definition of set equality.
	
	But if we assume that $A\subseteq\varnothing$, we can now use the first statement of this proof, which proves that $\varnothing\subseteq A$, to conclude, by definition of set equality, that $A=\varnothing$, and this finishes the proof.
\end{proof}

\subsection{United we stand, intersected we... Fall?}

Now that we have a basic understanding of sets and subsets, we're going to build new sets from existing ones.

\begin{df}
	Let $A$ and $B$ be two sets. The \textbf{union} of $A$ and $B$ is another set - denoted by $A\cup B$ defined by the following properties:
	\begin{enumerate}[(a)]
		\item $A\cup B$ contains both $A$ and $B$ as subsets;
		\item Any other set $C$ that contains both $A$ and $B$ as subsets also contains $A\cup B$ as a subset.
	\end{enumerate}
\end{df}

First things first, let us show that this definition makes sense - that is, that given two sets, their union is a unique set:

\begin{lemma}
	Let $A$ and $B$ be two sets, and $C$ and $D$ be two sets satisfying the above definition. Then $C=D$.
\end{lemma}
\begin{proof}
	Since $C$ and $D$ are unions of $A$ and $B$, they contain both of them as subsets (item (a)). Now, since $C$ satisfies (a) and $D$ satisfies (b), we get that $D\subseteq C$. Similarly, since $D$ satisfies (a) and $C$ satisfies (b), we get that $C\subseteq D$.
	
	It follows that $C=D$, and so the union of two sets is indeed well-defined,
\end{proof}

With that out of the way, let us show some examples to build some intuition:

\begin{ex}
	Let $A$ be the set of all dogs and $B$ be the set of all cats. Let $C$ be the set of all animals. We ask: $C=A\cup B$?
	
	Certainly, $C$ satisfies (a) (since all dogs and all cats are animals), but does it satisfy (b)?
	
	Well, certainly not! Because the set $D$ of all mammals also contains $A$ and $B$, but it clearly doesn't contain $C$ (because not every animal is a mammal - for instance, there are birds).
	
	Now we ask: Ok, since $C$ is not the union of $A$ and $B$, maybe $D$ is?
	
	Well, no, because we can consider $E$ - the set of all mammal quadrupeds  - and see that it contains both $A$ and $B$ as susbets, but not $D$.
	
	And so on, and so forth...
\end{ex}

How can we make sure that we don't get an endless regression - that is, we're always inching closer to the result, but never truly getting there?

Well, in formal set theory, for instance ZFC, you can always use your axioms to guarantee the existence of such a set. Here, however, we're going to have to appeal to intuition:

\begin{prop}
	Given two sets $A$ and $B$ and any set $C$ containing both $A$ and $B$, their union is precisely the subset of $C$ given by the proposition $\phi=$ ``is in any one of the sets $A$ or $B$''.
\end{prop}
\begin{proof}
	First, we'll show that $A\subset C_\phi$ and $B\subseteq C_\phi$.
	
	To do that we'll just use the definition: Take $a\in A$ (resp. $b\in B$). Since $A\subseteq C$ (resp. $B\subseteq C$) we have that $a\in C$ (resp. $b\in C$). We then ask: is $\phi(a)$ (resp. $\phi(b)$) true? Well, it trivially is - $\phi(x)$ is true if, and only if $x$ is in $A$ or $B$ - and $a$ (resp. $b$) certainly is. Therefore, for any $a\in A$ (resp. $b\in B$) we can conclude $\phi(a)$ (resp. $\phi(b)$) - and therefore, $a\in C_\phi$ (resp. $b\in C_\phi$). This shows that $A\subseteq C_\phi$ (resp. $B\subseteq C_\phi$) - and therefore, $C_\phi$ satifies item (a) of the definition of set union.
	
	Now, take any set $D$ such that $D$ contains both $A$ and $B$ as subsets. If we show that $D$ also contains $C_\phi$ as a subset, we'll have shown that $C_\phi$ satisfies the definition of union - and therefore it must be the union.
	
	To do that, take any $x\in C_\phi$. Then, by definition, $\phi(x)$ is true - that is, $x\in A$ or $x\in B$. But since both $A\subseteq D$ and $B\subseteq D$ hold, it doesn't matter if $x\in A$ or $x\in B$ is true - as long as one of them is true, we can conclude that $x\in D$. And since this holds for any $x\in C_\phi$, we have just shown that $C_\phi\subseteq D$.
	
	Since the $D$ we chose was general, the result follows.	
\end{proof}

This is important: We now have a way to construct the union of two sets - just take any set containing both of them and restrict it to be only the elements from the original sets.

That, however, requires the existence of some set containing both of them - and that's where ZFC comes in: There's an axiom that states that there always exists a set containing any amount of other sets.

Since we're foregoing axioms here, we're going to provide a ``construction'' that should be enough for most purposes:

\begin{ex}
	Following up on the previous example, we can now see that $A\cup B$ is any one of ``the set of all animals which are cats or dogs'', ``the set of all mammals which are cats or dogs'' or ``the set of all mammal quadrupeds which are cats or dogs'' - any one of those work, by what we've already proven.
	
	We could, however, give a more explicit construction: $A\cup B$ is just the set of all cats and dogs.
\end{ex}
\begin{ex}
	Another, even more constructive example: Let $A=\{a,b,c\}$ and $B=\{c,d,e,f\}$. Then $A\cup B=\{a,b,c,d,e,f\}$ (prove it using the definition if you're not convinced).
\end{ex}

Finally, let's end our discussions on the union with the following alternative characterization of it:

\begin{lemma}
	Let $A$ and $B$ be sets. Then $x\in A\cup B$ if, and only if, $x\in A$ or $x\in B$.
\end{lemma}
\begin{proof}
	One side of this proof is trivial and follows from the definition of set union.
	
	Let us prove then that $x\in A\cup B$ implies $x\in A$ or $x\in B$.
	
	Define $N=\{x\in A\cup B\mid x\notin A\mbox{ and }x\notin B\}$ the collection of all elements of $A\cup B$ which are in neither $A$ nor $B$ - which is, by definition, a subset of $A\cup B$.
	
	We can now define $U=\{x\in A\cup B\mid x\notin N\}$ the collection of all elements of $A\cup B$ which are not in $N$ - which is, by definition, a subset of $A\cup B$.
	
	We claim that $U$ contains $A$ and $B$ as subsets. This is easy to see: Take $y$ in either $A$ or $B$ (doesn't matter which). Since $A\cup B$ contains both of them, $y\in A\cup B$. But since $y$ came from either $A$ or $B$, it cannot be in $N$ (by definition of $N$) - so it must be in $U$ (by definition of $U$). It follows that both $A$ and $B$ are contained in $U$.
	
	But this is a conundrum, because $A\cup B$ is \textit{contained} in every set that contains $A$ and $B$ (by definition of set union) - in particular, since $U$ contains $A$ and $B$ this means that $U$ \textit{also contains} $A\cup B$.
	
	This shows that $U=A\cup B$, and therefore $N=\varnothing$.
	
	Finally, to show that $x\in A\cup B$ implies $x\in A$ or $x\in B$, take any $x\in A\cup B$ and notice, by what we've done, that this is the same as saying that $x\in U$. But, then again, this is the same as saying that $x\notin N$ - that is $x$ is in $A$ or $B$, just as stated. This finishes the proof.
\end{proof}

\bigskip
Going in the opposite direction of unions, there is the concept of intersections. If unions take two sets to build a bigger one, interceptions take two sets to build a smaller one:

\begin{df}
		Let $A$ and $B$ be two sets. The \textbf{intersection} of $A$ and $B$ is another set - denoted by $A\cap B$ defined by the following properties:
	\begin{enumerate}[(a)]
		\item $A\cap B$ is contained in both $A$ and $B$ as a subset;
		\item Any other set $C$ that is contained both $A$ and $B$ as a subset is also contained $A\cap B$ as a subset.
	\end{enumerate}
\end{df}

\begin{rmk}
	Notice that the two definitions are basically the same, just changing, in some sense, the ``order'' of the inclusions $\subseteq$.
\end{rmk}

Now, let us proceed to prove essentially the same results for intersections as we did for unions:

\begin{lemma}
	Let $A$ and $B$ be two sets, and $C$ and $D$ be two sets satisfying the above definition. Then $C=D$.
\end{lemma}
\begin{proof}
	Since $C$ and $D$ are intersections of $A$ and $B$, they are contained in both of them as subsets (item (a)). Now, since $C$ satisfies (a) and $D$ satisfies (b), we get that $C\subseteq D$. Similarly, since $D$ satisfies (a) and $C$ satisfies (b), we get that $D\subseteq C$.
	
	It follows that $C=D$, and so the intersection of two sets is indeed well-defined,
\end{proof}

Contrary to unions, however, we cannot refine intersections. We can, however, still give a construction of the intersection:

\begin{lemma}
	Let $A$ and $B$ be sets. Then $x\in A\cap B$ if, and only if, $x\in A$ and $x\in B$.
\end{lemma}
\begin{proof}
	One side of this proof is trivial and follows from the definition of set intersection.
	
	Let us prove then that $x\in A$ and $x\in B$ implies $x\in A\cap B$.
	
	Define $N=\{x\in A\mbox{ and } x\in B\mid x\notin A\cap B\}$ the collection of all elements which are, at once, in both $A$ and $B$, but not in $A\cap B$ - which is, by definition, a subset of both $A$ and $B$.
	
	We can now define $I=\{x\in A\mbox{ and } x\in B\mid x\notin N\}$ the collection of all elements of both $A$ and $B$ which are not in $N$ - which is, by definition, a subset of both $A$ and $B$. This implies, by definition of set intersection, that $I\subseteq A\cap B$.
	
	We claim now that $I$ contains $A\cap B$ as a subset. This is easy to see: Take any $y\in A\cap B$. Since $A\cap B\subseteq A$ and $A\cap B\subseteq B$, we see that $y\in A$ and $y\in B$. So this $y$ is an element of both $A$ and $B$ which is in $A\cap B$ - which is the definition of an element of $I$. This shows that $y\in I$.
	
	But this is a conundrum, because $A\cap B$ \textit{contains} every set that is contained in both $A$ and $B$ (by definition of set intersection).
	
	This shows that $I=A\cap B$, and therefore $N=\varnothing$.
	
	Finally, to show that $x\in A$ and $x\in B$ implies $x\in A\cap B$, take any $x\in A$ and $x\in B$ and notice, by what we've done, that this is the same as saying that $x\notin N$ (since it is empty). But, then again, this is the same as saying that $x\in I$ - that is $x$ is in $A\cap B$, just as stated. This finishes the proof.
\end{proof}

Finally, let's do some examples:

\begin{ex}
	\begin{itemize}
		\item Let $A=\{1,2,3,4\}$ and $B=\{1,3,5,7,9\}$. Then, $A\cap B=\{1,3\}$.
		\item If $A$ is the set of all even integers, and $B$ is the set of all odd integers, then $A\cap B=\varnothing$.
		\item If $A$ is the set of all cats and $B$ is the set of all brown animals, then $A\cap B$ is the set of all brown cats.
	\end{itemize}
\end{ex}


\bigskip
As a final topic on this section, let us consider another construction.

\begin{df}
	Given any set $X$ we denote the \textbf{set of all subsets of $X$} by $\mc P(X)$ (or $2^X$) and call it the \textbf{power set} of $X$.
\end{df}
\begin{rmk}
	Note that, at this point, we have not define products and sums of sets - even less exponents. So, for now, the symbol $2^X$ is just that - a symbol. It has no meaning resembling the powering of real numbers.
	
	We will, however, as this text progresses, show two reasons why this notation makes sense, and we'll expand it to be able to take any set to the power of any other set.
\end{rmk}

Okay, before anything else, let us do some examples:
\begin{ex}
	Let $A=\{1,2,3\}$. What is $\mc P(A)$? Well, by definition it is the set of all subsets of $A$. Well then - what are the subsets of $A$?
	
	We can list a few: $\varnothing$, $A$, $\{1\}$, $\{2\}$, $\{3\}$, $\{1,2\}$, $\{1,3\}$ and $\{2,3\}$. But are there any others?
	
	Well, assume $B\subseteq A$. Then we can ask if $B$ has any elements or not. If it doesn't, great!, because $B=\varnothing$, which we've already accounted for.
	
	If it does, we can ask if it contains $1$. And then, we can ask if it contains $2$ and $3$. And depending on those answers we can pinpoint $B$ exactly, and see that it is, indeed, in the list above (e.g., if it contains $1$ and $2$, but not $3$, then $B=\{1,2\}$, which is on the list above).
	
	At this point, it is easy to see that\[\mc P(A)=\{\varnothing,\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},A\}.\]
	
	The preceding reasoning, however, gives us our first insight into how to understand the symbol $2^A$: Making a subset of $A$ is the same as asking each element of $A$ if it is, or not, in there.
	
	Imagine the elements of $A$ are cards in a deck and you want to make a hand. Making a hand is the same as going through the deck, card by card, and choosing which cards you want to keep, or not.
	
	Since every card has two options (to be, or not to be), the amount of hands is precisely $2$ to the number of cards.
	
	Note that in this particular example, $2^A=\mc P(A)$ has precisely $2^3=8$ elements, while $A$ has precisely $3$ elements.
\end{ex}

Now that we're talking about power sets, we can define one of the most important concepts of set theory:

\begin{df}
	Let $X$ be a set and $2^X$ its power set. Given any $A\in 2^X$, we define its \textbf{complement} to be the set denoted by $X\setminus A$, which is given by\[X\setminus A:=\{x\in X\mid x\notin A\}.\]
\end{df}

That is, the complement of a set is the collection of all elements that \textit{do not} belong to that set.

\begin{ex}
	Following up on the previous example, let $B=\{1\}$. Then what is $A\setminus B$? Well, by definition, it's the collection of all elements of $A$ that are not in $B$ - that is, $2$ and $3$, so $A\setminus B=\{2,3\}$.
	
	Call $C=A\setminus B$. What is, then, $A\setminus C$? Once again, by definition, it's the set of all elements of $A$ which are not in $C$ - that is, $1$, so $A\setminus C=B$.
\end{ex}

And finally, just before wrapping up this section, let us give one final definition and example:

\begin{df}
	Let $A$ and $B$ be any two sets. We define $A\setminus B$ to be equal to $(A\cup B)\setminus B$ - that is, $A\setminus B$ is the complement of $B$ in $A\cup B$.
\end{df}
\begin{ex}
	Let $A=\{a,b,c,d,e,f,g,h,i,j\}$ and $B=\{a,e,i,o,u\}$. Then $A\setminus B$ is, by definition, the set of all elements of $A\cup B$ which are not in $B$. So writing $A\cup B=\{a,b,c,d,e,f,g,h,i,j,o,u\}$ we see that $A\setminus B$ is just $\{b,c,d,f,g,h,j\}$.
	
	Similarly, $B\setminus A$ is the set of all elements of $A\cup B$ which are not in $A$ - that is, $B\setminus A=\{o,u\}$.
\end{ex}

To really wrap up this section, then, we're gonna make a list of properties for the things we've just described. You're welcome to try to prove them, although most of them are really trivial (that is, they follow immediately from the definitions or a quick observation).

\begin{prop}
	Let $A,B,C$ be any three subsets of a given, fixed, set $X$. Then the following properties always hold:
	\begin{multicols}{2}
		\begin{enumerate}[(1)]
			\item $A\cup(B\cup C)=(A\cup B)\cup C$;
			\item $A\cup\varnothing = A$;
			\item $A\cup X=X$;
			\item $A\cup A=A$;
			\item $A\cap(B\cap C)=(A\cap B)\cap C$;
			\item $A\cap\varnothing=\varnothing$;
			\item $A\cap X=A$;
			\item $A\cap A=A$;
			\item $A\cup(B\cap C)=(A\cup B)\cap(A\cup C)$;
			\item $A\cap(B\cup C)=(A\cap B)\cup(A\cap C)$;
			\item $A\cup B=A$ if, and only if, $B\subseteq A$;
			\item $A\cap B=B$ if, and only if, $B\subseteq A$;
			\item $A\subseteq B$ implies $A\setminus B=\varnothing$;
			\item $A\cap B=\varnothing$ implies $A\setminus B=A$ and $B\setminus A=B$;
			\item $X=(X\setminus A)\cup A$;
			\item $(A\setminus B)\cup(B\setminus A)\cup(A\cap B)=A\cup B$;		
			\item $(A\setminus B)\cap(B\setminus A)=\varnothing$;
			\item $X\setminus A\in 2^X$;
			\item $X\setminus (A\cap B)=(X\setminus A)\cup(X\setminus B)$;
			\item $X\setminus (A\cup B)=(X\setminus A)\cap(X\setminus B)$;
		\end{enumerate}%
	\end{multicols}%
\end{prop}