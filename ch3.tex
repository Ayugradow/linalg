\chapter{Linear algebra in higher dimensions}
\section{Introduction}

Now that we're reasonably comfortable with $\R^2$ (don't worry, we'll come back to it) we're gonna take the next step: Consider the set $\R^3$ and study what are its vectors, how they behave etc.

But, as it's going to become apparent very soon, this is essentially \textbf{not different at all} from what we've already been doing for $\R^2$ - some proofs are actually literally the same, without changing anything.

As a consequence, this chapter shall be much shorter than the previous one. Here's the basic schema of this chapter:

\begin{itemize}
	\item First, we're gonna define $\R^3$ and stablish its properties. At this point we'll notice how it's basically $\R^2$ all over again, with some few minor changes;
	\item Then we're gonna prove whatever is new for $\R^3$ and state which results from $\R^2$ still hold.
\end{itemize}

With that said, let us begin!

\newpage
\subsection{Generalizing to $\R^3$}

By definition, $\R^3$ is the set of all ordered triples of real numbers - like $(1,2,3)$ or $(0,\pi, -4)$ etc. We won't go into as much detail as we did for $\R^2$, but we can prove the following result:

\begin{prop}
	The Euclidean space $E_*$ with distinguished point $*$ is in bijection with $\R^3$.
\end{prop}
\begin{cor}
	Every point in $\R^3$ can be thought of as either a point in the space, or a vector from the origin to its endpoint.
\end{cor}

The idea is pretty simple - we can think of any ordered triple $(x,y,z)\in \R^3$ as a set of ``coordinates'' in a grid system which tells us how to move away from the distinguished point $*$: Move $x$ steps away from $*$ in a certain direction, move $y$ steps away from $*$ in a different direction and then $z$ steps away from $*$ in a third direction.

And we can, as before, define:

\begin{df}
	Given two vectors $v=(v_1,v_2,v_3),u=(u_1,u_2,u_3)\in \R^3$ we define their \textbf{sum} $v+u$ to be the unique vector given by
	\[v+u:=(v_1+u_1,v_2+u_2,v_3+u_3).\]
\end{df}

The intuition here is also very similar as it was for $\R^2$: $0,v,u$ define a unique triangle in the space and, by axiom, there's a unique plane containing this triangle. Therefore, $v+u$ is the parallelogram which is contained in that plane and has $0v$ and $0u$ as its sides.

\begin{prop}
	The addition of vectors in $\R^3$ is associative, commutative, has identity element and inverses.
\end{prop}

\begin{df}
	Let us define a few important subsets of $\R^3$:
	\begin{itemize}
		\item The set $\{0\}\times \R\times \R$ will be called the $\mathds{YZ}$-plane;
		\item The set $\R\times\{0\}\times \R$ will be called the $\mathds{XZ}$-plane;
		\item The set $\R\times \R\times \{0\}$ will be called the $\mathds{XY}$-plane;
		\item The set $\R\times\{0\}\times \{0\}$ will be called the $\mathds{X}$-axis;
		\item The set $\{0\}\times \R\times\{0\}$ will be called the $\mathds{Y}$-axis;
		\item The set $\{0\}\times\{0\}\times \R$ will be called the $\mathds{Z}$-axis.
	\end{itemize}
\end{df}

\begin{df}
	Let $v=(v_1,v_2,v_3)\in \R^3$ and $\lambda\in \R$. We define the \textbf{scalar multiplication} of $v$ and $\lambda$ to be the vector $\lambda v\in\R^3$ given by
	\[\lambda v:=(\lambda v_1,\lambda v_2,\lambda v_3).\]
\end{df}

\begin{prop}
	Scalar multiplication of vectors in $\R^3$ is associative, commutative, has identity element and is distributive over both real and vector addition. Not only that, but $\lambda v=0$ if, and only if, $\lambda =0$.
\end{prop}

\begin{df}
	Given any vectors $v,u\in\R^3$ we define:
	\begin{itemize}
		\item $\R v:=\{w\in \R^3\mid w=\lambda v\mbox{ for some }\lambda\in \R\}$ the \textbf{line through zero containing $v$};
		\item $\R v+\R u:=\{w\in \R^3\mid w=\lambda v+\mu u\mbox{ for some }\lambda,\mu\in \R\}$ the \textbf{plane through zero containing $v$ and $u$}. 
	\end{itemize}
\end{df}

\begin{df}
	A subset $X\subseteq\R^3$ is called a \textbf{subspace} if it is closed under addition and scalar multiplication.
\end{df}

\begin{prop}
	Given any two non-zero vectors $v,u\in\R^3$, then both $\R v$ and $\R v+\R u$ are subspaces of $\R^3$.
\end{prop}
\begin{proof}
	Let $v',v''\in \R v$ - that is, $v'=\lambda' v$ and $v''=\lambda'' v$ for some $\lambda',\lambda''\in \R$. Then, for all $\lambda\in \R$:
	\[v'+v''=(\lambda' v)+(\lambda ''v)=(\lambda'+\lambda'')v\in \R v\]
	\[\lambda v'=\lambda(\lambda' v)=(\lambda \lambda')v\in \R v\]so $\R v$ is closed under addition and scalar multiplication.
	
	
	Let $w,w'\in \R v+\R u$ - that is, $w=\lambda v+\mu u$ and $w'=\lambda' v+\mu' u$. Then, for all $\omega \in \R$:
	\[w+w'=(\lambda v+\mu u)+(\lambda' v+\mu' u)=(\lambda+\lambda ')v+(\mu+\mu')u\in \R v+\R u\]
	\[\omega w=\omega(\lambda v+\mu u)=(\omega \lambda)v+(\omega \mu)u\in \R v+\R u\]so $\R v+\R u$ is closed under addition and scalar multiplication.
	
	This ends the proof.
\end{proof}

Now let's give some geometric definitions and interpret them with linear algebra:

\begin{df}
	Two lines are said to be
	\begin{itemize}
		\item \textbf{Parallel} if they lie in the same plane and don't meet;
		\item \textbf{Skew} if they don't lie in the same plane and don't meet;
		\item \textbf{Transversal} if they meet.
	\end{itemize}

Similarly, two planes are said to be \textbf{parallel} if they don't meet, and transversal if they meet.

Finally, a line and a plane are said to be \textbf{parallel} if they don't meet.
\end{df}

\begin{prop}
Any line in $\R^3$ is of the form $\R v+u$ for some vectors $v,u\in \R^3$, and any plane in $\R^3$ is of the form $\R v+\R u+w$ for some vectors $v,u,w\in \R^3$.
\end{prop}
\begin{proof}
	Let $r\subseteq \R^3$ be any line. Then $r\cap \mathds{YZ}$ is either empty, a single point or $r\subseteq \mathds{YZ}$.
	
	\begin{itemize}
		\item If $r\cap\mathds{YZ}$ is a single point $u$, then the line $r'\subseteq\R^3$ which is parallel to $r$ through zero is such that $r=r'+u$.
		
		\item If $r\cap \mathds{YZ}$ is empty or $r\subseteq\mathds{YZ}$, we then check $r\cap \mathds{XY}$ which can be, again, either empty, a single point or $r\subseteq \mathds{XY}$.
		
		\begin{itemize}
			\item If $r\cap \mathds{XY}$ is a single point, just do as we did above, taking a parallel through zero and adding this single point to it.
			
			\item If $r\cap\mathds{XY}$ is empty, then the fact that $r\cap\mathds{YZ}$ is also empty implies that $r\cap\mathds{XZ}$ is a single point and we can just iterate the construction above.
			
			\item If $r\subseteq\mathds{XY}$, then $r=\mathds{XY}\cap\mathds{YZ}=\mathds{Y}$ so it's already a line through zero.
		\end{itemize}
	\end{itemize}

Either way, we can always show that $r$ is parallel to a line through zero.

\bigskip
Let $\pi\subseteq\R^3$ be any plane. Then:
\begin{itemize}
	\item If $\pi\cap\mathds{YZ}$ is a line $s$, then we can take $\pi'$ the plane parallel to $\pi$ through zero, and $v=s\cap \mathds{Y}$ Then clearly $\pi=\pi'+v$.
	
	\item If $\pi\cap\mathds{YZ}=\varnothing$, then surely $\pi\cap \mathds{X}\neq\varnothing$ (since $\mathds{X}\perp\mathds{YZ}$), and since $\pi\parallel\mathds{YZ}$, we can take $v=\mathds{X}\cap r$ and see that $\pi=\mathds{YX}+v$. 
\end{itemize}

Once again, we see that no matter what, $\pi$ is always parallel to a plane through zero.

This ends the proof.
\end{proof}

\newpage
\subsection{Linear functions in $\R^3$}

In this section we're going to define linear functions for $\R^3$ and see that there's not much new going on.

\begin{df}
	Let $f:\R^3\to \R^3$ be a function. We'll say that $f$ is a \textbf{linear function} if $f(v+u)=f(v)+f(u)$ and $f(\lambda v)=\lambda f(v)$ for all $v,u\in \R^3$ and $\lambda\in \R$.
	
	We'll denote the set of all linear functions in $\R^3$ by $\hom_\R(\R^3,\R^3)$.
\end{df}

\begin{prop}
	Let $f:\R^3\to\R^3$. Then $f$ is linear if, and only if, $$f(x,y,z)=(ax+by+cz,dx+ey+fz,gx+hy+iz)$$ for some $a,b,c,d,e,f,g,h,i\in\R$.
\end{prop}

\begin{lemma}
	Let $f$ be a linear function in $\R^3$. Then $f$ is uniquely determined by how it acts on $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$.
\end{lemma}
\begin{proof}
	Since $f$ is linear, we have that for all $(x,y,z)\in\R^3$ the following equation holds:
	\begin{align*}
	f(x,y,z)&=f((x,0,0)+(0,y,0)+(0,0,z))\\
	&=f(x,0,0)+f(0,y,0)+f(0,0,z)=xf(1,0,0)+yf(0,1,0)+zf(0,0,1)
	\end{align*}So if we put $f(1,0,0)=v$, $f(0,1,0)=u$ and $f(0,0,1)=w$ we see that $f(x,y,z)=xv+yv+zw$.
	
	This ends the proof.
\end{proof}
\begin{cor}
	Let $f:\{(1,0,0),(0,1,0),(0,0,1)\}\to\R^3$. Then there's a unique linear function $f'$ in $\R^3$ such that $f'(x,y,z):=xf(1,0,0)+yf(0,1,0)+zf(0,0,1)$.
\end{cor}

\begin{df}
	We'll denote the vectors $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$ by $e_1,e_2$ and $e_3$, respectively.
\end{df}
\begin{df}
	A finite set $X=\{x_1,x_2,\cdots, x_n\}\subseteq \R^3$ is called a \textbf{base of $\R^3$} or a \textbf{basic set} if given any $v\in \R^3$ there is a unique choice of $\lambda_1,\lambda_2,\cdots,\lambda_n\in R$ such that $(a,b)=\lambda_1x_1+\lambda_2x_2+\cdots+\lambda_n x_n$.
\end{df}

\begin{rmk}
	It follows trivially from this definition that a set $X$ is a base if, and only if, the image of any linear function $f:\R^3\to\R^3$ is entirely determined by $f(X)$: Indeed, given any $v\in \R^3$, we can write it as 
	\[v=\lambda_1x_1+\cdots+\lambda_nx_n\]in a unique way, so $f(v)$ is just
	\[f(v)=\lambda_1f(x_1)+\cdots+\lambda_nf(x_n).\]
\end{rmk}
\begin{df}
	The set $E:=\{e_1,e_2,e_3\}$ is called the \textbf{canonical base} of $\R^3$.
\end{df}

\newpage
\subsection{Subspaces in $\R^3$}

In this section we're going to extend the result that said that subspaces of $\R^2$ had to be zero, lines through zero or $\R^2$ to $\R^3$. With this, we'll be able to see that the sum of two different lines is a plane, the sum of two different lines or two different planes is $\R^3$ and that the intersection of two planes is a line.

\begin{lemma}
	Let $X,Y\leq \R^3$ be any two subspaces, and take $v\in X\cap Y$ and $t\in X+Y$. Then not only do we have $\R v\subseteq X\cap Y$ and $\R t\subseteq X+Y$, but also $X+\R v=X$ and $Y+\R v=Y$.
\end{lemma}
\begin{proof}
	Let $v\in X\cap Y$ and take $u\in \R v$ - that is, $u=\lambda v$ for some $\lambda\in R$.
	
	Since $v\in X$ and $X$ is a subspace, $\mu v\in X$ for all $\mu\in \R$ - in particular, $u=\lambda v\in X$. Similarly for $Y$, since $v\in Y$ and $Y$ is a subspace, $\mu v\in Y$ for all $\mu\in \R$ and so $u=\lambda v\in Y$.
	
	But this tells us that $u\in X\cap Y$, by definition of set intersection. This shows that any vector of $\R v$ is in $X\cap Y$ - so $\R v\subseteq X\cap Y$.
	
	\bigskip
	Similarly for $t$, since $t\in X+Y$, we can write $t=x+y$ for some $x\in X$ and $y\in Y$. Take then $t'\in \R t$ - that is, $t'=\tau t$ for some $\tau\in \R$.
	
	Now, this implies that $t'=\tau t=\tau(x+y)=\tau x+\tau y$ and since $X$ and $Y$ are subspaces, $\tau x\in X$ and $\tau y\in Y$ - so $\tau x+\tau y\in X+Y$. This shows $\R t\subseteq X+Y$.	
	
	\bigskip
	To show the second statement, it suffices to realize that, by the previous statement, every element of $\R v$ already lies in $X$ and that $X$ is a subspace. So taking $u\in \R v$ and $x\in X$, we see that $x+u\in X$, so $X+\R v\subseteq X$.
	
	Conversely, any element of $X$ is, by definition, in the sum $\R v+X$ - for instance, any $x\in X$ can be seen as $0+x$, because, since $\R v$ is a subspace, $0\in \R v$. This shows that $X\subseteq \R v+X$ and ends the proof (because the argument for $Y$ is exactly the same).
\end{proof}

This result is mainly important because it tells us that subspaces are closed under taking subspaces - that is, if you take any subspace $X$ and any element inside it, then the line through that element is still in $X$.

That, combined with the following corollaries, will show that, indeed, subset sum is, in some sense, the ``extension'' of the concept of set union for vector spaces.

\begin{cor}
	Let $X,Y\leq \R^3$ be any two subspaces of $\R^3$ such that $Y\subseteq X$. Then $X+Y=X$.
\end{cor}
\begin{proof}
	Clearly, we already have $X\subseteq X+Y$ by definition of addition of subspaces.
	
	Take then any $x+y\in X+Y$. Since $Y\subseteq X$, we have that $y\in X$, and since $X$ is a subspace, we have that $x+y\in X$. So $X+Y\subseteq X$.
	
	This ends the proof.
\end{proof}
\begin{rmk}
	Compare this to the already known result for sets that $Y\subseteq X$ implies $X\cup Y=Y$.
\end{rmk}
\begin{cor}
	Let $X,Y,Z\leq \R^3$ be any three subspaces such that both or either of $Z\subseteq X$ and $Z\subseteq Y$ hold. Then $X+Y+Z=X+Y$.
\end{cor}
\begin{proof}
	Once again, by definition of subspace addition, we already have that $X+Y\subseteq X+Y+Z$.
	
	Take $x+y+z\in X+Y+Z$. Then, since both or either of $Z\subseteq X$ and $Z\subseteq Y$ holds, we see that $z\in X$ or $z\in Y$.
	
	In the first case, we now use the fact that $X$ is a subspace, so $x+z\in X$ and therefore $x+y+z=(x+z)+y\in X+Y$, which shows that $X+Y+Z\subseteq X+Y$.
	
	In the second case, we use the fact that $Y$ is a subspace, so $y+z\in Y$ and therefore $x+y+z=x+(y+z)\in X+Y$, which shows that $X+Y+Z\subseteq X+Y$.
	
	Either way, the result holds and this finishes the proof.
\end{proof}
\begin{rmk}
	Once again, compare this to the already know result about sets that if $Z\subseteq X$ or $Z\subseteq Y$ then $X\cup Y\cup Z=X\cup Y$.
\end{rmk}

\begin{lemma}
	Let $X,Y,Z\leq \R^3$. Then $$X+(Y\cap Z)=(X+Y)\cap(X+Z)$$$$X\cap(Y+Z)=(X\cap Y)+(X\cap Z).$$
\end{lemma}
\begin{proof}
	Take $x\in X$ and $w\in Y\cap Z$. Then, by definition, $x+w\in X+(Y\cap Z)$. Since $Y$ is a subspace and $w\in Y$, $x+w\in X+Y$. Similarly, since $Z$ is a subspace and $w\in Z$, $x+w\in X+Z$. Therefore, $x+w\in (X+Y)\cap(X+Z)$, so 
	\[X+(Y\cap Z)\subseteq (X+Y)\cap(X+Z).\]
	
	\bigskip
	Conversely, take $v\in(X+Y)\cap(X+Z)$. However, $X\subseteq X+Y$ and $X\subseteq X+Z$ together imply, by definition of intersection, that $(X+Y)\cap(X+Z)\subseteq X$ - so $v\in X$. Finally, since $X\subseteq X+(Y\cap Z)$, this shows that $v\in X+(Y\cap Z)$, so
	\[X+(Y\cap Z)=(X+Y)\cap(X+Z).\]
	
	\bigskip
	The second statement is analogous:
	
	Clearly $X\cap(Y+Z)$ contains both $X\cap Y$ and $X\cap Z$: If we take $y\in X\cap Y$, it is, in particular, in $Y$ and $X$. But since it is in $Y$, it is also in $Y+Z$. Now we see that this $y$ is in both $X$ and $Y+Z$, so it is in $X\cap(Y+Z)$. We can do the same reasoning to show that any $z\in X\cap Z$ is also in $X\cap (Y+Z)$.
	
	Now, by definition of subspace addition, since $X\cap(Y+Z)$ contains both of $X\cap Y$ and $X\cap Z$, it must be contained in their sum - that is, $(X\cap Y)+(X\cap Z)$.
	
	This shows $X\cap(Y+Z)\subseteq (X\cap Y)+(X\cap Z)$.
	
	\bigskip
	Conversely, any element of $(X\cap Y)+(X\cap Z)$ is of the form $v+u$ where $v\in X\cap Y$ and $u\in X\cap Z$. Notice, however, that both $v$ and $u$ lie, in particular, in $X$ - and since $X$ is a subspace, $v+u\in X$.
	
	On the other hand, since $v\in X\cap Y$ and $u\in X\cap Z$, in particular we have $v\in Y$ and $u\in Z$, so $v+u\in Y+Z$.
	
	Since $v+u\in X$ and $v+u\in Y+Z$ we can conclude that $v+u\in X\cap(Y+Z)$ and so
	\[(X\cap Y)+(X\cap Z)\subseteq X\cap(Y+Z).\]
	
	This shows that $X\cap(Y+Z)=(X\cap Y)+(X\cap Z)$ and ends the proof.
\end{proof}
\begin{rmk}
	This is once gain similar to what we had with sets - where unions and intersections distributed over each other.
\end{rmk}

Now that we're done with some more technical results, let's start characterizing stuffs:

\begin{prop}
	Let $v,u\in \R^3$ be any two non-null vectors. Then for any non-null $w\in\R v$ we have that $\R v=\R w$ and for any non-null $t\in \R v+\R u$ such that $t\notin \R v$ and $t\notin \R u$ we have that $\R t+\R u=\R v+\R t=\R v+\R u$.
\end{prop}
\begin{proof}
	If $w\in \R v$, then $w=\lambda v$ for some $\lambda\in \R$. Take then any other $w'\in \R v$. Once again, this means that $w'=\lambda' v$ for some $\lambda'\in \R$. But now, clearly we have
	\[w'=\lambda'v=\lambda'\frac{\lambda}{\lambda}v=\frac{\lambda'}{\lambda}\lambda v=\frac{\lambda'}{\lambda}w\]so $w'\in \R w$ and $\R v\subseteq \R w$.
	
	Conversely, every $w'\in \R w$ is of the form $w'=\omega w$ for some $\omega\in \R$, but since $w=\lambda v$, we have that
	\[w'=\omega w=(\omega \lambda) v\]so $w'\in \R v$ and $\R w\subseteq \R v$.
	
	Therefore $\R v=\R w$.
	
	\bigskip
	To prove the second statement, we proceed analogously: Take $t\in \R v+\R u$. This means that $t=\tau_1 v+\tau_2 u$ for some $\tau_1,\tau_2\in \R$. Notice that both of $\tau_1$ and $\tau_2$ must be non-zero, because otherwise $t$ would be on either or both of $\R v$ and $\R u$.
	
	Given any $t'\in \R v+\R u$, once again we can write it as $t'=\tau'_1 v+\tau_2' u$.
	
	Since $\tau_1\neq0$, we can then do
	\begin{align*}
		t'&=\tau'_1 v+\tau_2' u\\
		&=\tau'_1\frac{\tau_1}{\tau_1}v+\tau_2'u\\
		&=\frac{\tau_1'}{\tau_1}\tau_1v+\tau_2'u\\
		&=\frac{\tau_1'}{\tau_1}(t-\tau_2u)+\tau_2'u\\
		&=\frac{\tau_1'}{\tau_1}t-\frac{\tau_1'}{\tau_1}\tau_2u+\tau_2'u=\frac{\tau_1'}{\tau_1}t+\left(\tau_2'-\frac{\tau_1'}{\tau_1}\tau_2\right)u
	\end{align*}and we see that $t'\in \R t+\R u$.
	
	Since $\tau_2\neq 0$, we can then do
	\begin{align*}
	t'&=\tau'_1 v+\tau_2' u\\
	&=\tau'_1v+\tau_2'\frac{\tau_2}{\tau_2}u\\
	&=\tau'_1v+\frac{\tau_2'}{\tau_2}\tau_2u\\
	&=\tau'_1v+\frac{\tau_2'}{\tau_2}(t-\tau_1v)\\
	&=\tau_1'v+\frac{\tau_2'}{\tau_2}t-\frac{\tau_2'}{\tau_2}\tau_1v\\
	&=\left(\tau_1'-\frac{\tau_2'}{\tau_2}\tau_1\right)v+\frac{\tau_2'}{\tau_2}t
	\end{align*}and we see that $t'\in \R v+\R t$.
	
	So clearly, $t'\in \R v+\R u$ implies both of $t'\in \R t+\R u$ and $t'\in \R v+\R t$ - and therefore, $\R v+\R u$ is contained in both $\R v+\R t$ and $\R t+ \R u$.
	
	\bigskip
	On the other hand, take $x\in \R t+\R u$. This means that $x=\tau t+\mu u$ for some $\tau,\mu\in \R$. But then, since $t=\tau_1v+\tau_2u$, this tells us that
	\[x=\tau t+\mu u=\tau(\tau_1v+\tau_2u)+\mu u=(\tau\tau_1)v+(\tau\tau_2+\mu)u\]so $x\in \R v+\R u$ which shows that $\R t+\R u\subseteq \R v+\R u$.
	
	Similarly, for all $y\in \R v+\R t$ we can write it as $y=\lambda v+\tau t$ for some $\lambda,\tau\in \R$, so
	\[y=\lambda v+\tau t=\lambda v+\tau(\tau_1v+\tau_2u)=(\lambda+\tau\tau_1)v+(\tau\tau_2)u\]and we see $y\in \R v+\R u$, which implies $\R v+\R t\subseteq \R v+\R u$.
	
	\bigskip
	Now, finally, we have $$\R v+\R t\subseteq\R v+\R u\subseteq \R t+\R u$$and
	\[\R t+\R u\subseteq\R v+\R u\subseteq\R v+\R t\]so this implies that $\R v+\R t=\R t+\R u$ and therefore both of them equal $\R v+\R u$.
	
	This ends the proof.
\end{proof}
\begin{rmk}
	This first result is a generalization of a similar result in $\R^2$ which said that two vectors are parallel if, and only if, they lie in the same line through zero.
	
	Indeed, this result tells us not only that, but also that a vector $t$ is in the plane containing two other vectors $v,u$ if, and only if, $v$ is in the plane containing $t,u$ and $u$ is in the plane containing $t,v$.
	
	This gives us a lot of insight for the following definitions:
\end{rmk}

\begin{df}
	Let $v,u\in \R^3$ be two vectors such that $\R v=\R u$. In this case we say that $v$ and $u$ are \textbf{parallel}, which we denote by $v\parallel u$.
\end{df}

\begin{df}
	Let $v,u,w\in\R^3$ be three vectors. We say that they are \textbf{coplanar} if any one of them is in the plane through the origin containing the other two.
	
	In symbols: $v,u,w$ are coplanar if any of $v\in \R u+\R w$, $u\in\R v+\R w$ and $w\in\R v+\R u$ hold.
	
	We denote this by $\plane{v}{u}{w}$.
\end{df}
\begin{lemma}
	Let $v,u,w\in \R^3$. Then $v\parallel u$ implies $\plane{v}{u}{w}$.
\end{lemma}
\begin{proof}
	Since $v\parallel u$, we have that $\R v=\R u$, so, clearly, $u\in \R v+\R w$ and thus $\plane{v}{u}{w}$.
\end{proof}

That is, if two vectors are in the same line, then they are already coplanar.

Let's now use these definitions to start breaking $\R^3$ into smaller pieces:

\begin{lemma}
	The planes $\mb{XY}$, $\mb{YZ}$ and $\mb{ZX}$ are just the sums $\mb{X+Y}$, $\mb{Y+Z}$ and $\mb{Z+X}$, respectively.
\end{lemma}
\begin{proof}
	We'll show that $\mb{XY=X+Y}$. The other are analogous and will be left as an exercise to the reader.
	
	By definition, $\mb{XY}=\R\times \R\times \{0\}$. This means that $v\in \mb{XY}$ if, and only if, $v=(x,y,0)$ for some $x,y\in \R$.
	
	Clearly, then, for all $v\in \mb{XY}$ we can write it as $v=xe_1+ye_2+0e_3=xe_1+y_2$ which shows that $v\in \mb{X+Y}$ - and so $\mb{XY\subseteq X+Y}$.
	
	\bigskip
	Conversely, given any $u\in \mb{X+Y}$ there exists some $x\in \mb X$ and $y\in \mb Y$ such that $u=x+y$. But since $\mb X=\R e_1$, we see that $x=x'e_1$ and since $\mb Y=\Re_2$ we see that $y=y'e_2$, for some $x',y'\in \R$. This tells us that
	\[u=x+y=x'e_1+y'e_2=x'e_1+y'e_2+0e_3=(x',y',0)\]and so $u\in \mb{XY}$.
	
	This shows us that $\mb{XY}\subseteq \mb{X+Y}$, and ends the proof.
\end{proof}

\begin{cor}
	The following equation holds in $\R^3$:
	\[\mb{X+Y+Z}=\R^3=\mb{XY+YZ+ZX}.\]
\end{cor}
\begin{proof}
	The first equation is trivial: $\mb X=\R e_1$, $\mb Y=\R e_2$ and $\mb Z=\R e_3$, by definition, and we know that $E=\{e_1,e_2,e_3\}$ is a base. This means that any vector $v\in \R^3$ can be written with uniquely determined scalars $v_1,v_2,v_3\in\R$ as
	\[v=v_1e_1+v_2e_2+v_3e_3\]this tells us that $v\in \mb{X+Y+Z}$, and so $\R^3\subseteq \mb{X+Y+Z}$.
	
	Conversely, we have that $\mb{X+Y+Z}\subseteq \R^3$ simply by the fact that every element of $\mb{X+Y+Z}$ is, by definition, a sum of vectors, all of which lie in $\R^3$ - and hence so does their sum.
	
	\bigskip
	We would now like to show that $\R^3=\mb{XY+YZ+ZX}$, but in light of the preceding lemma, $\mb{XY+YZ+XZ}$ is just $\mb{X+Y+Y+Z+Z+X}$ which we already know is simply $\mb{X+Y+Z}$ - and this we've already proven to be equal to $\R^3$.
	
	This ends the proof.
\end{proof}
\begin{cor}
	We can weaken the preceding equation a bit:
	\[\mb{XY+YZ=YZ+ZX=ZX+XY}=\R^3.\]
\end{cor}
\begin{proof}
	It follows by the simple observation that all the subspace additions above equal $\mb{X+Y+Z}$.
\end{proof}
\begin{cor}
	Finally, we can weaken it even further:
	\[\mb{XY+Z=YZ+X=ZX+Y}=\R^3.\]
\end{cor}
\begin{proof}
	The same proof as above, since all of these sums equal $\mb{X+Y+Z}$.
\end{proof}
\begin{rmk}
	All of these can be seen as a generalization of the fact that, in $\R^2$, $\mb{X+Y}=\R^2$. Indeed, we're saying that $\R^3$ can be though of as either a set with three axes ($\mb{X+Y+Z}=\R^3$), two planes ($\mb{XY+YZ=YZ+ZX=ZX+XY}=\R^3$) or an axis and a plane ($\mb{XY+Z=YZ+X=ZX+Y}=\R^3$).
\end{rmk}

We can now use this to start classifying all subspaces of $\R^3$:

\begin{lemma}
	Let $v,u\in \mb{XY}$ be two non-null non-parallel vectors. Then $\R v+\R u=\mb{XY}$.
\end{lemma}
\begin{proof}
	Since both $v,u\in\mb{XY}$ we can write them as $v=(v_1,v_2,0)$ and $u=(u_1,u_2,0)$.
	
	If $v_2$ or $u_2$ are zero, then we're done: For instance, if $v_2=0$, then $v=(v_1,0,0)\in \mb X$, and since $v$ is non-null, $v_1\neq 0$. So we can define $v':=\dfrac{u_1}{v_1}v$ and see that
	\[v'=\frac{u_1}{v_1}(v_1,0,0)=\left(u_1,0,0\right)\]so
	\[u-v'=(0,u_2,0)\in \mb Y.\]
	
	But since $v\nparallel u$, we cannot have $u_2=0$. Therefore, we can write
	\[e_1=\frac{1}{v_1}v\in \R v\]
	\[e_2=\frac{1}{u_2}(u-v')\in \R v+\R u,\]so $\R v +\R u$ contains both $\mb X$ and $\mb Y$ and hence it contains $\mb{XY}$.
	
	If, instead of $v_2=0$ we had assumed that $u_2=0$ we would have arrived at a similar conclusion.
	
	\bigskip
	Similarly, we can do the same consideration for the cases where either $v_1$ or $u_1$ are zero. For instance, if $u_1=0$, then, since $u$ is non-null, we have that $u_2\neq 0$, so $u':=\dfrac{v_2}{u_2}u$ is such that
	\[u'=\frac{v_2}{u_2}(0,u_2,0)=(0,v_2,0)\in \mb Y\]and
	\[v-u'=(v_1,v_2,0)-(0,v_2,0)=(v_1,0,0)\in \mb X\]so, once again, $\R v+\R u$ contains both $e_1$ and $e_2$ and hence it contains $\mb{XY}$.
	
	Once more, we can do the same reasoning for the case where $v_1=0$ and arrive at the same conclusion.
	
	\bigskip
	Finally, let's assume that all of $v_1,v_2,u_1,u_2$ are non-zero. In this case, we define $v':=\dfrac{u_2}{v_2}v$ and see that
	\[v'=\frac{u_2}{v_2}v=\left(\frac{u_2}{v_2}v_1,u_2,0\right).\] For the sake of simplicity, let's call $v_1':=\dfrac{u_2}{v_2}v_1$ so $v'$ becomes just $(v_1',u_2,0)$.
	
	Now, once more, we see that
	\[u-v'=(u_1-v_1',0,0)\in\mb X.\] We claim that $u_1-v_1'\neq 0$ or, in other words, that $u_1\neq v_1'$. Indeed, if they were equal we would have
	\[u=(u_1,u_2,0)=\left(v_1',u_2\frac{v_2}{v_2},0\right)=\left(\dfrac{u_2}{v_2}v_1,\frac{u_2}{v_2}v_2,0\right)=\frac{u_2}{v_2}(v_1,v_2,0)=\frac{u_2}{v_2}v\]in other words, if $u_1=v_1'$ then $v\parallel u$. But we're assuming $v\nparallel u$, so, therefore, $u_1\neq v_1'$.
	
	This shows that we can divide $u-v'$ by $u_1-v_1'$ and get $e_1$, so $e_1\in \R v+\R u$.
	
	\bigskip
	Similarly, since $v_1\neq 0$, we can define $v'':=\dfrac{u_1}{v_1}v$, so
	\[v''=\left(u_1,\frac{u_1}{v_1}v_2,0\right)\]and, once again, we'll call $v''_2:=\frac{u_1}{v_1}v_2$, so $v''=(u_1,v_2'',0)$. Now, clearly,
	\[u-v''=(0,u_2-v_2'',0)\in\mb Y\]and we can, using the same arguments as before, show that $u_2=v_2''$ if, and only if, $v\parallel u$ - which tells us (since $v\nparallel u$) that if we divide $u-v''$ by $u_2-v''_2$ we obtain $e_2$. This tells us that $e_2\in \R v+\R u$.
	
	Since both $e_1,e_2\in\R v+\R u$ we can further see that $\mb{XY}\subseteq \R v +\R u$. This ends the proof.	
\end{proof}

\begin{cor}
	The same result holds for two non-null non-parallel vectors in $\mb YZ$ and $\mb ZX$ with essentially the same proof.
\end{cor}

\begin{rmk}
	This shows that the smallest subspace of the coordinate planes containing two non-parallel vectors is the plane itself.
	
	We will now generalize this for arbitrary planes through zero:
\end{rmk}

\begin{prop}
	Let $v,u\in\R^3$ be  two non-null non-parallel vectors and $\pi\subseteq\R^3$ be any plane through zero such that $v,u\in \pi$. Then $\R v+\R u=\pi$.
\end{prop}
\begin{proof}
	This proof follows in the same spirit as before: Let $\pi=\R p+\R q$ be any plane through zero. We're gonna show that $\R v+\R u$ contains both $p$ and $q$ - and thus contains $\pi$.
	
	Now, $v,u\in\pi$ implies the existence of $\lambda_1,\lambda_2,\mu_1,\mu_2\in \R$ such that
	\[v=\lambda_1 p+\lambda_2 q\]
	\[u=\mu_1p+\mu_2q.\]
	
	First, notice that we cannot have $\lambda_1=\lambda_2=\mu_1=\mu_2=0$ because that would imply $v=0=u$ and we're assuming they're both non-null. This means that, at least one of $\lambda_1,\lambda_2$ and one of $\mu_1,\mu_2$ must be non-zero.
	
	If either $\lambda_2$ or $\mu_2$ are zero, then we're done: For instance, if $\lambda_2=0$ then $v=\lambda_1p\in\R p$, and since $v$ is non-null, $\lambda_1$ cannot be zero. So we can define $v':=\dfrac{\mu_1}{\lambda_1}v$ and see that
	\[v'=\frac{\mu_1}{\lambda_1}v=\mu_1p\]so
	\[u-v'=(\mu_1p+\mu_2q)-\mu_1p=\mu_2q.\] Now, since $v\nparallel u$, this implies that $\mu_2$ cannot be zero (otherwise, we'd have $v=\lambda_1p$ and $u=\mu_1p$, so they'd be parallel). So we can write
	\[p=\frac{1}{\lambda_1}v\in\R v\]
	\[q=\frac{1}{\mu_2}(u-v')\in\R v+\R u,\]so $\R v+\R u$ contains both $p$ and $q$ - and thus it contains $\pi$.
	
	If instead of $\lambda_2$ we had taken $\mu_2$ to be non-zero, we would have arrived at a similar conclusion using essentially the same steps.
	
	\bigskip
	Similarly, we can do the same consideration for the cases where either $\lambda_1$ or $\mu_1$ are zero. For instance, if $\mu_1=0$, then, since $u$ is non-null, we have that $\mu_2\neq 0$, so we can define $u':=\dfrac{\lambda_2}{\mu_2}u$ in such a way that
	\[u'=\frac{\lambda_2}{\mu_2}u=\lambda_2q,\]so
	\[v-u'=(\lambda_1p+\lambda_2q)-\lambda_2q=\lambda_1p.\]Once again, since $v\nparallel u$, we cannot have $\lambda_1=0$. So we can write
	\[q=\frac{1}{\mu_2}u\]
	\[p=\frac{1}{\lambda_1}(v-u')\]so $\R v+\R u$ contains both $p$ and $q$ - and thus contains $\pi$.
	
	\bigskip
	Finally, if none of $\lambda_1,\lambda_2,\mu_1,\mu_2$ are zero, we define, once more, $v':=\dfrac{\mu_2}{\lambda_2}v$ and see that 
	\[v'=\frac{\mu_2}{\lambda_2}v=\frac{\mu_2}{\lambda_2}\lambda_1p+\mu_2q.\]For the sake of simplicity, let's call $\lambda'_1:=\frac{\mu_2}{\lambda_2}\lambda_1$, so $v'=\lambda_1'p+\mu_2q$.
	
	Now, once again, we see that
	\[u-v'=(\mu_1p+\mu_2q)-(\lambda'_1p+\mu_2q)=(\mu_1-\lambda_1')p.\]We claim that $\mu_1\neq \lambda_1'$. Indeed, if they were equal we would have
	\[u=\mu_1p+\mu_2q=\lambda_1'p+\mu_2\frac{\lambda_2}{\lambda_2}q=\frac{\mu_2}{\lambda_2}\lambda_1p+\frac{\mu_2}{\lambda_2}\lambda_2q=\frac{\mu_2}{\lambda_2}(\lambda_1p+\lambda_2q)=\frac{\mu_2}{\lambda_2}v,\]which contradicts the fact that $v\nparallel u$. So $\mu_1\neq\lambda_1'$.
	
	This tells us that
	\[p=\frac{1}{\mu_1-\lambda_1'}(u-v')\]so $p\in\R v+\R u$.
	
	\bigskip
	Analogously, if we define $v'':=\dfrac{\mu_1}{\lambda_1}v$ we can see that
	\[v''=\frac{\mu_1}{\lambda_1}v=\mu_1p+\frac{\mu_1}{\lambda_1}\lambda_2q.\]For the sake of simplicity, let's call $\lambda_2'':=\frac{\mu_1}{\lambda_1}\lambda_2$, so $v''=\mu_1p+\lambda_2''q$.
	
	Once again, we can see that
	\[u-v''=(\mu_1p+\mu_2q)-(\mu_1p+\lambda_2''q)=(\mu_2-\lambda_2'')q\]and we can see, by the same reasoning, that $\mu_2=\lambda_2''$ if, and only if, $v\parallel u$. Since $v\nparallel u$, then, we can conclude that $\mu_2\neq \lambda_2''$ and so
	\[q=\frac{1}{\mu_2-\lambda_2''}(u-v'')\]which tells us that $u\in\R v+\R u$.
	
	\bigskip
	Combining these two, we see that both $p$ and $q$ are in $\R v+\R u$ - and thus $\pi$ is also in $\R v+\R u$. This ends the proof.
\end{proof}

\begin{rmk}
	This tells us that just like lines are determined by any non-null vector in them, planes are determined by any two non-null non-parallel vectors in them.
\end{rmk}

From this it now follows that:

\begin{cor}
	Let $v,u,w\in\R^3$ be three non-null non-coplanar vectors. Then $\R v+\R u+\R w=\R^3$.
\end{cor}
\begin{proof}
	We'll prove that $\R v+\R u+\R w$ contains all of the planes $\mb{XY}$, $\mb{YZ}$ and $\mb{ZX}$. Since they're pretty much the same proof, we'll only prove it for $\mb{XY}$ and leave the other two as exercises to the reader.
	
	To start it off, write $v=(v_1,v_2,v_3)$, $u=(u_1,u_2,u_3)$ and $w=(w_1,w_2,w_3)$.
	
	Now, if any two of the last coordinates above are zero - say, $v_3=u_3=0$ - then clearly $\R v+\R u$ already contains $\mb{XY}$ (since $v,u,w$ are non-coplanar and, therefore, $v,u$ are non-parallel).
	
	Therefore, we can assume that, at most, one of the last coordinates is zero.
	
	Assume, without loss of generality, that $u_3$ and $w_3$ are certainly non-zero (so $v_3$ can be zero). Then we can define $u':=\dfrac{w_3}{u_3}u$ and see
	\[u'=\dfrac{w_3}{u_3}u=(u'_1,u_2',w_3)\]where $u'_1:=\dfrac{w_3}{u_3}u_1$ and $u_2':=\dfrac{w_3}{u_3}u_2$.
	
	Now:
	\[w-u'=(w_1,w_2,w_3)-(u_1',u_2',w_3)=(w_1-u_1',w_2-u_2',0).\]
	
	Now we have two possible cases:
	\begin{itemize}
		\item If $v_3=0$, then $v=(v_1,v_2,0)$. We claim that $v\nparallel (w-u')$. Indeed, since $w-u'=w-\dfrac{w_3}{u_3}u\in \R u+\R w$, we have that if $v\parallel (w-u')$ then $v\in \R u+\R w$.
		
		But we're assuming, by hypothesis, that $v,u,w$ are non-coplanar, so we get that $v$ cannot be parallel to $w-u'$.
		
		This ends the reasoning for this case, because now $v$ and $w-u'$ are two non-parallel vectors in $\mb{XY}$ and, thus, by the preceding proposition, $\mb{XY}=\R v+\R(w-u')$. But clearly, we have that $w-u'\in \R u+\R w$, which implies $\R(w-u')\subseteq \R u+\R w$ and thus
		
		\[\mb{XY}\subseteq\R v+\R u+\R w.\]
		
		\item If, however, $v_3$ does not equal zero, we have to do a small adjustment: Let, as before, $v':=\dfrac{w_3}{v_3}v$, so
		\[v'=\frac{w_3}{v_3}v=(v_1',v_2',w_3)\]where $v_1'=\dfrac{w_3}{v_3}v_1$ and $v_2'=\dfrac{w_3}{v_3}v_2$. Now:
		
		\[w-v'=(w_1,w_2,w_3)-(v_1',v_2',w_3)=(w_1-v_1',w_2-v_2',0).\]We claim that $(w-v')\nparallel(w-u')$. Indeed, if they were parallel then there would be some $\lambda\in \R$ such that $(w-v')=\lambda(w-u')$. But then:
		\begin{gather*}
			w-v'=\lambda(w-u')\\
			w-\dfrac{w_3}{v_3}v=\lambda\left(w-\dfrac{w_3}{u_3}u\right)\\
			w-\dfrac{w_3}{v_3}v=\lambda w-\dfrac{\lambda w_3}{u_3}u\\
			-\dfrac{w_3}{v_3}v=\lambda w-\dfrac{\lambda w_3}{u_3}u-w\\
			-\dfrac{w_3}{v_3}v=(\lambda-1) w-\dfrac{\lambda w_3}{u_3}u\\
			\dfrac{w_3}{v_3}v=\dfrac{\lambda w_3}{u_3}u-(\lambda-1) w
		\end{gather*}and since we're assuming that $w_3\neq 0$, we see that 
		\[v=\frac{v_3}{w_3}\left(\dfrac{\lambda w_3}{u_3}u-(\lambda-1) w\right)=\frac{\lambda v_3}{u_3}u-\frac{(\lambda-1)v_3}{w_3}w\]and so $v\in\R u+\R w$. But this contradicts our initial hypothesis that $v,u,w$ are non-coplanar.
		
		This shows that, indeed, $(w-v')\nparallel(w-u')$ and since they're both in $\mb{XY}$ and are non-parallel, we see, by the preceding proposition, that $\mb{XY}\subseteq\R(w-v')+\R(w-u')$.
		
		Finally, $(w-v')\in\R v+\R w$ and $(w-u')\in\R u+\R w$ implies that $\R(w-v')+\R(w-u')\subseteq\R v+\R u+\R w$.
		
		Combining these two we get that $\mb{XY}\subseteq \R v+\R u+\R w$.
	\end{itemize}

Either way, we can conclude that if $v,u,w$ are non-coplanar, then $\mb{XY}\subseteq \R v+\R u+\R w$.

\bigskip
Like we said at the beginning, we can repeat this same argument by eliminating the second or first coordinates to show that both $\mb{YZ}$ and $\mb{ZX}$ are also in $\R v+\R u+\R w$.

This shows that $\mb{XY+YZ+ZX}\subseteq\R v+\R u+\R w$.

But we've already proven that $\mb{XY+YZ+ZX}=\R^3$, so $\R^3\subseteq\R v+\R u+\R w\subseteq\R^3$ shows us that 
\[\R v+\R u+\R w=\R^3\]which ends the proof.
\end{proof}

Now we're ready to fully classify all subspaces in $\R^3$:

\begin{theorem}
	The only possible subspaces in $\R^3$ are zero, lines through zero, planes through zero and $\R^3$.
\end{theorem}
\begin{proof}
	Let $X$ be a subspace in $\R^3$. If $X$ has only one point, then it's zero (since all subspaces have zero).
	
	If it has more than one point, say $v$, then $\R v\subseteq X$. Now, if $X=\R v$, then $X$ is a line through zero.
	
	If not, then there's a point $u\in X$ such that $v\nparallel u$. So $\R v+\R u\subseteq X$. If $X=\R v+\R u$, then $X$ is a plane through zero.
	
	If not, then there's a point $w\in X$ which is non-coplanar with $v,u$. So $ \R v+\R u+\R w\subseteq X$.
	
	But we now know that since $v,u,w$ are non-coplanar, then $\R v+\R u+\R w=\R^3$, so this shows that $\R^3\subseteq X\subseteq \R^3$ - and so $X=\R^3$.
	
	This ends the proof.
\end{proof}

\begin{cor}
	Let $X,Y\leq \R^3$ be two distinct subspaces of $\R^3$. Then:
	\begin{enumerate}[a)]
		\item If $X=Y=0$, then $X+Y=X\cap Y=0$;
		\item If $X$ and $Y$ are lines, then $X+Y$ is a plane and $X\cap Y=0$;
		\item If $X$ and $Y$ are planes, then $X+Y=\R^3$ and $X\cap Y$ is a line.
	\end{enumerate}
\end{cor}
\begin{proof}
	\begin{enumerate}[a)]
		\item This item is trivial.
		
		\item Let $X=\R v$ and $Y=\R u$ be two lines. We know that $X\neq Y$ if, and only if, $v\nparallel u$. But we also know that $v\nparallel u$ if, and only if, $\R v+\R u$ is a plane and $\R v\cap\R u=0$. This proves this item.
		
		\item Let $X=\R v+\R u$ and $Y=\R p+\R q$. We know that $X\cap Y$ is a subset of both $X$ and $Y$ which is also a subspace of $\R^3$. Therefore, $X\cap Y$ is either zero, a line through zero, a plane through zero or $\R^3$.
		
		It certainly cannot be $\R^3$, since $X\subset\R^3$. It also cannot be a plane, otherwise there would be a plane inside both $X$ and $Y$ which would, at once, be different from both of them and contain two non-parallel vectors - which is impossible.
		
		So it's either a line or zero.
		
		But then, this tells us that at least one triple in $\{v,u,p,q\}$ is non-coplanar - and hence that the sum of the lines through them is $\R^3$ - so we already get that $X+Y=\R^3$.
		
		For instance, if $v,u,p$ are non-coplanar, then $X+Y=\R v+\R u+\R p=\R^3$. But this means that there exist some $q_1,q_2,q_3\in\R$ such that 
		\[q=q_1v+q_2u+q_3p.\]
		
		But we can rearrange this into
		\[-q_3p+q=q_1v+q_2u\]which tells us that $-q_3p+q\in \R v+\R u=X$. But $-q_3p+q\in\R p+\R q= Y$, by definition, so we have that $-q_3p+q\in X\cap Y$ and therefore, $\R (-q_3p+q)\subseteq X\cap Y$.
		
		So we know that $X\cap Y$ contains a line and isn't a plane - so it can only be that line.
		
		This shows that $X\cap Y$ is a line and $X+Y=\R^3$.
	\end{enumerate}

This ends the proof.
\end{proof}
\begin{rmk}
	This tells us that these subspaces behave exactly like their namesake geometric counterparts: The whole space is bigger than planes, which are bigger than lines, which are bigger than points; planes meet in lines, and lines meet in points; two lines determine a unique plane, and two planes determine a unique space.
\end{rmk}

\newpage
\subsection{Spanning sets and linear dependency in $\R^3$}

Like we said previously, this section wasn't really necessary for $\R^3$. In $\R^3$, although not entirely necessary, it starts to become a little less useless - even if only for introducing useful notation.

\begin{df}
	Let $v\in \R^3$ be any vector. We define the \textbf{subspace spanned by $v$} to be the subspace $\spen{v}$ defined by
	\[\spen{v}:=\R v.\]
	
	Analogously, given any finite set $X=\{x_1,x_2,\cdots,x_n\}\subseteq\R^3$, we define the \textbf{subspace spanned by $X$} to be the subspace $\spen X$ defined by
	\[\spen X:=\R x_1+\R x_2+\cdots+\R x_n.\]
\end{df}

Now we're going to introduce a notation that will follow and haunt us forever: The sigma notation.

\begin{df}
	Let $X=\{x_1,x_2,\cdots,x_n\}$ be a finite set. We denote the \textbf{sum of all elements of $X$} by the symbol $\dps \sum_{i=1}^n x_i$. In other words,
	\[\sum_{i=1}^nx_i:=x_1+x_2+\cdots+x_n\]so it's just a shorthand notation for not writing long sums.
\end{df}

\begin{rmk}
	At this point there are two remarks that need to be made:
	
	First, how to interpret the sigma notation. The $i$ is called the summation index. The $i=1$ below the $\sum$ symbol means ``we'll start making $i=1$ and the $n$ above the $um$ symbol means ``we'll stop when $i=n$. They're called, respectively, the summation limits/extremes/starting and ending point.
	
	So then the summation proceeds by starting at the starting index and then increasing one by one until it reaches the ending index.
	
	
	\bigskip
	Second, we've already proven in the set theory chapter that any finite set is in bijection with a natural number - that's why we can take any finite set $X$ and give its elements numbered indices.
	
	If $X$ was infinite, however, notice that it wouldn't be possible: For instance, we cannot give natural numbers as indices to the elements of $\R$.
\end{rmk}

\begin{ex}
	Let $X=\{x_1,x_2,\cdots,x_n\}\subseteq \R^3$. Then we can write
	\[\spen X=\sum_{i=1}^n \R x_i\]which is way more compact.
	
	Let $E$ be the canonical base of $\R^3$. Then 
	\[\spen E=\sum_{i=1}^3 \R e_i=\R e_1+\R e_2+\R e_3=\mb{X+Y+Z}=\R^3.\]
\end{ex}

In some sense, the subspace spanned by a set is the smallest subspace containing that set. This can be made precise with the following statement:

\begin{lemma}
	Let $X\subseteq\R^3$ be a finite set and let 
	\[\mc X:=\{Y\leq \R^3\mid X\subseteq Y\}\]be the set of all subspaces of $\R^3$ containing $X$.
	
	Then, if we denote by $\dps\bigcap\mc X$ the ``intersection of all elements of $\mc X$'', we have
	\[\spen X=\bigcap\mc X.\]
\end{lemma}
\begin{proof}
	First things first, do notice that $X$ belongs to both sides.
	
	Clearly, $\spen X$ is a subspace of $\R^3$ containing $X$, so $\spen X\in \mc X$, which tells us that $$\bigcap\mc X\subseteq\spen X.$$
	
	On the other hand, let $Y\in\mc X$. We're going to show that $\spen X\subseteq Y$.
	
	But this is obvious: Since $Y$ is a subspace, it is closed under addition and scaling. But since $Y$ contains $X$, any sum and scaling of elements of $X$ is also in $Y$. This means that the set of all sums and scalings of elements of $X$ (that is, $\spen X$) is contained in $Y$.
	
	So
	\[\spen X\subseteq Y.\]
	
	Finally, notice that $\bigcap\mc X$ is also a subspace of $\R^3$ containing $X$ (because, as stated, $X\subseteq\bigcap\mc X$ and intersection of subspaces is a subspace) - that is, $\bigcap \mc X\in \mc X$. So, by what we just did, any element ov $\mc X$ contains $\spen X$ - in particular, $\bigcap\mc X$ contains $\spen X$ - that is,
	\[\spen X\subseteq\bigcap\mc X.\]
	
	This ends the proof.
	
\end{proof}

So it makes sense to think of the span of a set of vectors as the ``smallest'' vector space containing that set of vectors. Now we can start working with linear dependency:

\begin{df}
	Let $X\subseteq\R^3$ be a finite set. We say that $X$ is \textbf{linearly dependent} if there is a proper subset $Y\subset X$ such that $\spen X=\spen Y$.
	
	Conversely, we say that $X$ is \textbf{linearly independent} if for all proper subsets $Y\subset X$, we have that $\spen Y\subset \spen X$.
\end{df}

\begin{ex}
	Let $E=\{e_1,e_2,e_3\}\subseteq \R^3$. We claim that $E$ is linearly independent:
	
	Indeed, $\mc P(E)=\{\varnothing,\{e_1\},\{e_2\},\{e_3\},\{e_1,e_2\},\{e_1,e_3\},\{e_2,e_3\},E\}$, and we know that $\spen E=\R^3$.
	
	So:
	\begin{align*}
		\spen \varnothing &= 0\subset \R^3\\
		\spen \{e_1\}&=\mb X\subset\R^3\\
		\spen \{e_2\}&=\mb Y\subset\R^3\\
		\spen \{e_3\}&=\mb Z\subset\R^3\\
		\spen \{e_1,e_2\}&=\mb {XY}\subset\R^3\\
		\spen \{e_1,e_3\}&=\mb{ XZ}\subset\R^3\\
		\spen \{e_2,e_3\}&=\mb {YZ}\subset\R^3
	\end{align*}and we see that any proper subset of $E$ spans a proper subset of $\spen E$ - so, by definition, $E$ is linearly independent.
	
	\bigskip
	On the other hand, let $X=\{e_1,e_2,(1,-8,0)\}$ We claim that $X$ is linearly dependent.
	
	To see that, first we need to compute $\spen X$:
	\begin{align*}
		\spen X&=\R e_1+\R e_2+\R(1,-8,0)\\
		&=\mb{X+Y}+\R(1,-8,0)\\
		&=\mb{XY}+\R(1,-8,0)=\mb{XY}
	\end{align*} since $(1,-8,0\in \mb{XY})$.
	
	Now, clearly, $\{e_1,e_2\}\subset X$ is a proper subset such that 
	\[\spen\{e_1,e_2\}=\mb{XY}=\spen X\]so $X$ is linearly dependent.
\end{ex}

This gives  us great insight on the following proof:

\begin{lemma}
	Let $X\subseteq \R^3$ be a finite set. Then $X$ is linearly dependent if, and only if, there is some $x\in X$ such that $\spen(X\setminus\{x\})=\spen X$.
\end{lemma}
\begin{proof}
	Clearly, if there is some $x\in X$ such that $\spen(X\setminus\{x\})=\spen X$, then $X\setminus\{x\}$ is a proper subset of $X$ which spans the same subspace as $X$, so $X$ is linearly dependent.
	
	\bigskip
	Conversely, if $X$ is linearly dependent, then there's some proper subset $Y\subset X$ such that $\spen Y=\spen X$.
	
	Now, let $Z:=X\setminus Y$. Choose any $z\in Z$. We claim that $\spen (X\setminus \{z\})=\spen X$.
	
	This is obvious: Since $Y=X\setminus Z$, we have that $Y\subseteq X\setminus \{z\}$, so $\spen Y\subseteq \spen(X\setminus \{z\})$.
	
	But $(X\setminus\{z\})\subseteq X$, so $\spen(X\setminus\{z\})\subseteq\spen X$.
	
	Finally, since $\spen Y=\spen X$ we get the following expression:
	\[\spen X=\spen Y\subseteq \spen(X\setminus\{z\})\subseteq\spen X\]and therefore $\spen (X\setminus\{z\})=\spen X$.
	
	This ends the proof.
\end{proof}
\begin{cor}
	Let $X\subseteq\R^3$ be a finite set. Then $X$ is linearly dependent if, and only if, there's some $x\in X$ such that $x\in \spen(X\setminus\{x\})$.
\end{cor}
\begin{proof}
	If there's some $x\in X$ such that $x\in \spen(X\setminus\{x\})$, then $\R x\subseteq \spen(X\setminus\{x\})$, so
	\[\spen X=\sum_{x_i\in X}\R x_i=\sum_{x_i\in X\setminus \{x\}}\R x_i+\R x=\spen(X\setminus \{x\})+\R x=\spen (X\setminus \{x\})\]and $X$ is linearly dependent.
	
	\bigskip
	Conversely, if $X$ is linearly dependent, then there's some $x\in X$ such that $\spen X=\spen(X\setminus\{x\})$. But this means that
	\[\spen X=\spen(X\setminus \{x\})+\R x=\spen (X\setminus \{x\})\]and hence that $\R x\in \spen(X\setminus\{x\})$. But this happens if, and only if, $x\in \spen (X\setminus \{x\})$.
	
	This ends the proof.
\end{proof}
\begin{cor}
	Let $X=\{x_1,x_2,\cdots,x_n\}\subseteq\R^3$ be a finite set. Then $X$ is linearly dependent if, and only if, there are real numbers $a_1,a_2,\cdots,a_n\in \R$ not all zero such that
	\[a_1x_1+a_2x_2+\cdots+a_nx_n=0.\]
\end{cor}
\begin{proof}
	Let $X$ be linearly dependent. Then there's some $x\in X$ which is spanned by $X\setminus\{x\}$. Let $X\setminus\{x\}=\{x'_1,x'_2,\cdots,x'_{n-1}\}$, just to give those elements a name.
	
	But then, $x\in\spen X\setminus\{x\}$ implies the existence of some real numbers $a_1,a_2,\cdots,a_{n-1}\in \R$ such that
	\[x=a_1x_1'+a_2x_2'\cdots+a_{n-1}x'_{n-1}\]and hence
	\[a_1x_1'+a_2x'_2+\cdots+a_{n-1}x'_{n-1}+(-1)x=0.\]
	
	So there are real numbers $a_1,a_2,\cdots,a_{n-1},(-1)\in \R$ not all zero (since one of them is $-1$) such that
	\[a_1x_1'+a_2x'_2+\cdots+a_{n-1}x'_{n-1}+(-1)x=0.\]
	
	\bigskip
	Conversely, if there are real numbers $a_1,a_2,\cdots,a_n\in \R$ not all zero such that
	\[a_1x_1+a_2x_2+\cdots+a_nx_n=0,\]let us assume (without loss of generality) that $a_1\neq 0$. If it isn't you can just do the same reasoning by replacing the first non-null index with 1, and the proof will still work.
	
	Since $a_1\neq 0$, we can rewrite the above equation as
	\[x_1+\frac{a_2}{a_1}x_2+\cdots+\frac{a_n}{a_1}x_n=0.\]Let $a_i':=\dfrac{a_i}{a_1}$ for all $i\leq n$. Then this is just
	\[x_1+a'_2x_2+\cdots+a'_nx_n=0\] which allows us to do
	\[x_1=-a'_2x_2-a'_3x_3-\cdots-a'_nx_n.\]But this tells us that $x_1\in\spen X\setminus\{x_1\}$ - and so $X$ is linearly dependent.
	
	This ends the proof.
\end{proof}

So, all these results together tell us that a set being linearly dependent means that one of the vectors is ``superfluous'' in the sense that it is already spanned by the other vectors in the set. Conversely, a set being linearly independent means that none of the vectors are superfluous, and you need each one of them to span the whole set.

But this has an obvious consequence in $\R^3$:

\begin{lemma}
	Let $X\subseteq \R^3$ be a finite set. If $\# X>3$ then $X$ is linearly dependent.
\end{lemma}
\begin{proof}
	This is obvious. If $X=\{x_1,x_2,x_3,\cdots,x_n\}$ with $n>3$, then we can consider cases:
	
	\begin{itemize}
		\item $\{x_1,x_2,x_3\}$ is linearly dependent.
		
		This would imply that one of them is already spanned by the other two (and, in particular, by all the other vectors in $X$) - and hence the whole $X$ would be linearly dependent.
		
		\item $\{x_1,x_2,x_3\}$ is linearly independent.
		
		This would imply that none of them is spanned by the others. In particular, this implies that $x_1$ isn't spanned by $\{x_2,x_3\}$ - that is, $x_1,x_2,x_3$ are non-coplanar.
		
		But we know that any three non-null non-coplanar vectors satisfy
		\[\R x_1+\R x_2+\R x_3=\R^3\]so $\spen\{x_1,x_2,x_3\}=\R^3$. This means that every other vector in $\R^3$ (in particular, any other vector in $X$) is already spanned by $\{x_1,x_2,x_3\}$ - so $X$ is linearly dependent.
	\end{itemize}

Either way, we can always see that $X$ is linearly dependent.

This ends the proof.
\end{proof}
\begin{cor}
	Equivalently, if $X$ is linearly dependent, then $\#X\leq 3$.
\end{cor}
\begin{proof}
	This is the same result, but phrased in another manner and thus doesn't require further proof.
\end{proof}

Similarly, we can talk about the size of spanning sets:

\begin{lemma}
	Let $X\subseteq\R^3$ be a finite set. If $\#X<3$ then $\spen X\subset\R^3$.
\end{lemma}
\begin{proof}
	This has already been proven, in that you need at least three non-null non-coplanar vectors to span $\R^3$.
\end{proof}
\begin{cor}
	Equivalently, if $\spen X=\R^3$ then $\#X\geq 3$.
\end{cor}
\begin{proof}
	Once again, this is the same result, just stated in another manner, and so doesn't require further proof.
\end{proof}

Now we can combine these two lemmas into a very important lemma:

\begin{lemma}
	Let $X\subseteq\R^3$ be a finite set of linearly independent vectors such that $\spen X=\R^3$. Then $\#X=3$.
\end{lemma}
\begin{proof}
	Since $X$ is linearly independent, $\#X\leq 3$. But since $X$ spans $\R^3$, $\#X\geq 3$. Combining these, we see that $\#X=3$, as we had claimed.
\end{proof}

With this we can now state a similar result to what we have already proved in $\R^2$.

\begin{theorem}
	Let $X\subseteq\R^3$ be a subset of $\R^3$. Then the following are equivalent:
	\begin{enumerate}[a)]
		\item $X$ is a base for $\R^3$;
		\item $X$ is linearly independent and has three elements;
		\item $X$ spans $\R^3$ and has three elements;
		\item $X$ is a linearly independent set which spans $\R^3$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	We already know that (d) implies both (b) and (c) by the preceding lemma. Let, once and for all, $X=\{x_1,x_2,\cdots,x_n\}$. Then:
	\begin{itemize}
		\item \underline{(a) $\Rightarrow$ (c)}:
		
		Since $X$ is a base, given any linear function $f$, the image of any point $v$ under $f$ is uniquely determined by the image of $X$ under $f$.
		
		In particular, for the identity function, we see that for any $v\in\R^3$ there are uniquely determined $v_1,v_2,\cdots,v_n\in\R$ such that
		\[\id_{\R^3}(v)=\sum_{i=1}^nv_i\id_{\R^3}(x_i)\]which is just
		\[v=\sum_{i=1}^nv_ix_i\]and so $v\in \spen X$.
		
		This tells us that any $v\in\R^3$ is spanned by $X$, so $X$ spans $\R^3$.
		
		We claim that $x_1,x_2,x_3$ are non-coplanar (and hence $\spen X=\spen \{x_1,x_2,x_3\}$).
		
		Indeed, since $X$ is a base, there's a unique way to write any vector in $\R^3$ using vectors in $X$ - in particular, there's a unique way to write $x_1,x_2$ and $x_3$ in terms of $X$:
		\[x_1=1\cdot x_1+0\cdot x_2+0\cdot x_3+\cdots+0\cdot x_n\]
		\[x_2=0\cdot x_1+1\cdot x_2+0\cdot x_3+\cdots+0\cdot x_n\]
		\[x_3=0\cdot x_1+0\cdot x_2+1\cdot x_3+\cdots+0\cdot x_n\]this tells us that they're all non-parallel, and that no two of them span the other one - so they are non-coplanar. So $\spen X=\spen \{x_1,x_2,x_3\}$.
		
		But notice that we can do the same for any $x_i\in X$:
		\[x_i=0\cdot x_1+\cdots+1\cdot x_i+\cdots+0\cdot x_n.\]
		
		So if $n>3$, we'd have two ways of writing any $x_i$ in terms of $X$: Either as $x_i=1\cdot x_i$ or as being spanned by $\{x_1,x_2,x_3\}$. But $X$ is a base, so there's only one way to write any vector in terms of $X$.
		
		That tells us that $X$ must have precisely $3$ elements - otherwise there would be some elements which would be able to be written in multiple ways in terms of $X$, which contradicts the fact that $X$ is a base.
		
		This shows that a base is a spanning set with three elements.
		
		\item \underline{(b) $\iff$ (c)}:
		
		In this case, we're always assuming that $X$ has three vectors. We need to show that $X$ is, then, linearly independent if, and only if, it is a spanning set for $\R^3$.
		
		Take $v\in \R^3$, any vector. Then $X$ is a spanning set if, and only if, $\spen X=\R x_1+\R x_2+\R x_3$ is $\R^3$. But we have proven that this happens if, and only if, $X$ is a set of non-null, non-coplanar vectors - which is the same as saying that $X$ is linearly independent.
		
		So $X$ is a spanning set with three elements if, and only if, it is a linearly independent set with three elements.
		
		\item \underline{(b) $\implies$ (d)}:
		
		If $X$ is a spanning set with three elements, then we've already proven that it is also linearly independent. So $X$ is a linearly independent set which spans $\R^3$.
		
		\item \underline{(d) $\implies$ (a)}:
		
		Let $X$ be a linearly independent spanning set for $\R^3$. We've already proven that this set has precisely three elements - let them be $X=\{x_1,x_2,x_3\}$.
		
		Since $X$ spans $\R^3$, given any $v\in \R^3$ there are (not necessarily unique) real numbers $v_1,v_2,v_3\in\R$ such that
		\begin{equation}
			v=\sum_{i=1}^3v_ix_i.
		\end{equation}
		
		Suppose that there were other numbers $v'_1,v'_2,v'_3\in\R$ such that
		\begin{equation}
			v=\sum_{i=1}^3v'_ix_i.
		\end{equation}We want to show that $v_i=v_i'$ for all $i\leq 3$ and so that there's a unique way to write each vector in $\R^3$ as being spanned by $X$.
		
		But since (1) and (2) are just two expressions for $v$, we can write
		\[\sum_{i=1}^3v_ix_i=v=\sum_{i=1}^3v'_ix_i.\]
		
		But then we can rearrange this into
		\[\sum_{i=1}^3(v_i-v'_i)x_i=0\]and since $X$ is linearly independent, this implies that $v_i-v_i'=0$ for all $i\leq 3$. But this is just the same as saying $v_i=v_i'$ for all $i\leq 3$.
		
		Therefore, there's a unique way to write each vector $v\in\R^3$ as being spanned by $X$.
		
		This shows that $X$ is a base.
	\end{itemize}

This ends the proof: Indeed, we've proven
\[(a)\implies (c)\iff (b)\iff (d)\implies(a)\]so they're all equivalent.
\end{proof}

\newpage
\subsection{Back to linearity in $\R^3$}

This section we'll explore a bit more about linear functions in $\R^3$, but since most results and proofs are literally the same as in $\R^2$, we're just gonna skip those and only show the new stuff.

\begin{prop}
	For any linear function $f:\R^3\to\R^3$, both $\im f$ and $\Ker f$ are subspaces.
\end{prop}

\begin{cor}
	For any linear function $f$, the image (resp. the kernel) is either zero, a line through zero, a plane through zero or $\R^3$.
\end{cor}

\begin{prop}
	Let $f:\R^3\to\R^3$ be a linear function. Then $f$ is injective if, and only if, $\Ker f =0$.
\end{prop}

\begin{df}
	Given a function $f:\R^3\to\R^3$ we'll say that \textbf{$f$ is a linear isomorphism} if it is both linear and a bijection.
\end{df}

\begin{lemma}
	If $f:\R^3\to\R^3$ is a linear isomorphism, then so is $f^{-1}$.
\end{lemma}

\begin{prop}
	Let $f,g:\R^3\to\R^3$ be linear functions. Then both $f\circ g$ and $g\circ f$ are also linear functions.
\end{prop}
\begin{cor}
	If both $f$ and $g$ are linear isomorphisms, then both $f\circ g$ and $g\circ f$ are also linear isomorphisms.
\end{cor}

\begin{theorem}
	Let $f:\R^3\to\R^3$ be a linear function. Then the following are equivalent:
	\begin{enumerate}[a)]
		\item $f$ is a linear isomorphism;
		\item $f$ is injective;
		\item $f$ is surjective.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Clearly, by definition of linear isomorphism, (a) implies both (b) and (c).
	
	\begin{itemize}
		\item \underline{(b) $\implies$ (a) and (c)}:
		
		Let $f$ be injective and take $X\subseteq\R^3$ any base. We'll show that $f(X)$ is non coplanar - and thus that $f(X)$ spans $\R^3$. This suffices, because $f(X)\subseteq \im f$, so we will have shown that $\R^3\subseteq\im f\subseteq \R^3$, and so $\im f=\R^3$.
		
		First, notice that since $f$ is injective, $f(x_i)\neq f(x_j)$ for all $i\neq j$ in $\{1,2,3\}$.
		
		Suppose $f(x_1)\parallel f(x_2)$ - that is, $f(x_1)=\lambda f(x_2)$ for some $\lambda\in \R$.
		
		But $f$ is linear, so $\lambda f(x_2)=f(\lambda x_2)$.
		
		Since $f$ is injective, $f(x_1)=f(\lambda x_2)$ implies $x_1=\lambda x_2$ - that is, $x_1\parallel x_2$.
		
		But we know that $X$ is a base, and thus linearly independent - so $x_1\nparallel x_2$. Therefore, $f(x_1)\nparallel f(x_2)$ - since that would lead to a contradiction.
		
		
		\bigskip
		Analogously, if we assume $\plane{f(x_1)}{f(x_2)}{f(x_3)}$, then we'd have $f(x_1)=\lambda_2f(x_2)+\lambda_3f(x_3)$ for some $\lambda_2,\lambda_3\in \R$ and, once again, since $f$ is linear, this implies $f(x_1)=f(\lambda_2x_2+\lambda_3x_3)$.
		
		But $f$ is, once more, linear, so $x_1=\lambda_2x_2+\lambda_3x_3$ and hence $\plane{x_1}{x_2}{x_3}$, which contradicts the fact that $X$ is a base (and thus linearly independent).
		
		Therefore, $f(X)$ is also a set with three non-coplanar (i.e. linearly independent) vectors - and hence it is a base. It follows that $f(X)$ spans $\R^3$ and, so, $f$ is surjective.
		
		Since $f$ is already injective, it is a linear isomorphism.
		
		\item \underline{(c) $\implies$ (a) and (b)}:
		
		Assume now that $f$ is a linear surjection, and take $X\subseteq\R^3$ a base for $\R^3$. Since $f$ is a surjection, put $y_i:=f^{-1}(x_i)$ for all $i\leq 3$, and let $Y=\{y_1,y_2,y_3\}$. We claim that $Y$ is a base.
		
		Indeed, following a reasoning similar to what we did before, the fact that $f$ is linear implies that if we could write $y_1=\lambda_2y_2+\lambda_3y_3$, for some $\lambda_2,\lambda_3\in \R$, then we'd get that $x_1=\lambda_2x_2+\lambda_3x_3$ - which contradicts the fact that $X$ is a base. This tells us that $Y$ has three elements and is linearly independent, so it is a base.
		
		Take now any $z\in Ker f$ - that is, $f(z)=0$.
		
		Since $Y$ is a base, there are some $z_1,z_2,z_3\in\R$ uniquely determined such that 
		\[z=\sum_{i=1}^3z_iy_i.\]If we now apply $f$ on both sides we get
		\[0=f(z)=\sum_{i=1}^3z_if(y_i)=\sum_{i=1}^3z_ix_i\]and since $X$ is a base, this implies that $z_i=0$ for all $i\leq 3$.
		
		Finally, this tells us that $z=z_1y_1+z_2y_2+z_3y_3=0+0+0=0$.
		
		So we showed that any element $z$ in the kernel of $f$ has to be zero. This shows that $\Ker f=0$ and, so, $f$ is injective.
		
		Finally, since $f$ is already surjective, it is an isomorphism.
	\end{itemize}

This ends the proof.
\end{proof}

Nothing new to see here, and the same will hold for the following section.

\newpage
\subsection{Matrices in $\R^3$}

Once again, this is just a restatement of the same results as matrices in $\R^2$, but now in three dimensions.

\begin{prop}
	The sets $M_{3\times 1}(\R)$ and $M_{1\times 3}(\R)$ are in bijection with $\R^3$.
\end{prop}
\begin{proof}
	This can easily be seen to follow by putting
	\begin{align*}
		\phi:M_{3\times 1}(\R)&\to\R^3\\\begin{pmatrix}
		a\\b\\c
		\end{pmatrix}&\mapsto (a,b,c)
	\end{align*}
	\begin{align*}
	\varphi:M_{1\times 3}(\R)&\to\R^3\\\begin{pmatrix}
	a&b&c
	\end{pmatrix}&\mapsto (a,b,c)
	\end{align*}
\end{proof}
\begin{cor}
	The sets $M_{3\times 1}(\R)$ and $M_{1\times 3}(\R)$ are in bijection.
\end{cor}

\begin{df}
	We'll denote the bijection between $M_{3\times 1}(\R)$ and $M_{1\times 3}(\R)$ and its inverse by $(-)^t$ such that
	\[\begin{pmatrix}
	a\\b\\c
	\end{pmatrix}^t:=\begin{pmatrix}
	a&b&c
	\end{pmatrix}\]and
	\[\begin{pmatrix}
	x&y&z
	\end{pmatrix}^t:=\begin{pmatrix}
	x\\y\\z
	\end{pmatrix}\]for any $\begin{pmatrix}
	a\\b\\c
	\end{pmatrix}\in M_{3\times 1}(\R)$ and any $\begin{pmatrix}
	x&y&z\end{pmatrix}\in M_{1\times 3}(\R)$.
	
	This is called a \textbf{transposition}.
\end{df}

\begin{df}
	We define additions $$+:M_{3\times 1}(\R)\times M_{3\times 1}(\R)\to M_{3\times 1}(\R),\quad\quad+:M_{1\times 3}(\R)\times M_{1\times 3}(\R)\to M_{1\times 3}(\R)$$ and scalar multiplications
	\[\cdot: \R\times M_{3\times 1}(\R)\to M_{3\times 1}(\R),\quad\quad\cdot: \R\times M_{1\times 3}(\R)\to M_{1\times 3}(\R)\]by putting
	\[A+B:=\begin{pmatrix}
	a_1+b_1\\a_2+b_2\\a_3+b_3
	\end{pmatrix},\quad\quad C+D:=\begin{pmatrix}
	c_1+d_1&c_2+d_2&c_3+d_3
	\end{pmatrix}\]
	\[\lambda A:=\begin{pmatrix}
	\lambda a_1\\\lambda a_2\\\lambda a_3
	\end{pmatrix},\quad\quad\lambda C:=\begin{pmatrix}
	\lambda c_1&\lambda c_2&\lambda c_3
	\end{pmatrix}\]for all $A=\begin{pmatrix}
	a_1\\a_2\\a_3
	\end{pmatrix}$ and $B=\begin{pmatrix}
	b_1\\b_2\\b_3
	\end{pmatrix}$ in $M_{3\times 1}(\R)$, all $C=\begin{pmatrix}
	c_1&c_2&c_3
	\end{pmatrix}$ and $D=\begin{pmatrix}
	d_1&d_2&d_3
	\end{pmatrix}$ in $M_{1\times 3}(\R)$ and all $\lambda\in \R$.
\end{df}

\begin{prop}
	The additions defined above are associative, commutative, have identity and inverse elements, and the scalar multiplications defined above are associative, commutative, have identity and inverse elements, and are distributive over both scalar and matrix addition.
\end{prop}


\begin{df}
	Given any two matrices $X\in M_{3\times 1}(\R)$ and $Y\in M_{1\times 3}(\R)$, we define the product $YX$ to be the $1\times 1$ matrix given by
	\[YX:=\begin{pmatrix}
	\sum_{i=1}^3x_iy_i
	\end{pmatrix}\]where $X=\begin{pmatrix}
	x_1\\x_2\\x_3
	\end{pmatrix}$ and $Y=\begin{pmatrix}
	y_1&y_2&y_3
	\end{pmatrix}$.
\end{df}

\begin{prop}
	The following equations hold:
	\begin{enumerate}[a)]
		\item $(Y+Y')X=YX+Y'X$
		\item $Y(X+X')=YX+YX'$
		\item $\lambda(YX)=(\lambda Y)X=Y(\lambda X)$
		\item $(YX)^t=X^tY^t$
	\end{enumerate}for all $Y,Y'\in M_{1\times 3}(\R)$, $X,X'\in M_{3\times 1}(\R)$ and $\lambda\in \R$.
\end{prop}

Now let's go back to talking about square matrices.

\begin{rmk}
	Notice that to describe any matrix it suffices to describe all its coefficients/entries. With that in mind, we can, for instance, write the matrix $I=\begin{pmatrix}
	1&0&0\\
	0&1&0\\
	0&0&1
	\end{pmatrix}$ more simply as $I=(I_{i,j})_{\substack{i\leq 3\\j\leq 3}}$ with
	\[I_{i,j}:=\begin{cases}
	1,\mbox{ if }i=j\\
	0,\mbox{ otherwise}
	\end{cases}\]where $I_{i,j}$ denotes the entry of $I$ which lies in the row $i$, column $j$.
\end{rmk}

\begin{lemma}
	Given $X,X'\in M_{3\times 1}(\R)$, $Y,Y'\in M_{1\times 3}(\R)$ and $\lambda \in \R$, then the following equations hold:
	\begin{multicols}{2}
		\begin{enumerate}[a)]
			\item $(X+X')_{i,j}=X_{i,j}+X'_{i,j}$
			\item $(Y+Y')_{i,j}=Y_{i,j}+Y'_{i,j}$
			\item $(\lambda X)_{i,j}=\lambda X_{i,j}$
			\item $(\lambda Y)_{i,j}=\lambda Y_{i,j}$
			\item $(X^t)_{i,j}=X_{j,i}$
			\item $(Y^t)_{i,j}=Y_{j,i}$
			\item $(YX)_{i,j}=\sum_{l=1}^3 Y_{i,l}X_{l,j}$
		\end{enumerate}
	\end{multicols}
	for all $i,j$.
\end{lemma}

This makes it easier to state and prove many of the results that will follow for matrices.

\begin{prop}
	There is a bijection between $M_{3\times 3}(\R)$ and $\hom_\R(\R^3,\R^3)$.
\end{prop}

\begin{df}
	The above bijection induces for any two matrices $X,Y\in M_{3\times 3}(\R)$ and any real number $\lambda\in \R$ the following operations:
	\[(X+Y)_{i,j}:=X_{i,j}+Y_{i,j}\]
	\[(\lambda X)_{i,j}:=\lambda X_{i,j}\]
	\[(XY)_{i,j}:=\sum_{l=1}^3X_{i,l}Y_{l,j}\]where $i,j\leq 3$.
\end{df}
\begin{lemma}
	There's a unique matrix $I\in M_{3\times 3}(\R)$ such that $IX=XI=I$ for all $X\in M_{3\times 3}(\R)$.
\end{lemma}
\begin{proof}
	If we define \[I_{i,j}:=\begin{cases}
	1, \mbox{ if }i=j\\
	0, \mbox{otherwise}
	\end{cases}\]then $I:=(I_{i,j})_{i,j\leq 3}$ can be easily shown to satisfy $IX=XI=X$ for all $X\in M_{3\times 3}(\R)$.
\end{proof}

\begin{df}
	We'll call the matrix $I$ above the \textbf{identity matrix of $\R^3$}.
\end{df}

Once again, we'll define how to multiply $3\times 3$ matrices by $3\times 1$ matrices by applying all the isomorphisms we have so far:

\begin{df}
	We define the \textbf{evaluation map} $ev:\hom_\R(\R^3,\R^3)\times\R^3\to \R^3$ to be the map given by $ev(f,v):=f(v)$.
\end{df}

\begin{df}
	Given any $A\in M_{3\times 3}(\R)$ and any $X\in M_{3\times 1}(\R)$, we define the multiplication $AX$ to be the extension of the evaluation map to matrices.
	
	In other words, if we let $f_A\in \hom_\R(\R^3,\R^3)$ be the image of $A$ under the isomorphism $M_{3\times 3}(\R)\iso\hom_\R(\R^3,\R^3)$ and $v_X$ be the image of $X$ under the isomorphism $M_{3\times 1}(\R)\iso \R^3$ then $ev(f_A,v_X)$ is a vector in $\R^3$, so we can look at its inverse image under the above-mentioned isomorphism and call it $AX$.
\end{df}

\begin{lemma}
	If $A=(a_{i,j})_{i,j\leq 3}$ and $X=(x_{i,1})_{i\leq 3}$ then $AX$ is given by
	\[(AX)_{i,1}=\sum_{l=1}^3a_{i,l}x_{l,1}.\]
\end{lemma}

\begin{df}
	Similarly, given any $A\in M_{3\times 3}(\R)$ and any $Y\in M_{1\times 3}(\R)$, we define the multiplication $YA$ to be $(A^tY^t)^t$.
\end{df}

\begin{lemma}
	If $A=(a_{i,j})_{i,j\leq 3}$ and $Y=(y_{1,j})_{j\leq 3}$ then $YA$ is given by
\[(YA)_{1,j}=\sum_{l=1}^3y_{1,l}a_{l,j}.\]
\end{lemma}

\begin{prop}
	Let $A,A' \in M_{3\times 3}(\R)$, $X,X'\in M_{3\times 1}(\R)$, $Y,Y'\in M_{1\times 3}(\R)$ and $\lambda \in \R$. Then the following hold:
	\begin{multicols}{2}
		\begin{enumerate}[a)]
			\item $Y(AX)=(YA)X$
			\item $(AA')X=A(A'X)$
			\item $Y(AA')=(YA)A'$
			\item $\lambda(AX)=(\lambda A)X=A(\lambda X)$
			\item $\lambda(YA)=(\lambda Y)A=Y(\lambda A)$
			\item $A(X+X')=AX+AX'$
			\item $(Y+Y')A=YA+Y'A$
			\item $(A+A')X=AX+A'X$
			\item $Y(A+A')=YA+YA'$
			\item $(AX)^t=X^tA^t$
			\item $(YA)^t=A^tY^t$
		\end{enumerate}
	\end{multicols}
\end{prop}

And with this we end our brief discussion about matrices in $\R^3$. Once again, it's nothing new - just the same as it was in $\R^2$.

\newpage
\subsection{Distances in $\R^3$}

Just like in $\R^2$, the inner product in $\R^3$ is very useful and needed for measuring distances and angles. So let's define it first and foremost:

\begin{df}
	Let $v,u\in \R^3$. We define the \textbf{inner product} (sometimes called the dot product) of $v$ and $u$ to be the real number $\gen{v,u}$ given by
	\[\gen{v,u}:=\sum_{i=1}^3v_iu_i,\]where $v=(v_1,v_2,v_3)$ and $u=(u_1,u_2,u_3)$.
\end{df}

Now let us show that the inner product in $\R^3$ is as well-behaved as the one in $\R^2$:

\begin{prop}
	Let $v,u,w\in \R^3$ and $\lambda\in \R$. Then:
	\begin{enumerate}[a)]
		\item $\gen{v,u+w}=\gen{v,u}+\gen{v,w}$;
		\item $\gen{v+u,w}=\gen{v,w}+\gen{u,w}$;
		\item $\lambda \gen{v,u}=\gen{\lambda v,u}=\gen{v,\lambda u}$;
		\item $\gen{v,u}=\gen{u,v}$;
		\item $\norm{v}^2=\gen{v,v}$.
	\end{enumerate}
\end{prop}

And once more we can use the Cosine Law to infer the following result:

\begin{lemma}
	Let $v,u\in \R^3$ and let $\theta\in[0,2\pi)$ be the angle between $v$ and $u$. Then 
	\[\cos\theta=\frac{\gen{v,u}}{\norm{v}\norm{u}}.\]
\end{lemma}

From which it follows trivially that
\begin{cor}
	Let $v,u\in \R^3$. Then $v\perp u$ if, and only if, $\gen{v,u}=0$.
\end{cor}

With this, we can proceed as before to show that the norm given by the inner product is a good norm:

\begin{prop}
	Let $v,u\in \R^3$ and $\lambda \in \R$. Then the following hold:
	\begin{enumerate}[a)]
		\item $\norm{\lambda v}=\abs{\lambda}\norm v$;
		\item $\norm v\geq 0$, with equality happening if, and only if, $v=0$;
		\item $\norm{v+u}\leq\norm{v}+\norm u$.
	\end{enumerate}
\end{prop}
\begin{cor}
	For any non-null vector $v\in \R^3$, we have that \[\Norm{\frac{v}{\norm v}}=1.\]
\end{cor}

And now we can define distance in $\R^3$:

\begin{df}
	Given any two vectors $v,u\in \R^3$, we define the \textbf{distance between $v$ and $u$} to be the real number $d(v,u)$ given by
	\[d(v,u):=\norm{v-u}.\]
\end{df}

And it follows trivially that this distance satisfies the same properties as it did in $\R^2$:

\begin{prop}
	Let $v,u,w\in \R^3$. Then:
	\begin{enumerate}[a)]
		\item $d(v,u)=d(u,v)$;
		\item $d(v,u)\geq 0$, with equality happening if, and only if, $v=u$;
		\item $d(v,u)\leq d(v,w)+d(w,u)$.
	\end{enumerate}
\end{prop}
\begin{cor}
	Item (c) above is an equality if, and only if, $w$ not only is in the same line as $v$ and $u$, but also is between them.
\end{cor}

Finally we're ready to do some geometry in $\R^3$!

\newpage
\subsection{Geometry in $\R^3$}

Let's start off with some results that were true in $\R^2$ and continue being true in $\R^3$:

\begin{prop}
	Any two lines in $\R^3$ either meet in a single point or not at all.
\end{prop}


\begin{df}
	Given any vector $v\in \R^3$ and any non-negative real number $r$, we define the \textbf{sphere centered in $v$ with radius $r$} to be the set $B(v,r)$ given by
	\[B(v,r):=\{u\in \R^3\mid d(v,u)=r\}.\]
\end{df}

\begin{df}
	Given any vector $v$ and any line $r$ both in $\R^3$, we define the \textbf{distance between $v$ and $r$} to be the real number $d(v,r)$ which is given by
	\[d(v,r):=\min_{u\in r}\{d(v,u)\}.\]
\end{df}

\begin{prop}
	Given any vector $v\in \R^3$ and any line $r\subseteq \R^3$, there's a unique vector $u\in r$ such that $d(v,u)=d(v,r)$. In other words, given any point and any line, we can always compute the distance between them.
\end{prop}
\begin{cor}
	Given any vector $v\in \R^3$ and any line $r\subseteq \R^3$, then a vector $u\in r$ satisfies $d(v,u)=d(v,r)$ if, and only if, the line connecting $v$ and $u$ is orthogonal to $r$ - that is, $(v-u)\perp r$.
\end{cor}

Aside from those, all the results concerning shapes (rectangles, rhombi and circles) still hold.

However, unlike in $\R^2$, we cannot define a ``canonical'' orthogonal vector $v^\perp$ to any given vector $v$. Let's explain why:

\begin{prop}
	Let $v\in \R^3$ be any non-null vector. Then the set $v^\perp:=\{u\in \R^3\mid v\perp u\}$ is a plane.
\end{prop}
\begin{proof}
	From what we've already done, it suffices to show that $v^\perp$ is spanned by two vectors.
	
	Let $v=(v_1,v_2,v_3)$. Then we can rewrite $v^\perp$ as 
	\[v^\perp=\{(x,y,z)\in \R^3\mid v_1x+v_2y+v_3z=0\},\]by definition of orthogonality.
	
	Since $v$ is non-null, at least one of its coordinates is non-zero. Assume, without loss of generality, that $v_1\neq 0$. In that case, we see that $u=(u_1,u_2,u_3)$ is in $v^\perp$ if, and only if, $\sum_{i=1}^3 v_iu_i=0$ - which, in other words, means that
	\[u_1=-\frac{v_2}{v_1}u_2-\frac{v_3}{v_1}u_3\]so $u$ can be written as
	\begin{align*}
		u&=(u_1,u_2,u_3)\\
		&=\left(-\frac{v_2}{v_1}u_2-\frac{v_3}{v_1}u_3,u_2,u_3\right)\\
		&=\left(-\frac{v_2}{v_1}u_2,u_2,0\right)+\left(-\frac{v_3}{v_1}u_3,0,u_3\right)=u_2\left(-\frac{v_2}{v_1},1,0\right)+u_3\left(-\frac{v_3}{v_1},0,1\right)
	\end{align*}and since $\left\{\left(-\frac{v_2}{v_1},1,0\right),\left(-\frac{v_3}{v_1},0,1\right)\right\}$ are linearly independent, this means that $$v^\perp = \spen\left\{\left(-\frac{v_2}{v_1},1,0\right),\left(-\frac{v_3}{v_1},0,1\right)\right\}=\R\left(-\frac{v_2}{v_1},1,0\right)+\R\left(-\frac{v_3}{v_1},0,1\right)$$or, in other words, $v^\perp$ is a plane.
\end{proof}

Therefore, we can't just do as we did in $\R^2$ and pick one of two orthogonal vectors with the same size as given vector (since it was just a line). Now we have infinitely many orthogonal vectors with the same size as a given vector, so there's not obvious choice of which one of these is going to be our "chosen vector".

That's why, going forward, we're going to use the following definition for $v^\perp$:

\begin{df}
	Given any non-null vector $v\in \R^3$, we define its \textbf{orthogonal complement} to be the plane $v^\perp$ given by
	\[v^\perp:=\{u\in \R^3\mid v\perp u\}.\]
\end{df}

What's interesting about this definition is that not only is the orthogonal complement of any vector a plane (as we've shown above), but also the converse is true:

\begin{lemma}
	Let $\pi\subseteq\R^3$ be any plane containing zero. Then there's some vector $v\in \R^3$ such that $\pi=v^\perp$.
\end{lemma}

This will actually require some other lemmas first in order to prove it. We can, however, prove the following slightly weaker statement:

\begin{prop}
	Let Let $\pi\subseteq\R^3$ be any plane containing zero and $v\in \R^3$ such that $v\perp \pi$ - in other words, $v\perp u$ for all $u\in \pi$. Then $\pi=v^\perp$.
\end{prop}
\begin{proof}
	Since, by hypothesis, $v\perp u$ for all $u\in \pi$, we have that every $u\in \pi$ is also in $v^\perp$, and so $\pi\subseteq v^\perp$. Since we already know that $v^\perp$ is a plane, we get that $\pi=v^\perp$.
\end{proof}

In other words, all that we need to prove in order to prove the preceding lemma is that given any plane $\pi$ we can always find a vector $v$ such that $v\perp \pi$. That's precisely what we're going to do now.

To do that, however, we'll need to work slightly backwards from what we did back in $\R^2$. If you'll recall, we started by defining what $v^\perp$ meant, and then we defined $\det(v,u)$ to be $\gen{v,u^\perp}$.

Here we're going to do the opposite: We'll first describe what the determinant is, and then we'll use that to define the orthogonal vector to a given plane.

\bigskip
To do that, first we'll need to give more structure to $\R^3$:

\begin{df}
	Let $\leq$ in $\R^3$ be the relation given by: $(v_1,v_2,v_3)\leq (u_1,u_2,u_3)$ if, and only if, any of the following hold:
	\begin{enumerate}[(1)]		
		\item $v_1< u_1$;
		\item $v_1=u_1$, but $v_2< u_2$;
		\item $v_1=u_1$ and $v_2=u_2$, but $v_3< u_3$;
		\item $v_1=u_1$, $v_2=u_2$ and $v_3=u_3$.
	\end{enumerate}

	This is an order, which is called the \textbf{lexicographical order}, or dictionary order.
\end{df}

\begin{prop}
	$\R^3$ with the lexicographical order is a totally-ordered set.
\end{prop}
\begin{proof}
	First, let us show that $\leq$ the lexicographical order is indeed an order:
	
	\begin{itemize}
		\item \underline{$\leq$ is reflexive:}
		
		This is trivial, by definition of $\leq$.
		
		\item \underline{$\leq$ is antisymmetric:}
		
		Let $v,u\in \R^3$ be such that $v\leq u$ and $u\leq v$. We need to show that this implies $v=u$. We'll do this by considering all the possible cases as to why $v\leq u$:
		
		\begin{itemize}
			\item If $v_1\leq u_1$, then $u_1>v_1$ which contradicts $u\leq v$, so this cannot happen.
			\item If $v_1=u_1$, but $v_2<u_2$, then $u_2>v_2$ which contradicts $u\leq v$, so this cannot happen.
			\item If $v_1=u_1$ and $v_2=u_2$, but $v_3<u_3$, then $u_3>v_3$ which contradicts $u\leq v$, so this cannot happen.
			\item If $v_1=u_1$, $v_2=u_2$ and $v_3=u_3$, this implies $v=u$ which doesn't contradict $u\leq v$.
		\end{itemize}
	
		Since the only case for $v\leq u$ which doesn't contradict $u\leq v$ is $v=u$, we have that $\leq$ is indeed antisymmetric.
		
		\item \underline{$\leq$ is transitive:}
		
		Let $v,u,w\in \R^3$ be such that $v\leq u$ and $u\leq w$. We need to show that this implies $v\leq w$. We once again need to do that by considering all possible cases. To do so, we'll use the following table:
			\begin{center}
				\begin{tabu}{|c|[2pt]c|c|c|c|}
					\hline
					& $u(1)w$ & \(u(2)w\) & \(u(3)w\) & \(u(4)w\)\\\tabucline[2pt]{-}
					$v(1)u$ &&&& \\\hline
					$v(2)u$ &&&&\\\hline
					$v(3)u$ &&&&\\\hline
					$v(4)u$ &&&&\\\hline
				\end{tabu}
			\end{center}		
		Here, $u(2)w$, for instance, should be read as ``$u\leq w$ because they satisfy condition (2) in the definition of $\leq$'' - or, in other words, $u(2)w$ means ``$u_1=w_1$, but $u_2<w_2$'' - and so on.
		
		That said, this is how this table should be filled-in: For instance, suppose we have $v\leq u$ because $v(3)u$, and $u\leq w$ because $u(2)w$. This means that ``$v_1=u_1$ and $v_2=u_2$, but $v_3<u_3$'' and ``$u_1=w_1$, but $u_2<w_2$''.
		
		So combining these two into a single statement we get ``$v_1=u_1=w_1$, $v_2=u_2<w_2$ and $v_3<u_3$'' - which, in particular, tells us that ``$v_1=w_1$, but $v_2<w_2$'' which is the definition of $v(2)w$.
		
		So we put $v(2)w$ in the row $v(3)u$ and column $u(2)w$.
		
		Proceeding like this for all rows and columns, we get the following table:
		\begin{center}
			\begin{tabu}{|c|[2pt]c|c|c|c|}
				\hline
				& $u(1)w$ & \(u(2)w\) & \(u(3)w\) & \(u(4)w\)\\\tabucline[2pt]{-}
				$v(1)u$ & $v(1)w$ & $v(1)w$ & $v(1)w$ & $v(1)w$\\\hline
				$v(2)u$ & $v(1)w$ & $v(2)w$ & $v(2)w$ & $v(2)w$\\\hline
				$v(3)u$ & $v(1)w$ & $v(2)w$ & $v(3)w$ & $v(3)w$\\\hline
				$v(4)u$ & $v(1)w$ & $v(2)w$ & $v(3)w$ & $v(4)w$\\\hline
			\end{tabu}
		\end{center}which tells us that no matter the reason why $v\leq u$ and $u\leq w$, this is always enough to conclude $v\leq w$ - and thus $\leq$ is transitive.
	\end{itemize}

	This shows that $\leq$ is indeed an order in $\R^3$.
	
	Now all that's left is to show that it is a total order - i.e., that given any two vectors $v,u\in\R^3$ then either or both of $v\leq u$ and $u\leq v$ hold.
	
	To do that, take any $v,u\in \R^3$.
	\begin{itemize}
		\item If $v_1<u_1$ or $u_1<v_1$, then we're done (the first case tells us $v(1)u$ and hence $v\leq u$, and the second case tells us $u(1)v$ and hence $u\leq v$). Otherwise we have $v_1=u_1$ and we now check the second entries.
		\item If $v_2<u_2$ or $u_2<v_2$, since we already have $v_1=u_1$ (otherwise we would have stopped in the first step), we're done (because these imply $v\leq u$ and $u\leq v$ by $v(2)u$ and $u(2)v$ respectively). Otherwise we have $v_2=u_2$.
		\item If $v_3<u_3$ or $u_3<v_3$, since we already have $v_1=u_1$ (otherwise we would have stopped in the first step) and $v_2=u_2$ (otherwise we would have stopped in the second step), we're done (because these imply $v\leq u$ and $u\leq v$ by $v(3)u$ and $u(3)v$ respectively). Otherwise we have $v_3=u_3$.
		\item But now we have $v_1=u_1$, $v_2=u_2$ and $v_3=u_3$ (otherwise we would have stopped at either the first, second or third steps) and thus $v=u$, so both $v\leq u$ and $u\leq v$ hold.
	\end{itemize}

	No matter the case, then, we can always conclude either or both of $v\leq u$ and $u\leq v$. This implies that $\leq$ is a total order and finishes the proof.
\end{proof}

\begin{ex}
	Let us compare some vectors to see what's going on in here:
	
	Let $v=(1,2,3)$, $u=(1,0,0)$, $w=(-3,1,4)$ and $z=(1,2,-4)$.
	
	We claim that $w\leq u\leq z\leq v$.
	
	To see that, think of vectors as ``words'' in a dictionary.
	
	You start by comparing the first letter: $w_1<v_1=u_1=z_1$, so we know $w$ comes before the others.
	
	Now we check the second letter: $u_2<v_2=z_2$ so we know $u$ comes after $w$, but before $v$ and $z$.
	
	Finally we check the final letter: $z_3<v_3$, so $v$ comes last.
\end{ex}

And, just before actually defining the determinant, we have to give one last definition:

\begin{df}
	Let $(X,\leq_X)$ and $(Y,\leq_Y)$ be totally ordered sets. A function $f:X\to Y$ is said to be a \textbf{cycle} if for all $x\leq_X y\leq_X z$ in $X$ we have that either of $f(x)\leq_Y f(y)\leq_Y f(z)$, $f(y)\leq_Y f(z)\leq_Y f(x)$ or $f(z)\leq_Y f(x)\leq_Y f(y)$ hold.
\end{df}

We can start off by giving an intuitive definition:
\begin{df}
	Let $f:\R^3\to\R^3$ be a linear function. We define the \textbf{determinant of $f$} to be the real number $\det f$ given by the signed volume of the parallelepiped with edges $f(e_1),f(e_2),f(e_3)$, where the sign is positive if $f:\{e_1,e_2,e_3\}\to\{f(e_1),f_(e_2),f(e_3)\}$ is a cycle, and negative otherwise.
\end{df}