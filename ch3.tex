\chapter{Spaaaaaace!}
\section{Introduction}

Now that we're reasonably comfortable with $\R^2$ (don't worry, we'll come back to it) we're gonna take the next step: Consider the set $\R^3$ and study what are its vectors, how they behave etc.

But, as it's going to become apparent very soon, this is essentially \textbf{not different at all} from what we've already been doing for $\R^2$ - some proofs are actually literally the same, without changing anything.

As a consequence, this chapter shall be much shorter than the previous one. Here's the basic schema of this chapter:

\begin{itemize}
	\item First, we're gonna define $\R^3$ and stablish its properties. At this point we'll notice how it's basically $\R^2$ all over again, with some few minor changes;
	\item Then we're gonna prove whatever is new for $\R^3$ and state which results from $\R^2$ still hold.
\end{itemize}

With that said, let us begin!

\newpage
\subsection{A small step for mankind... Kinda}

By definition, $\R^3$ is the set of all ordered triples of real numbers - like $(1,2,3)$ or $(0,\pi, -4)$ etc. We won't go into as much detail as we did for $\R^2$, but we can prove the following result:

\begin{prop}
	The Euclidean space $E_*$ with distinguished point $*$ is in bijection with $\R^3$.
\end{prop}
\begin{cor}
	Every point in $\R^3$ can be thought of as either a point in the space, or a vector from the origin to its endpoint.
\end{cor}

The idea is pretty simple - we can think of any ordered triple $(x,y,z)\in \R^3$ as a set of ``coordinates'' in a grid system which tells us how to move away from the distinguished point $*$: Move $x$ steps away from $*$ in a certain direction, move $y$ steps away from $*$ in a different direction and then $z$ steps away from $*$ in a third direction.

And we can, as before, define:

\begin{df}
	Given two vectors $v=(v_1,v_2,v_3),u=(u_1,u_2,u_3)\in \R^3$ we define their \textbf{sum} $v+u$ to be the unique vector given by
	\[v+u:=(v_1+u_1,v_2+u_2,v_3+u_3).\]
\end{df}

The intuition here is also very similar as it was for $\R^2$: $0,v,u$ define a unique triangle in the space and, by axiom, there's a unique plane containing this triangle. Therefore, $v+u$ is the parallelogram which is contained in that plane and has $0v$ and $0u$ as its sides.

\begin{prop}
	The addition of vectors in $\R^3$ is associative, commutative, has identity element and inverses.
\end{prop}

\begin{df}
	Let us define a few important subsets of $\R^3$:
	\begin{itemize}
		\item The set $\{0\}\times \R\times \R$ will be called the $\mathds{YZ}$-plane;
		\item The set $\R\times\{0\}\times \R$ will be called the $\mathds{XZ}$-plane;
		\item The set $\R\times \R\times \{0\}$ will be called the $\mathds{XY}$-plane;
		\item The set $\R\times\{0\}\times \{0\}$ will be called the $\mathds{X}$-axis;
		\item The set $\{0\}\times \R\times\{0\}$ will be called the $\mathds{Y}$-axis;
		\item The set $\{0\}\times\{0\}\times \R$ will be called the $\mathds{Z}$-axis.
	\end{itemize}
\end{df}

\begin{df}
	Let $v=(v_1,v_2,v_3)\in \R^3$ and $\lambda\in \R$. We define the \textbf{scalar multiplication} of $v$ and $\lambda$ to be the vector $\lambda v\in\R^3$ given by
	\[\lambda v:=(\lambda v_1,\lambda v_2,\lambda v_3).\]
\end{df}

\begin{prop}
	Scalar multiplication of vectors in $\R^3$ is associative, commutative, has identity element and is distributive over both real and vector addition. Not only that, but $\lambda v=0$ if, and only if, $\lambda =0$.
\end{prop}

\begin{df}
	Given any vectors $v,u\in\R^3$ we define:
	\begin{itemize}
		\item $\R v:=\{w\in \R^3\mid w=\lambda v\mbox{ for some }\lambda\in \R\}$ the \textbf{line through zero containing $v$};
		\item $\R v+\R u:=\{w\in \R^3\mid w=\lambda v+\mu u\mbox{ for some }\lambda,\mu\in \R\}$ the \textbf{plane through zero containing $v$ and $u$}. 
	\end{itemize}
\end{df}

\begin{df}
	A subset $X\subseteq\R^3$ is called a \textbf{subspace} if it is closed under addition and scalar multiplication.
\end{df}

\begin{prop}
	Given any two non-zero vectors $v,u\in\R^3$, then both $\R v$ and $\R v+\R u$ are subspaces of $\R^3$.
\end{prop}
\begin{proof}
	Let $v',v''\in \R v$ - that is, $v'=\lambda' v$ and $v''=\lambda'' v$ for some $\lambda',\lambda''\in \R$. Then, for all $\lambda\in \R$:
	\[v'+v''=(\lambda' v)+(\lambda ''v)=(\lambda'+\lambda'')v\in \R v\]
	\[\lambda v'=\lambda(\lambda' v)=(\lambda \lambda')v\in \R v\]so $\R v$ is closed under addition and scalar multiplication.
	
	
	Let $w,w'\in \R v+\R u$ - that is, $w=\lambda v+\mu u$ and $w'=\lambda' v+\mu' u$. Then, for all $\omega \in \R$:
	\[w+w'=(\lambda v+\mu u)+(\lambda' v+\mu' u)=(\lambda+\lambda ')v+(\mu+\mu')u\in \R v+\R u\]
	\[\omega w=\omega(\lambda v+\mu u)=(\omega \lambda)v+(\omega \mu)u\in \R v+\R u\]so $\R v+\R u$ is closed under addition and scalar multiplication.
	
	This ends the proof.
\end{proof}

Now let's give some geometric definitions and interpret them with linear algebra:

\begin{df}
	Two lines are said to be
	\begin{itemize}
		\item \textbf{Parallel} if they lie in the same plane and don't meet;
		\item \textbf{Skew} if they don't lie in the same plane and don't meet;
		\item \textbf{Transversal} if they meet.
	\end{itemize}

Similarly, two planes are said to be \textbf{parallel} if they don't meet, and transversal if they meet.

Finally, a line and a plane are said to be \textbf{parallel} if they don't meet.
\end{df}

\begin{prop}
Any line in $\R^3$ is of the form $\R v+u$ for some vectors $v,u\in \R^3$, and any plane in $\R^3$ is of the form $\R v+\R u+w$ for some vectors $v,u,w\in \R^3$.
\end{prop}
\begin{proof}
	Let $r\subseteq \R^3$ be any line. Then $r\cap \mathds{YZ}$ is either empty, a single point or $r\subseteq \mathds{YZ}$.
	
	\begin{itemize}
		\item If $r\cap\mathds{YZ}$ is a single point $u$, then the line $r'\subseteq\R^3$ which is parallel to $r$ through zero is such that $r=r'+u$.
		
		\item If $r\cap \mathds{YZ}$ is empty or $r\subseteq\mathds{YZ}$, we then check $r\cap \mathds{XY}$ which can be, again, either empty, a single point or $r\subseteq \mathds{XY}$.
		
		\begin{itemize}
			\item If $r\cap \mathds{XY}$ is a single point, just do as we did above, taking a parallel through zero and adding this single point to it.
			
			\item If $r\cap\mathds{XY}$ is empty, then the fact that $r\cap\mathds{YZ}$ is also empty implies that $r\cap\mathds{XZ}$ is a single point and we can just iterate the construction above.
			
			\item If $r\subseteq\mathds{XY}$, then $r=\mathds{XY}\cap\mathds{YZ}=\mathds{Y}$ so it's already a line through zero.
		\end{itemize}
	\end{itemize}

Either way, we can always show that $r$ is parallel to a line through zero.

\bigskip
Let $\pi\subseteq\R^3$ be any plane. Then:
\begin{itemize}
	\item If $\pi\cap\mathds{YZ}$ is a line $s$, then we can take $\pi'$ the plane parallel to $\pi$ through zero, and $v=s\cap \mathds{Y}$ Then clearly $\pi=\pi'+v$.
	
	\item If $\pi\cap\mathds{YZ}=\varnothing$, then surely $\pi\cap \mathds{X}\neq\varnothing$ (since $\mathds{X}\perp\mathds{YZ}$), and since $\pi\parallel\mathds{YZ}$, we can take $v=\mathds{X}\cap r$ and see that $\pi=\mathds{YX}+v$. 
\end{itemize}

Once again, we see that no matter what, $\pi$ is always parallel to a plane through zero.

This ends the proof.
\end{proof}

\newpage
\subsection{Linear functions IN SPACE!}

\begin{df}
	Let $f:\R^3\to \R^3$ be a function. We'll say that $f$ is a \textbf{linear function} if $f(v+u)=f(v)+f(u)$ and $f(\lambda v)=\lambda f(v)$ for all $v,u\in \R^3$ and $\lambda\in \R$.
	
	We'll denote the set of all linear functions in $\R^3$ by $\hom_\R(\R^3,\R^3)$.
\end{df}

\begin{prop}
	Let $f:\R^3\to\R^3$. Then $f$ is linear if, and only if, $$f(x,y,z)=(ax+by+cz,dx+ey+fz,gx+hy+iz)$$ for some $a,b,c,d,e,f,g,h,i\in\R$.
\end{prop}

\begin{lemma}
	Let $f$ be a linear function in $\R^3$. Then $f$ is uniquely determined by how it acts on $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$.
\end{lemma}
\begin{proof}
	Since $f$ is linear, we have that for all $(x,y,z)\in\R^3$ the following equation holds:
	\begin{align*}
	f(x,y,z)&=f((x,0,0)+(0,y,0)+(0,0,z))\\
	&=f(x,0,0)+f(0,y,0)+f(0,0,z)=xf(1,0,0)+yf(0,1,0)+zf(0,0,1)
	\end{align*}So if we put $f(1,0,0)=v$, $f(0,1,0)=u$ and $f(0,0,1)=w$ we see that $f(x,y,z)=xv+yv+zw$.
	
	This ends the proof.
\end{proof}
\begin{cor}
	Let $f:\{(1,0,0),(0,1,0),(0,0,1)\}\to\R^3$. Then there's a unique linear function $f'$ in $\R^3$ such that $f'(x,y,z):=xf(1,0,0)+yf(0,1,0)+zf(0,0,1)$.
\end{cor}

\begin{df}
	We'll denote the vectors $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$ by $e_1,e_2$ and $e_3$, respectively.
\end{df}
\begin{df}
	A finite set $X\subseteq\R^3$ is called a \textbf{base} of $\R^3$ if any linear function is uniquely determined by the image of $X$ - that is, if we write $X=\{x_1,x_2,\cdots,x_n\}$, then for every linear function $f$ in $\R^3$ and for all $v\in \R^3$, there are uniquely determined $\lambda_1,\cdots,\lambda_n\in \R$ such that $f(v)=\lambda_1x_1+\cdots+\lambda_nx_n$.
\end{df}
\begin{df}
	The set $E:=\{e_1,e_2,e_3\}$ is called the \textbf{canonical base} of $\R^3$.
\end{df}

\newpage
\subsection{SubSPACE!!! emissary}

\begin{prop}
	Let $v,u\in \R^3$ be any two non-null vectors. Then for any non-null $w\in\R v$ we have that $\R v=\R w$ and for any non-null $t\in \R v+\R u$ we have that $\R t+\R u=\R v+\R t=\R v+\R u$.
\end{prop}
\begin{proof}
	If $w\in \R v$, then $w=\lambda v$ for some $\lambda\in \R$. Take then any other $w'\in \R v$. Once again, this means that $w'=\lambda' v$ for some $\lambda'\in \R$. But now, clearly we have
	\[w'=\lambda'v=\lambda'\frac{\lambda}{\lambda}v=\frac{\lambda'}{\lambda}\lambda v=\frac{\lambda'}{\lambda}w\]so $w'\in \R w$ and $\R v\subseteq \R w$.
	
	Conversely, every $w'\in \R w$ is of the form $w'=\omega w$ for some $\omega\in \R$, but since $w=\lambda v$, we have that
	\[w'=\omega w=\omega \lambda v\]so $w'\in \R v$ and $\R w\subseteq \R v$.
	
	Therefore $\R v=\R w$.
	
	\bigskip
	To prove the second statement, we proceed analogously: Take $t\in \R v+\R u$. This means that $t=\tau_1 v+\tau_2 u$ for some $\tau_1,\tau_2\in \R$. Notice that at least one of $\tau_1,\tau_2$ must be non-zero, because otherwise $t=0$.
	
	Given any $t'\in \R v+\R u$, once again we can write it as $t'=\tau'_1 v+\tau_2' u$.
	
	If $\tau_1\neq0$, we can then do
	\begin{align*}
		t'&=\tau'_1 v+\tau_2' u\\
		&=\tau'_1\frac{\tau_1}{\tau_1}v+\tau_2'u\\
		&=\frac{\tau_1'}{\tau_1}\tau_1v+\tau_2'u\\
		&=\frac{\tau_1'}{\tau_1}(t-\tau_2u)+\tau_2'u\\
		&=
	\end{align*}
\end{proof}